2025-01-23 03:40:05 - openai._base_client - INFO - Retrying request to /chat/completions in 0.424722 seconds
2025-01-23 03:41:06 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:41:06 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:06 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:06 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:07 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-23 03:41:07 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:08 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:41:08 - root - INFO - SQL:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
Results:
path
push_notifications/migrations/0002_auto_20160106_0850.py
pgcrypto_expressions/test/settings.py
tests/python/tests/test_updatecollection.py
google/cloud/dataflow/utils/__init__.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
image_cropping/utils.py
src/VBox/Additions/common/crOpenGL/stub_common.py
examples/backend/myproject/myproject/wsgi.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
layout_scanner.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
third_party/jinja2/_compat.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
customuser/__init__.py
arc_net/arc_net/urls.py
web/adventures/admin.py
dns/api/main_test.py
container_engine/django_tutorial/polls/migrations/__init__.py
scipy/optimize/_basinhopping.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
third_party/chromite/scripts/dep_tracker_unittest.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Plex/Errors.py
lib/python3.4/site-packages/pip/backwardcompat/__init__.py
lib/python3.4/site-packages/django/contrib/gis/geos/point.py
bin/python_scripts/src/configuration/config_generator.py
app/util/timestamp_util.py
update-project.py
boilerplate/external/requests/packages/charade/eucjpprober.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
lib/python3.4/site-packages/django/core/management/templates.py
stores/apps/core/context_processors.py
arduino+esp01/correlation data/getSensorCorrelation.py
tests/test_folder.py
Three-Jumpstart/node_modules/node-gyp/gyp/pylib/gyp/MSVSToolFile.py
lib/django/db/migrations/operations/base.py
utilities/goliat3_script.py
bot/htmltagremove.py
pdc/apps/common/migrations/0002_auto_20150512_0703.py
settings.py
base/maps/compile.py
examples/graph.py
scripts/lib/hash_reqs.py
pypsi/progress.py
CSipSimple/jni/pjsip/sources/tests/pjsua/scripts-sendto/330_srtp_prefer_rtp_savp.py
fasta_extract.py
tools/perf/tests/attr.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
third_party/catapult/tracing/tracing_build/__init__.py
djangoproject/core_splinter_tests/__init__.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
prompt_toolkit/key_binding/digraphs.py
price_monitor/api/views/EmailNotificationListView.py
lib/python3.4/site-packages/django/contrib/gis/geos/prototypes/geom.py
lib/python3.4/site-packages/django/views/decorators/csrf.py
python/desc/twinkles/registry_tools.py
djangoproject/core/views/github_hook_views.py
clean_corpus.py
scripts/external_libs/scapy-2.3.1/python3/scapy/volatile.py
rest_framework_hstore/serializers.py
boilerplate/external/apiclient/discovery.py
ironic_inspector_client/__init__.py
python/v2.5/get_change_logs_for_advertiser.py
python/v2.5/get_user_roles.py
scipy/stats/tests/test_rank.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
build/android/pylib/local/__init__.py
stores/apps/sell/countries.py
stores/apps/core/views.py
USEFUL/snippets_and_examples_Flask/example_new_user_form/app/auth/views.py
commands/cat_allocation.py
python/v2.5/share_remarketing_list_to_advertiser.py
sklearn/externals/joblib/hashing.py
scipy/weave/examples/tuple_return.py
numpy/core/tests/test_ufunc.py
scipy/sparse/linalg/tests/test_onenormest.py
tests/dmarc_parser_tests.py
lib/django/contrib/gis/geos/libgeos.py
lib/django/db/migrations/operations/__init__.py
lib/django/contrib/gis/management/commands/__init__.py
ch07/code/ch07-01_dem2postgis.py

2025-01-23 03:41:08 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:08 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:08 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:09 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-23 03:41:09 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:09 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:41:10 - root - INFO - SQL:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.R'
   LIMIT 100;
Results:
path
code/summarize_mcc.R
Rmd_files/figureCode/figure6_invertTest.R
data_analysis/crossvalidation/randomForest.R
series/process-series.R
R/td.rank.R
r_modules/dataPackages/TCGAread/inst/import/protein/createProteinMatrix.R
snpGWAS/snpLogRegression.R
code/combine_data.R
h2o-r/demos/rdemo.gbm.R
R/inference_utility.R
src/library/graphics/R/persp.R
src/library/base/R/array.R
R/MxBounds.R
src/extra/testr/filtered-test-suite/isvector/tc_isvector_3.R
src/extra/testr/filtered-test-suite/list/tc_list_8.R
src/extra/testr/filtered-test-suite/gregexpr/tc_gregexpr_3.R
src/extra/testr/filtered-test-suite/match/tc_match_5.R
src/extra/testr/filtered-test-suite/seqint/tc_seqint_2.R
src/extra/testr/filtered-test-suite/beta/tc_beta_2.R
src/extra/testr/filtered-test-suite/format/tc_format_2.R
src/extra/testr/filtered-test-suite/cumsum/tc_cumsum_12.R
src/extra/testr/filtered-test-suite/attr/tc_attr_42.R
src/extra/testr/filtered-test-suite/asvector/tc_asvector_11.R
src/extra/testr/filtered-test-suite/names/tc_names_24.R
src/extra/testr/filtered-test-suite/attributes/tc_attributes_15.R
src/library/utils/R/objects.R
src/extra/testr/filtered-test-suite/length/tc_length_11.R
src/extra/testr/filtered-test-suite/sample/tc_sample_1.R
src/extra/testr/filtered-test-suite/cosh/tc_cosh_1.R
src/extra/testr/filtered-test-suite/deparse/tc_deparse_23.R
src/extra/testr/R/S4.R
tests/testthat/test-sas.R
examples/tm_raster.R
R/run_zinba.R
R/plot-tweets.R
inst/examples/plotMaps_usingExtractMethod.R
helper/R/mxVtDep.R
src/library/datasets/data/esoph.R
src/library/methods/tests/duplicateClass.R
conf/tables/r/subsetSite.R
R/meta.R
books/Gelman_BDA2_ARM/doc/arm2/pictures/logistic.R
R/merSubstEff.R
tests/extratests/mplus/mplusFunctions.R
R/ga-api-coerce.R
Mage/R/plotLifeCycle.R
examples/sim.setting.a.R
src/library/base/R/funprog.R
R/dataBench.R
R/data.R
scripts/manuscripts/manuscript_1/variance_partitioning_script_burnin2.R
Statistical_Inference/Expectations/normalMeans.R
R/zzz.R
src/library/stats/R/acf.R
src/L0_gbm41/model_parameters.R
R/order-book-reconstruction.R
sparkr-interpreter/src/main/resources/R/pkg/inst/tests/test_includePackage.R
util/R/get_Poisson_conf_intervals.R
basic/software.R
src/extra/testr/filtered-test-suite/paste/tc_paste_7.R
src/extra/testr/filtered-test-suite/match/tc_match_18.R
R/repositories.R
src/library/base/R/builtins.R
tests/testthat/projects/overwritten_controller/example/config/application.R
R/utils.R
Biology/DoobGillespieBirthsDensityDependent.R
r_modules/dataPackages/DEMOdz/inst/import/networks/mut/go.R
src/library/graphics/R/hist.R
tests/testthat/test-cnSpec_qual.R
lib/pegasus/r/Pegasus/DAX/tests/testthat/test_executable.R
demo/lda_fgs_acgs_ppc_R.R
82.R
R/bootstrap-FDR.R
src/library/grid/R/layout.R
R/checkDays.R
src/LDA-RLFM/R/model/Gibbs_LDA.R
books/Gelman_BDA2_ARM/doc/Book_Codes/Ch.2/2.3_CI.R
src/library/utils/R/package.skeleton.R
jobs/dream4_bbsr_noprior.R
R-src/src/library/base/R/sample.R
aws-blog-sparkr-geospatial-analysis/Spark-Redshift.R
src/library/grDevices/R/windows/cairo.R
src/library/datasets/data/DNase.R
R-src/src/library/tools/R/packages.R
Financial-Risk-Modelling-and-Portfolio-Optimization-with-R/source-code/Part3Chapter2Ex1.R
plot/plot_distribution.R
 dev_tools/package_dev.R
Coursera Statistics Princeton/stats1_supplemental_Stats1.13.Lab.06.R
PracticalML/quiz3.R
R/tests/testdir_jira/runit_hex_1484_rf_ls.R
R/tests/testdir_munging/slice/runit_rbind.R
R/tests/Utils/shared_javapredict_DL.R
h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_offset.R
h2o-r/tests/testdir_algos/gbm/runit_GBM_groupsplit_smallcat.R
R/install_h2o.R
R/is_standardUnit.R
data-raw/Hunting.R
src/library/datasets/data/islands.R
src/library/base/R/findInt.R
inst/models/passing/MxEvalDefinitionVar.R

2025-01-23 03:41:10 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:10 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:10 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:11 - snowflake.connector.cursor - INFO - Number of results in first chunk: 86
2025-01-23 03:41:11 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:11 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:41:11 - root - INFO - SQL:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.ipynb'
   LIMIT 100;
Results:
path
exercises/06-Titanic_cross_validation.ipynb
bcomposes-examples/a01-hidden-notebook.ipynb
BinPy/examples/notebook/connectors/Linker.ipynb
docs/geoms/geom_jitter.ipynb
BatchImporter/InvertedPendulum/InvertedPendulum_SimulinkControl.ipynb
nglview/tests/notebooks/closest_waters.ipynb
nglview/tests/notebooks/test_camera.ipynb
tutorials/tutorial-2-iges.ipynb
notebooks/IntroPython.ipynb
2015_Fall/MATH-578B/Homework10/Homework 3.ipynb
tutorials/new_data_iso.ipynb
doc/Tutorials/Exploring_Data.ipynb
code/webapp/code/music_note_cloud/cloudmaker.ipynb
nbgrader/docs/source/user_guide/release/ps1/problem2.ipynb
examples/.ipynb_checkpoints/compare_kernelKmenas_vs_kmeans-checkpoint.ipynb
notebooks/.ipynb_checkpoints/ntf_movie_lens-checkpoint.ipynb
pylearn2/scripts/tutorials/softmax_regression/softmax_regression.ipynb
examples/axon_patterns.ipynb
python_interface/ipython_notebooks/FlandmarkPythonInterfaceExample.ipynb
ipynb/module2_v1.0.ipynb
notebooks/1. Introduction to PyMC3.ipynb
doc/ipython-notebooks/classification/HashedDocDotFeatures.ipynb
doc/timeout.ipynb
hash-collision-prob.ipynb
contrib/roary_plots/roary_plots.ipynb
tensorflow-tutorials/01_basics.ipynb
examples/gradev-demo.ipynb
example/pandas_validator_example_ja.ipynb
slides/commit plots.ipynb
ar1_pymc.ipynb
introduction-to-big-data-with-apache-spark/lab-4/lab4_machine_learning_student.ipynb
notebooks/1_Introduction/helloworld.ipynb
examples/.ipynb_checkpoints/remove_distortion_from_edi-checkpoint.ipynb
tensorflow/examples/udacity/4_convolutions.ipynb
Python/Blaze/blaze_test.ipynb
code/08.sklearn_stock_market.ipynb
python/ipnb/testData/outputs.ipynb
notebooks/.ipynb_checkpoints/Oscillator Regimes-checkpoint.ipynb
doc/TypeCheck.ipynb
doc/pub/mcint/ipynb/mcint.ipynb
notebooks/wradlib_workflow.ipynb
demos_ch2/demo2_4.ipynb
OCR_Example.ipynb
notebooks/02-IntroMachineLearning_update.ipynb
"4 - pandas Basics/4-7 pandas DataFrame Summary Statistics, Filtering, Dropping and adding Rows and Columns, Grouping Basics.ipynb"
chapters/statistics/notebooks/Convergence.ipynb
chapters/statistics/notebooks/maximum_posteriori.ipynb
BinPy/examples/notebook/algorithms/expression_convert_example.ipynb
h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.ipynb
docs/examples/Adjusting Shapes.ipynb
BatchImporter/Introduction/Introduction_ControlPID.ipynb
docs/intro.ipynb
examples/notebook/Introduction.ipynb
develop/Simple%20Demo.ipynb
Logica Proposicional/Logica proposicional.ipynb
docs/old/RCall.ipynb
deepthought/experiments/ismir2014/plots/.ipynb_checkpoints/cube_plot-checkpoint.ipynb
tlg/Parse AUTHTAB.DIR.ipynb
data/examples/python2.7/Markdown.ipynb
demos_ch3/demo3_6.ipynb
notebooks/delete repos.ipynb
doc/ipython-notebooks/gaussian_process/variational_classifier.ipynb
notebook/.ipynb_checkpoints/ex4_layer-checkpoint.ipynb
deliver/02-Visualization-and-code-organization.ipynb
notebooks/jupyter-examples/6. Exercises.ipynb
lqcontrol/lqcontrol_solutions_jl.ipynb
Lecture-15/2.Topic-Modeling.ipynb
notebooks/profiling/fst.ipynb
powerline/doc/n_day_var.ipynb
docs/tutorial/closer_look_at_viz.ipynb
bugreport/boxplot_notch/boxplot_notch.ipynb
examples/pq-dbquantity.ipynb
blog-part-3.ipynb
tests/notebook/nbformat3/Index.ipynb
All.ipynb
docs/notebooks/4-Create-model.ipynb
notebooks/scratch/onlineai.ipynb
11_hyperbolic-2.ipynb
Exercises-4.ipynb
algorithms-in-C++/data-structures_03_queues.ipynb
Extra/ELGMarkov.ipynb
introduction-to-big-data-with-apache-spark/solutions/lab1_word_count_student.ipynb
examples/ipython/alanine_dipeptide_mstis_analysis.ipynb
Examples/CesiumWidget Example KML.ipynb
lectures/lec_04.ipynb
script/SGEMM_NT/explore-n-lws.ipynb

2025-01-23 03:41:11 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:11 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:11 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:13 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-23 03:41:13 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:13 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:41:13 - root - INFO - SQL:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if has

2025-01-23 03:41:13 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:13 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:13 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:15 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-23 03:41:15 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:15 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:41:39 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:41:39 - root - INFO - ```sql
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  ON c."sample_path" = f."path" AND c."sample_repo_name" = f."repo_name"
LIMIT 100;
```
2025-01-23 03:41:39 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:41:39 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:41:39 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:41:41 - snowflake.connector.cursor - INFO - Number of results in first chunk: 20
2025-01-23 03:41:43 - snowflake.connector.connection - INFO - closed
2025-01-23 03:41:43 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:42:50 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:42:50 - root - INFO - SQL:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  ON c."sample_path" = f."path" AND c."sample_repo_name" = f."repo_name"
LIMIT 100;
Results:
Too long, hard cut:
content
""
"Awful TV show. Cheap cheesy effects and the dumbest most repetitive plots in all of TVdom. Can't hold a candle to any of the movie versions all of which are far, far better. Perhaps the only draw of this show is the fact that in the first season it featured a little bit of nudity. Nice gimmick, didn't ABC do that years ago?"
".. _install:

############
Installation
############

Ce chapitre décrit les différentes étapes de mise en place d'une plate-forme de supervision basée sur Centreon.

La plate-forme de supervision peut-être installée de plusieurs manières. Cependant, **nous vous recommandons vivement d'utiliser Centreon ISO (ex CES) pour installer votre plate-forme**. Profitez ainsi de nos travaux d'industrialisation de l'installation et de la mise à jour de l'environnement. Profitez également des optimisations installées en standard par le système.
L'installation de Centreon peut être effectuée à partir des sources (tar.gz) mais le travail est plus complexe. De plus l'installeur ne sera supporté que par la communauté.

Avant toute installation, veillez à bien suivre les pré-requis d'installation et de dimensionnement (ressources CPU, mémoire, disques, partitionnement, etc...). Prenez également soin de bien choisir le type d'architecture qu'il convient de mettre en place pour vos besoins. 
Enfin, vous pourrez procéder à l'installation de la plate-forme.

.. toctree::
   :maxdepth: 2

   prerequisites
   architecture/index
   installation/index
   downloads
   01a
   from_packages
   from_sources
"
"# Emercoin Instance

This Microsoft Azure template deploys a single Emercoin client which will connect to the public Emercoin network.

[![Deploy to Azure](http://azuredeploy.net/deploybutton.png)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2Femercoin-ubuntu%2Fazuredeploy.json)
<a href=""http://armviz.io/#/?load=https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2Femercoin-ubuntu%2Fazuredeploy.json"" target=""_blank"">
    <img src=""http://armviz.io/visualizebutton.png""/>
</a>

Once your deployment is complete you will be able to connect to the Emercoin public network.

![Emercoin-Azure](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/emercoin-ubuntu/images/emercoin.png)

# Template Parameters
When you launch the installation of the VM, you need to specify the following parameters:
* `vmDnsPrefix`: this is the public DNS name for the VM that you will use interact with your console. You just need to specify an unique name.
* `adminUsername`: self-explanatory. This is the account you will use for connecting to the node
* `adminPassword`: self-explanatory. Be aware that Azure requires passwords to have One upper case, one lower case, a special character, and a number
* `vmSize`: The type of VM that you want to use for the node. The default size is D1_v2 but you can change that if you expect to run workloads that require more RAM or CPU resources.

# Emercoin Deployment Walkthrough
1. Get your node's IP
 1. browse to https://portal.azure.com

 2. then click browse all, followed by ""resource groups"", and choose your resource group

 3. then expand your resources, and public ip address of your node.

2. Connect to your node
 1. SSH to the public ip of your node as the user you specified for `adminUsername`, enter your `adminPassword`
 2. Try to use the cli-client by `emc help` or `emc getinfo`
 3. Point your browser to the public ip of your node, sign in with `adminUsername` and `adminPassword` specified before (note that browser may show you a warning of bad certificate - it's OK, you may replace the self-signed certificates by yours at /etc/ssl/emc/emcweb*)
"
"b3e.editor.ShortcutManager = function(editor) {
  ""use strict"";

  this._applySettings = function(settings) {};
};"
""
"@ECHO OFF

REM Command file for Sphinx documentation

if ""%SPHINXBUILD%"" == """" (
	set SPHINXBUILD=sphinx-build
)
set BUILDDIR=build
set SRCDIR=src
set ALLSPHINXOPTS=-d %BUILDDIR%/doctrees %SPHINXOPTS% %SRCDIR%
set I18NSPHINXOPTS=%SPHINXOPTS% %SRCDIR%
if NOT ""%PAPER%"" == """" (
	set ALLSPHINXOPTS=-D latex_paper_size=%PAPER% %ALLSPHINXOPTS%
	set I18NSPHINXOPTS=-D latex_paper_size=%PAPER% %I18NSPHINXOPTS%
)

if ""%1"" == """" goto help

if ""%1"" == ""help"" (
	:help
	echo.Please use `make ^<target^>` where ^<target^> is one of
	echo.  html       to make standalone HTML files
	echo.  dirhtml    to make HTML files named index.html in directories
	echo.  singlehtml to make a single large HTML file
	echo.  pickle     to make pickle files
	echo.  json       to make JSON files
	echo.  htmlhelp   to make HTML files and a HTML help project
	echo.  qthelp     to make HTML files and a qthelp project
	echo.  devhelp    to make HTML files and a Devhelp project
	echo.  epub       to make an epub
	echo.  latex      to make LaTeX files, you can 

2025-01-23 03:42:50 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:42:50 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:42:50 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:42:52 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-23 03:42:52 - snowflake.connector.connection - INFO - closed
2025-01-23 03:42:52 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:43:28 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:43:28 - root - INFO - ```sql
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%'
LIMIT 100;
```
2025-01-23 03:43:28 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:43:28 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:43:28 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:43:30 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-23 03:43:31 - snowflake.connector.connection - INFO - closed
2025-01-23 03:43:31 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:33 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:44:33 - root - INFO - SQL:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%'
LIMIT 100;
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"cmake_minimum_required(VERSION 2.8.3)
project(socketcan_interface)

find_package(catkin REQUIRED
  COMPONENTS
    class_loader
)

find_package(Boost REQUIRED
  COMPONENTS
    chrono
    system
    thread
)

catkin_package(
  INCLUDE_DIRS
    include
  LIBRARIES
    socketcan_interface_string
  CATKIN_DEPENDS
  DEPENDS
    Boost
)

include_directories(
  include
  ${Boost_INCLUDE_DIRS}
  ${catkin_INCLUDE_DIRS}
)

# socketcan_interface_string
add_library(socketcan_interface_string
    src/string.cpp
)

# socketcan_dump
add_executable(socketcan_dump
  src/candump.cpp
)
target_link_libraries(socketcan_dump
   socketcan_interface_string
   ${catkin_LIBRARIES}
   ${Boost_LIBRARIES}
)

# socketcan_interface_plugin
add_library(socketcan_interface_plugin
  src/socketcan_interface_plugin.cpp
)
target_link_libraries(socketcan_interface_plugin
  ${catkin_LIBRARIES}
  ${Boost_LIBRARIES}
)

install(
  TARGETS
    socketcan_dump
    socketcan_interface_plugin
    socketcan_interface_string
  ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
  LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
  RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
)

install(
  DIRECTORY
    include/${PROJECT_NAME}/
  DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}
  FILES_MATCHING PATTERN ""*.h""
)

install(
  FILES
    socketcan_interface_plugin.xml
  DESTINATION ${CATKIN_PACKAGE_SHARE_DESTINATION}
)
if(CATKIN_ENABLE_TESTING)

  catkin_add_gtest(${PROJECT_NAME}-test_dummy_interface
    test/test_dummy_interface.cpp
  )
  target_link_libraries(${PROJECT_NAME}-test_dummy_interface
    socketcan_interface_string
    ${catkin_LIBRARIES}
  )
endif()
"
"// Includes
#include ""stdafx.h""
#include ""Synergy Editor.h""
#include ""ChildFrm.h""
#include ""Doc.h""
#include ""View.h""
#include ""Help.h""
#include ""HTMLDocument.h""
#include ""Keywords.h""
#include ""Utilities.h""
#include ""Settings.h""
#include ""ReadProject.h""
#include ""afxwin.h""
#include ""Variables.h""
#include ""direct.h""
#include ""io.h""
//#include ""adpcore.h""

// Globals
int g_uTriggerIntroOn = 0;
bool g_bIsNETBOOKVersion = false;

// Prototype for associated GETDXVER.CPP (or empty true in MAIN.CPP if compiler)
DWORD g_dwDirectXVersion = 0;
TCHAR g_strDirectXVersion[10] = { 0 };
HRESULT GetDXVersion( DWORD* pdwDirectXVersion, TCHAR* strDirectXVersion, int cchDirectXVersion );

// Message Pump
BEGIN_MESSAGE_MAP(App, CWinApp)
	ON_COMMAND(ID_APP_ABOUT, OnAppAbout)
	ON_COMMAND_EX_RANGE(ID_FILE_MRU_FILE1, ID_FILE_MRU_FILE16, OnOpenRecentFile)
	// Standard file based document commands
	ON_COMMAND(ID_FILE_NEW, CWinApp::OnFileNew)
	ON_COMMAND(ID_FILE_OPEN, CWinApp::OnFileOpen)
	// Standard print setup command
	ON_COMMAND(ID_FILE_PRINT_SETUP, CWinApp::OnFilePrintSetup)
	ON_COMMAND(ID_HELP_DARKBASICPROFESSIONALHELP, OnHelp)
END_MESSAGE_MAP()

// App construction
App::App() :
	CBCGPWorkspace (TRUE /* m_bResourceSmartUpdate */)
{
	// TODO: add construction code here,
	// Place all significant initialization in InitInstance
}

// The one and only App object
App theApp;

// App initialization
BOOL App::InitInstance()
{
	/*
	// U75 - 171109 - check if special NETBOOK version
	g_bIsNETBOOKVersion = false;
	char pOldDir [ _MAX_PATH ];
	_getcwd ( pOldDir, _MAX_PATH );
	createdSettings = Settings::LoadSettings();
	CString pCertFolder = Settings::DBPLocation + L""Compiler\\certificates\\"";
	SetCurrentDirectory ( pCertFolder );
	_finddata_t filedata;
	long hInternalFile = _findfirst(""*.*"", &filedata);
	if(hInternalFile!=-1L)
	{
		int FileReturnValue = 0;
		bool bContainsDBPString = false;
		while ( FileReturnValue==0 && g_bIsNETBOOKVersion==false )
		{
			if( !(filedata.attrib & _A_SUBDIR) )
			{
				char pFilename[256];
				strcpy ( pFilename, filedata.name );
				if ( pFilename )
					if ( stricmp ( pFilename, ""a3330fee1fd9f540f18ee37c331c46ea"" )==NULL )
						g_bIsNETBOOKVersion = true;
			}
			FileReturnValue = _findnext(hInternalFile, &filedata);
		}
		_findclose(hInternalFile);
		hInternalFile=NU

2025-01-23 03:44:33 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:33 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:33 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:35 - snowflake.connector.cursor - INFO - Number of results in first chunk: 11
2025-01-23 03:44:36 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:36 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:36 - root - INFO - SQL:
-- Corrected SQL 2
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py' AND c."content" ILIKE '%import %'
LIMIT 100
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------

2025-01-23 03:44:36 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:36 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:36 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:38 - snowflake.connector.cursor - INFO - Number of results in first chunk: 7
2025-01-23 03:44:38 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:39 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:39 - root - INFO - SQL:
-- Corrected SQL 3
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.R' AND c."content" ILIKE '%library(%'
LIMIT 100
Results:
Too long, hard cut:
content
"library(raster)
library(data.table)
library(microbenchmark)
library(SpaDES)

N=1e4
sim <- new.env(parent = .GlobalEnv)
sim <- new.env()
sim$landscapeDT = data.table(test=1:(N^2), hello = sample(1:(N^2),(N^2)))
sim$ras <- raster(x=extent(0,N,0,N), res=1)
sim$ras[] = sim$landscapeDT$test


landscapeDT = copy(sim$landscapeDT)
ras <- raster(x=extent(0,N,0,N), res=1)
ras[] = -sim$landscapeDT$test

vec <- getValues(ras)

ras2 <- 1:10

# Note return invisible empty ... means no copy is made. The object is actually modified in place
#  as long as it is the same name, i.e., sim, as the environment name
changeValDT  <- function(sim, index, val) {
  sim$landscapeDT[index,hello:=val]
  invisible()
}

changeValDTNoSim  <- function(sim, index, val) {
  landscapeDT[index,hello:=val]
  invisible()
}

changeValRas  <- function(sim, index, val) {
  sim$ras[index]  <- val
  invisible()
}

index=sample(N^2, 5); print(index)

# Note - exact same time, meaning no copying is happening
microbenchmark(times=2000,
               new=changeValDT(sim, index=index, val=index),
               old=#{landscape <- getGlobal(""landscape""); 
                landscapeDT[index,hello:=index],
               alt=changeValDTNoSim(sim, index=index, val=index)
               # assignGlobal(""landscape"")}
               )
print(sim$landscapeDT[index,])
print(landscapeDT[index,])
print(landscapeDT[index,])

# Note - exact same time, meaning no copying is happening
microbenchmark(times=10,
               changeValRas(sim, index, index),
               ras[index]<-index,
               vec[index] <- index)

sim = new.env()
microbenchmark(times=100,
               makeEnv={sim$ras <- ras; sim$landscape <- landscape},
               makeList={sim2=list(ras, landscape)})

names(sim$ras) <- ""Fires""
setColors(sim$ras) <- c(""white"", rev(heat.colors(9)))
sim$ras <- setValues(sim$ras, 1)

"
"library(sptrans)
olhovivo_login()
"
"#!/usr/bin/env Rscript

library(edgeR)
library(getopt)

filterCountsFile <- function(inputFile, countsPerMillion, numSamples, outputFile) {
    counts <- read.table(inputFile, head=T, row.names=1)
    keep <- rowSums(cpm(counts)>countsPerMillion) >= numSamples
    output <- counts[keep,]
    write.table(output, outputFile,  quote=FALSE, sep=""\t"",
        row.names=TRUE, col.names=NA)
}

parseOptions <- function(optSpec) {
    opts <- getopt(optSpec)

    if (!is.null(opts$help)) {
        cat(getopt(optSpec, usage=TRUE))
        q(status=0)
    }

    required <- optSpec[optSpec[,3] == ""1"", 1]
    for (req in required) {
        if (is.null(opts[[req]])) {
            cat(paste(""Required option"", req, ""missing!\n""))
            q(status=1)
        }
    }

    opts
}

optSpec = matrix(c(
    ""help"", ""h"", 0, NA,
    ""input-file"", ""i"", 1, ""character"",
    ""counts-per-million"", ""c"", 1, ""integer"",
    ""num-samples"", ""n"", 1, ""integer"",
    ""output-file"", ""o"", 1, ""character""
    ), byrow=TRUE, ncol=4)

opts <- parseOptions(optSpec)

dge <- filterCountsFile(opts$`input-file`, opts$`counts-per-million`, opts$`num-samples`, opts$`output-file`)
"
"library(shogun)

fm_train_real <- t(as.matrix(read.table('../data/fm_train_real.dat')))
fm_test_real <- t(as.matrix(read.table('../data/fm_test_real.dat')))

# poly
print('Poly')

feats_train <- RealFeatures()
dummy <- feats_train$set_feature_matrix(fm_train_real)
feats_test <- RealFeatures()
dummy <- feats_test$set_feature_matrix(fm_test_real)
degree <- as.integer(4)
inhomogene <- FALSE

kernel <- PolyKernel(
	feats_train, feats_train, degree, inhomogene)

km_train <- kernel$get_kernel_matrix()
dump <- kernel$init(feats_train, feats_test)
km_test <- kernel$get_kernel_matrix()
"
"library(PLSR)
library(RUnit)
library(DEMOdz)
library(TCGAgbm)
Sys.setlocale(""LC_ALL"", ""C"")

if(!exists(""marker.genes.545"")){
    print(load(system.file(package=""TCGAgbm"", ""extdata"", ""genesets.RData"")))
    marker.genes.545 <- genesets$marker.genes.545
    tcga.GBM.classifiers <- genesets$tcga.GBM.classifiers
    }
#----------------------------------------------------------------------------------------------------
runTests <- function()
{
  test_constructor()
  test_summarizePatientNumericFactors()
  test_summarizePatientNumericFactors_TCGAgbm_poorPatientOverlap()
  test_summarizePatientNumericFactors_DEMOdz()
  test_createClassificationMatrix()
  test_createClassificationMatrix.1factor()
  test_createClassificationMatrix.2factors()
  test_createBigClassificationMatrix()
  test_calculateTiny.ageDxDemo()
  #test_calculateTiny.ageDxAndSurvival.demo()
  test_calculateSmall.ageDxOnly()
  test_calculateSmall.survivalOnly()
  test_calculateSmall()
  test_calculateBig()
  test_changeDataSetsAndCalculateSmall()

} # runTests
#----------------------------------------------------------------------------------------------------
test_constructor = function()
{
   printf(""--- test_constructor"")
   demoDz <- DEMOdz()
   mrna.datasets <- sort(grep(""mtx.mrna"", manifest(demoDz)$

2025-01-23 03:44:39 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:39 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:39 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:41 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-23 03:44:42 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:43 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:43 - root - INFO - SQL:
-- Corrected SQL 4
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.ipynb' AND c."content" ILIKE '%import %'
LIMIT 100
Results:
Too long, hard cut:
content
"{
 ""cells"": [
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""<!-- dom:TITLE: Introduction to Monte Carlo methods -->\n"",
    ""# Introduction to Monte Carlo methods\n"",
    ""<!-- dom:AUTHOR: Morten Hjorth-Jensen   Email morten.hjorth-jensen@fys.uio.no at Department of Physics and Center of Mathematics for Applications, University of Oslo & National Superconducting Cyclotron Laboratory, Michigan State University -->\n"",
    ""<!-- Author: --> **Morten Hjorth-Jensen   Email morten.hjorth-jensen@fys.uio.no**, Department of Physics and Center of Mathematics for Applications, University of Oslo and National Superconducting Cyclotron Laboratory, Michigan State University\n"",
    ""\n"",
    ""Date: **Fall 2015**\n"",
    ""\n"",
    ""## Monte Carlo methods, plan for the lectures\n"",
    ""1. First week: intro, MC integration and probability distribution functions (PDFs)\n"",
    ""\n"",
    ""2. Second week: More on integration, PDFs, MC integration and random walks.\n"",
    ""\n"",
    ""3. Third week: random walks and statistical physics.\n"",
    ""\n"",
    ""4. Fourth week: Statistical physics and the Ising and Potts models\n"",
    ""\n"",
    ""5. Fifth week: Most likely quantum Monte Carlo\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Monte Carlo: Enhances algorithmic thinking!\n"",
    ""* Be able to generate random variables following a given probability distribution function PDF\n"",
    ""\n"",
    ""* Find a probability distribution function (PDF)\n"",
    ""\n"",
    ""* Sampling rule for accepting a move\n"",
    ""\n"",
    ""* Compute standard deviation and other expectation values\n"",
    ""\n"",
    ""* Techniques for improving errors\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Domains and probabilities\n"",
    ""Consider the following simple example, namely the tossing of a dice, resulting in  the following possible values""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{2,3,4,5,6,7,8,9,10,11,12\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""These values are called the *domain*. \n"",
    ""To this domain we have the corresponding *probabilities*""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{1/36,2/36/3/36,4/36,5/36,6/36,5/36,4/36,3/36,2/36,1/36\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""## Monte Carlo methods, tossing a dice\n"",
    ""The numbers in the domain are the outcomes of the physical process tossing the dice.\n"",
    ""We cannot tell beforehand whether the outcome is 3 or 5 or any other number in this domain.\n"",
    ""This defines the randomness of the outcome, or unexpectedness or any other synonimous word which\n"",
    ""encompasses the uncertitude of the final outcome. \n"",
    ""\n"",
    ""The only thing we can tell beforehand\n"",
    ""is that say the outcome 2 has a certain probability.  \n"",
    ""If our favorite hobby is to  spend an hour every evening throwing dice and \n"",
    ""registering the sequence of outcomes, we will note that the numbers in the above domain""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{2,3,4,5,6,7,8,9,10,11,12\\},\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""appear in a random order. After 11 throws the results may look like""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{10,8,6,3,6,9,11,8,12,4,5\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""## Stochastic variables\n"",
    ""\n"",
    ""**Random variables are characterized by a domain which contains all possible values that the random value may take. This domain has a corresponding PDF**.\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Stochastic variables and the main concepts, the discrete case\n"",
    ""There are two main concepts associated with a stochastic variable. The\n"",
    ""*domain* is the set $\\mathbb D = \\{x\\}$ of all accessible values\n"",
    ""the variable can assume, so that $X \\in \\mathbb D$. An example of a\n"",
    ""discrete domain is the set of six different numbers that we may get by\n"",
    ""throwing of a dice, $x\\in\\{1,\\,2,\\,3,\\,4,\\,5,\\,6\\}$.\n"",
    ""\n"",
    ""The *probability distribution function (PDF)* is a function\n"",
    ""$p(x)$ on the domain which, in the discrete case, gives us the\n"",
    ""probability or relative frequency with which these values of $X$\n"",
    ""occur""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""p(x) = \\mathrm{Prob}(X=x).\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"

2025-01-23 03:44:43 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:43 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:43 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:44 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-23 03:44:44 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:44 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:45 - root - INFO - SQL:
-- Corrected SQL 5
SELECT DISTINCT c."sample_repo_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
LIMIT 100
Results:
sample_repo_name
cloudmatrix/esky
vmesel/GeoMath
lsaffre/lino
gunnery/gunnery
jvanasco/metadata_parser
poldrack/myconnectome
olhoneles/olhoneles
openstack/nova
Nymphet/sexinsex-crawler
D-L/SimpleBookMarks
grundgruen/powerline
omab/psa-recovery
n3ko1/WrapMap
google/grr
DarkFenX/Pyfa
rocky/python3-trepan
piiswrong/dec
openconnectome/m2g
RealTimeWeb/blockpy
funkandwagnalls/pythonpentest
pyQode/pyqode.core
exaile/exaile
openstack/monasca-agent
feuervogel/django-taggit-templatetags
hpcugent/hanythingondemand
wreckage/django-happenings
openstack/rack
alexforencich/python-ivi
udp/json-parser
maxcutler/python-wordpress-xmlrpc
ejeschke/ginga
LinuxCNC/linuxcnc
networkx/networkx
drvink/epanos
wmayner/pyphi
mbuesch/awlsim
espadrine/opera
tgrosinger/aenea-grammars
openstack/cinder
ceph/ceph-installer
croach/django-simple-rest
svpcom/hyperloglog
dropbox/changes
openstack/python-neutronclient
OpenBazaar/OpenBazaar-Server
bdoin/GCompris
hydralabs/pyamf
golismero/golismero
edx/edx-platform
mushorg/conpot
mila-udem/fuel
robweber/xbmcbackup
milkbikis/powerline-shell
rusthon/Rusthon
GeoNode/geonode
pazz/alot
KhronosGroup/COLLADA-CTS
jdapena/swftools
calmofthestorm/aenea
pyca/cryptography
ARM-software/workload-automation
ahmetcemturan/SFACT
Stanford-Online/edx-platform
crossbario/crossbarexamples
nsnam/ns-3-dev-git
apache/stratos
mne-tools/mne-python
PyTables/PyTables
j5shi/Thruster
mollstam/UnrealPy
pybuilder/pybuilder
cgstudiomap/cgstudiomap
cjones/madcow
rec/echomesh
biocore/qiime
viewfinderco/viewfinder
mdaniel/virtualbox-org-svn-vbox-trunk
SciTools/iris
openhatch/oh-mainline
OCA/l10n-spain
spulec/moto
Kitware/VTK
openfun/edx-platform
gem5/gem5
bugvm/bugvm-studio
chyyuu/ucore_os_lab
kovidgoyal/calibre
basicthinker/ThyNVM
goace/personal-file-sharing-center
grap/odoo-addons-misc
Ulauncher/Ulauncher
pegasus-isi/pegasus
openstack/keystone
SekoiaLab/Fastir_Collector
openstack/murano
harrymvr/absorbing-centrality
jacinda/pycon_library
MartinPyka/Parametric-Anatomical-Modeling
qdqmedia/wiggum
openstack/fuel-plugin-midonet

2025-01-23 03:44:45 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:45 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:45 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:46 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-23 03:44:46 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:46 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:46 - root - INFO - SQL:
-- Corrected SQL 6
SELECT c."sample_repo_name", COUNT(*) AS "python_file_count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
GROUP BY c."sample_repo_name"
ORDER BY "python_file_count" DESC
LIMIT 100
Results:
sample_repo_name,python_file_count
anhstudios/swganh,11
mollstam/UnrealPy,7
bugvm/bugvm-studio,5
Cadasta/cadasta-platform,4
JetBrains/intellij-community,4
qunaibit/zippy-mirror,4
rocky/python3-trepan,3
espadrine/opera,3
golismero/golismero,3
samabhi/pstHealth,3
yade/trunk,3
ArduPilot/MissionPlanner,3
dimagi/commcare-hq,3
dropbox/changes,2
Moguri/BlenderPanda,2
django/django-formtools,2
SekoiaLab/Fastir_Collector,2
mbuesch/awlsim,2
exaile/exaile,2
junalmeida/Sick-Beard,2
nuagenetworks/monolithe,2
linkedin/Zopkio,2
rizar/attention-lvcsr,2
MissionCriticalCloud/cosmic,2
openstack/rack,2
sassoftware/conary,2
aliyun/oss-ftp,2
h2oai/h2o-2,2
openhatch/oh-mainline,2
GoogleCloudPlatform/gcloud-python,2
openstack/nova,2
kret0s/gnuhealth-live,2
otsaloma/gaupol,2
SeleniumHQ/selenium,2
lsaffre/lino,2
DarkFenX/Pyfa,2
viewfinderco/viewfinder,2
getsentry/sentry,2
hazelcast/hazelcast-python-client,2
bokeh/bokeh,2
moesy/AWS-Lambda-ML-Microservice-Skeleton,2
MITRECND/crits,2
harmy/kbengine,2
zeromq/pyzmq,2
wummel/linkchecker,2
PacificBiosciences/pypeFLOW,2
ngageoint/scale,2
edx/edx-platform,2
thiago-silva/memetalk,2
IONISx/edx-platform,2
openstack/monasca-agent,2
OpenBazaar/OpenBazaar-Server,2
inasafe/inasafe,2
edx/ecommerce,2
django/django,2
autotest/autotest,2
Stanford-Online/edx-platform,2
Datera/cinder,2
hakobera/luigi-bigquery,2
cloudera/hue,2
cgstudiomap/cgstudiomap,2
openconnectome/m2g,2
openstack/vitrage,2
openstack/cinder,2
sfepy/sfepy,2
vlegoff/tsunami,2
rec/echomesh,2
biocore/qiime,2
feist/pcs,1
PySide/pyside2,1
Ulauncher/Ulauncher,1
nfco/netforce,1
sockeye44/instavpn,1
openstack/third-party-ci-tools,1
Yelp/fullerite,1
egaxegax/django-egaxegax,1
wmayner/pyphi,1
d-ai/Sourcepawn,1
marsyas/marsyas,1
Microsoft/PTVS,1
aut-sepanta/Sepanta3,1
gurock/testrail-api,1
piiswrong/dec,1
hpcugent/hanythingondemand,1
regreg/regreg,1
feuervogel/django-taggit-templatetags,1
oVirt/vdsm,1
iridium-browser/iridium-browser-ubuntu,1
mapzen/TileStache,1
drvink/epanos,1
LinuxCNC/linuxcnc,1
bq/bitbloq-offline,1
MyRobotLab/pyrobotlab,1
scrapinghub/splash,1
azatoth/scons,1
Flexget/Flexget,1
statsmodels/statsmodels,1
etetoolkit/ete,1
wreckage/django-happenings,1
RealTimeWeb/blockpy,1

2025-01-23 03:44:46 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:46 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:46 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:48 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-23 03:44:48 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:48 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:48 - root - INFO - SQL:
-- Corrected SQL 7
SELECT c."size"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
LIMIT 100
Results:
size
975
1395
1448
44812
3942
1636
184
285
6556
2189
3459
1445
17043
17923
2535
23052
2807
473
951
4259
577
226
24133
1739
26622
669
1320
3528
2610
1537
4499
2101
385
3835
1681
1937
12907
13943
638
27648
775
11355
1640
125539
240
4015
5311
761
590
18432
2147
14896
9381
5214
544
483
332
3519
451
2632
2736
8277
5111
3718
73117
7005
5349
8333
26113
2276
1057
2289
1138
1037
317
22903
317
203
2058
1654
2887
17115
836
430
22259
6933
478
3961
26343
1268
3114
3519
2506
807
545
1473
62274
3376
4641
41

2025-01-23 03:44:48 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:48 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:48 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:50 - snowflake.connector.cursor - INFO - Number of results in first chunk: 18
2025-01-23 03:44:51 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:51 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:51 - root - INFO - SQL:
-- Corrected SQL 8
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py' AND c."content" ILIKE '%from % import %'
LIMIT 100
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------

2025-01-23 03:44:51 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:51 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:51 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:53 - snowflake.connector.cursor - INFO - Number of results in first chunk: 2
2025-01-23 03:44:53 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:53 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:53 - root - INFO - SQL:
-- Corrected SQL 9
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.R' AND c."content" ILIKE '%require(%'
LIMIT 100
Results:
Too long, hard cut:
content
"setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$""f"")))
source(""../../../scripts/h2o-r-test-setup.R"")
### This tests observation weights in glm ######




test_weights_by_row_duplication <- function() {
  
  require(testthat)
  
  #create data
  print(""create synthetic data with a numeric outcome"")
  set.seed(45541)
  n <- 100  #Number of training observations (also nrow(test) = n)
  p <- 20  #Number of features
  x <- matrix(rnorm(n*p), n, p)
  y <- rnorm(n)
  
  # Training data with weights
  # draw some random weights ~ Poisson, add 'x1' weight col and y to df, hdf
  set.seed(1234)
  x1 <- rpois(n, rep(2, n)) + 1  #Random integer-valued (>=1) weights
  df <- data.frame(x, x1, y)  #design matrix with weight and outcome cols
  hdf <- as.h2o(df, destination_frame = ""hdf"")  #for h2o
  
  # Training data (weights == 1.0 with repeated rows instead of weights)
  rep_idxs <- unlist(sapply(1:n, function(i) rep(i, df$x1[i])))
  rdf <- df[rep_idxs,]  #repeat rows
  rdf$x1 <- 1  #set weights back to 1.0
  rhdf <- as.h2o(rdf, destination_frame = ""rhdf"")  #for h2o
  
  ## for glmnet
  #df <- as.matrix(df)
  #rdf <- as.matrix(rdf)
  
  # Test data
  set.seed(2641)
  newx <- matrix(rnorm(n*p), n, p)
  newy <- rnorm(n)
  
  x1 <- rep(1, n)
  valid1 <- data.frame(newx, x1, y = newy)
  val1 <- as.h2o(valid1, destination_frame = ""val1"")
  valid1 <- as.matrix(valid1)
  
  print(""build models with weights vs repeated rows with h2o and lambda!=0"")
  hh1 <- h2o.gbm(x = 1:20, y = ""y"", 
                 training_frame = hdf, 
                 validation_frame = val1, 
                 ntrees = 50, 
                 weights_column = ""x1"")
  hh2 <- h2o.gbm(x = 1:20, y = ""y"", 
                 training_frame = rhdf, 
                 validation_frame = val1,
                 ntrees = 50, 
                 weights_column = ""x1"")
  
  print(""compare results"")
  expect_equal(hh1@model$training_metrics@metrics$MSE, 
               hh2@model$training_metrics@metrics$MSE,
               tolerance = 1e-6)
  expect_equal(hh1@model$training_metrics@metrics$r2, 
               hh2@model$training_metrics@metrics$r2,
               tolerance = 1e-6)

  expect_equal(hh1@model$validation_metrics@metrics$MSE,
               hh2@model$validation_metrics@metrics$MSE,
               tolerance = 1e-6)
  expect_equal(hh1@model$validation_metrics@metrics$r2,
               hh2@model$validation_metrics@metrics$r2,
               tolerance = 1e-6)
  
    
  #predictions
  print(""compare predictions"")
  ph1 <- as.data.frame(h2o.predict(object = hh1, newdata = val1))
  ph2 <- as.data.frame(h2o.predict(object = hh2, newdata = val1))
  mse1 <- mean((ph1$predict - newy)^2)
  mse2 <- mean((ph2$predict - newy)^2)
  #expect_equal(mse1, mse2)  #1.49 - 1.46 == 0.0291
  
  
  
}


doTest(""GBM weight Test: GBM w/ weights test by row duplication"", test_weights_by_row_duplication)
"
"## This script checks whether the reads in each splitted file belong to the same barcode
## and whether the file name contains the corresponding sample id name.
## Note, this script is a memory hog. 
## It was used to check the idemp output for one of our miseq runs.
##source(""http://bioconductor.org/biocLite.R"")
##biocLite(""ShortRead"")
require(ShortRead)

#### Files original and decoded ####
barcodeFile=""barcode_sample.txt""
I1File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_I1_001.fastq.gz""
R1File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_R1_001.fastq.gz"" 
R2File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_R2_001.fastq.gz""
decodedFile=list.files(""./"", pattern=""*.fastq.gz$"")

#### Read in barcode table, sequence names and barcodes ####
firstLine=readLines(""barcode_sample.txt"",1)
barcodeTable=read.table(barcodeFile, header=grepl(""arcode"", firstLine))
### Read in seqquence names and barcodes ###
barcodeReads=readFastq(I1File)
I1Names=unname(sapply(as.character(id(barcodeReads)), function(x) (strsplit(x,"" "")[[1]][1] ) ) )
I1Barcodes=as.character(sread(barcodeReads))

#### Loop through decoded files ####
for( i in 1:length(decodedFile) ) {
  cat(i, "" of "", length(decodedFile), ""\n"")
  cat(decodedFile[i],""\n"")
  decodedReads=readFastq(decodedFile[i])
  seqNames=unname(sapply(as.character(id(decodedReads)), function(x) (strsplit(x,"" "")[[1]][1] ) ) )
  seqBarcodes=as.character(sread(decodedReads))
  
  idx=match(seqNames, I1Names)
  #table(I1Barcodes[idx])
  #sort(table(I1Barcodes[idx]), decreasing=T)
  bctable=sort(table(I1Barcodes[idx]), decreasing=T)
  if ( length(bctable)> 1000 ) {
    message(length(bctable), "" barcodes found out of "", length(seqNames), "" reads."") 
  }
  if ( length( grep(names(bctable)[1], barcodeTable[,1]) )==0 ) {
    message(""most frequent barcode "",names(bctable)[1], "" not matched."") 
    next
  }
  if ( length(bctable)> 1000 ) n

2025-01-23 03:44:53 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:44:53 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:44:53 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:44:55 - snowflake.connector.cursor - INFO - Number of results in first chunk: 11
2025-01-23 03:44:56 - snowflake.connector.connection - INFO - closed
2025-01-23 03:44:56 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:44:57 - root - INFO - SQL:
-- Corrected SQL 10
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE (c."sample_path" LIKE '%.py' OR c."sample_path" LIKE '%.ipynb' OR c."sample_path" LIKE '%.R')
  AND c."content" ILIKE '%import %'
LIMIT 100
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------

2025-01-23 03:44:57 - root - INFO - itercount: 0
2025-01-23 03:44:57 - root - INFO - Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
DDL describes table information.
,0
table_name,SAMPLE_FILES
description,
DDL,"create or replace TABLE SAMPLE_FILES (
	""repo_name"" VARCHAR(16777216),
	""ref"" VARCHAR(16777216),
	""path"" VARCHAR(16777216),
	""mode"" NUMBER(38,0),
	""id"" VARCHAR(16777216),
	""symlink_target"" VARCHAR(16777216)
);"

,1
table_name,LANGUAGES
description,
DDL,"create or replace TABLE LANGUAGES (
	""repo_name"" VARCHAR(16777216),
	""language"" VARIANT
);"

,2
table_name,LICENSES
description,
DDL,"create or replace TABLE LICENSES (
	""repo_name"" VARCHAR(16777216),
	""license"" VARCHAR(16777216)
);"

,3
table_name,SAMPLE_COMMITS
description,
DDL,"create or replace TABLE SAMPLE_COMMITS (
	""commit"" VARCHAR(16777216),
	""tree"" VARCHAR(16777216),
	""parent"" VARIANT,
	""author"" VARIANT,
	""committer"" VARIANT,
	""subject"" VARCHAR(16777216),
	""message"" VARCHAR(16777216),
	""trailer"" VARIANT,
	""difference"" VARIANT,
	""difference_truncated"" BOOLEAN,
	""repo_name"" VARCHAR(16777216),
	""encoding"" VARCHAR(16777216)
);"

,4
table_name,SAMPLE_CONTENTS
description,
DDL,"create or replace TABLE SAMPLE_CONTENTS (
	""id"" VARCHAR(16777216),
	""size"" NUMBER(38,0),
	""content"" VARCHAR(16777216),
	""binary"" BOOLEAN,
	""copies"" NUMBER(38,0),
	""sample_repo_name"" VARCHAR(16777216),
	""sample_ref"" VARCHAR(16777216),
	""sample_path"" VARCHAR(16777216),
	""sample_mode"" NUMBER(38,0),
	""sample_symlink_target"" VARCHAR(16777216)
);"

,5
table_name,SAMPLE_REPOS
description,
DDL,"create or replace TABLE SAMPLE_REPOS (
	""repo_name"" VARCHAR(16777216),
	""watch_count"" NUMBER(38,0)
);"

External knowledge that might be helpful: 
None
The table structure information is ({database name: {schema name: {table name}}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_FILES', 'SAMPLE_REPOS']}}
Begin Exploring Related Columns
**Consideration of Relevant Tables and Columns:**

- `"SAMPLE_FILES"."path"`: Contains the file paths, which can be used to identify files with extensions '.py', '.ipynb', and '.R'.
  
  - Potential Usage: Filtering files by their extensions to focus on Python, R, and IPython script files.

- `"SAMPLE_FILES"."id"`: Unique identifier for each file, which can be used to join with `"SAMPLE_CONTENTS"`.

  - Potential Usage: Joining with `"SAMPLE_CONTENTS"` to retrieve the content of specific files.

- `"SAMPLE_CONTENTS"."content"`: Contains the actual content of the files.

  - Potential Usage: Searching for import statements within the file content.

- Conditions that may be used:

  - Filter `"path"` using `LIKE '%.py'`, `LIKE '%.ipynb'`, or `LIKE '%.R'` to select specific file types.
  - Use `ILIKE '%import %'` or `ILIKE '%library(%'` to find lines that contain import statements.

---

**SQL Queries:**

1. **Select distinct paths of Python files:**

   ```sql
   SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
   ```

2. **Select distinct paths of R files:**

   ```sql
   SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.R'
   LIMIT 100;
   ```

3. **Select distinct paths of IPython notebook files:**

   ```sql
   SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.ipynb'
   LIMIT 100;
   ```

4. **Retrieve content of Python files:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
   ```

5. **Retrieve content of R files:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.R'
   LIMIT 100;
   ```

6. **Retrieve content of IPython notebook files:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.ipynb'
   LIMIT 100;
   ```

7. **Find Python file contents that contain 'import' statements:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."content" ILIKE '%import %'
   LIMIT 100;
   ```

8. **Find R file contents that contain 'library' statements:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.R' AND c."content" ILIKE '%library(%'
   LIMIT 100;
   ```

9. **Find IPython notebook contents that contain 'import' statements:**

   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.ipynb' AND c."content" ILIKE '%import %'
   LIMIT 100;
   ```

10. **List repositories with Python files:**

    ```sql
    SELECT DISTINCT f."repo_name"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
    WHERE f."path" LIKE '%.py'
    LIMIT 100;
    ```

11. **Count number of Python files per repository:**

    ```sql
    SELECT f."repo_name", COUNT(*) AS "python_file_count"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
    WHERE f."path" LIKE '%.py'
    GROUP BY f."repo_name"
    ORDER BY "python_file_count" DESC
    LIMIT 100;
    ```

12. **Retrieve sizes of Python files:**

    ```sql
    SELECT c."size"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
      ON c."id" = f."id"
    WHERE f."path" LIKE '%.py'
    LIMIT 100;
    ```

13. **Find Python file contents that contain 'from' import statements:**

    ```sql
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
      ON c."id" = f."id"
    WHERE f."path" LIKE '%.py' AND c."content" ILIKE '%from % import %'
    LIMIT 100;
    ```

14. **Find R file contents that contain 'require' statements:**

    ```sql
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
      ON c."id" = f."id"
    WHERE f."path" LIKE '%.R' AND c."content" ILIKE '%require(%'
    LIMIT 100;
    ```

15. **Retrieve content of files containing 'import' statements across all file types:**

    ```sql
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
      ON c."id" = f."id"
    WHERE (f."path" LIKE '%.py' OR f."path" LIKE '%.ipynb' OR f."path" LIKE '%.R')
      AND c."content" ILIKE '%import %'
    LIMIT 100;
    ```

**Note:** These queries are designed to explore the dataset and understand the distribution and content of Python, R, and IPython script files in the GitHub sample dataset. By examining file paths, content, and repositories, one can gain insights into which modules or libraries are frequently used.

---

This approach focuses on retrieving relevant data from the tables provided, adhering to the given instructions, and using the appropriate SQL syntax and functions. The queries consider various possibilities, such as different file extensions and common import patterns in Python and R files.Query:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
Answer:
path
push_notifications/migrations/0002_auto_20160106_0850.py
pgcrypto_expressions/test/settings.py
tests/python/tests/test_updatecollection.py
google/cloud/dataflow/utils/__init__.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
image_cropping/utils.py
src/VBox/Additions/common/crOpenGL/stub_common.py
examples/backend/myproject/myproject/wsgi.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
layout_scanner.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
third_party/jinja2/_compat.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
customuser/__init__.py
arc_net/arc_net/urls.py
web/adventures/admin.py
dns/api/main_test.py
container_engine/django_tutorial/polls/migrations/__init__.py
scipy/optimize/_basinhopping.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
third_party/chromite/scripts/dep_tracker_unittest.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Plex/Errors.py
lib/python3.4/site-packages/pip/backwardcompat/__init__.py
lib/python3.4/site-packages/django/contrib/gis/geos/point.py
bin/python_scripts/src/configuration/config_generator.py
app/util/timestamp_util.py
update-project.py
boilerplate/external/requests/packages/charade/eucjpprober.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
lib/python3.4/site-packages/django/core/management/templates.py
stores/apps/core/context_processors.py
arduino+esp01/correlation data/getSensorCorrelation.py
tests/test_folder.py
Three-Jumpstart/node_modules/node-gyp/gyp/pylib/gyp/MSVSToolFile.py
lib/django/db/migrations/operations/base.py
utilities/goliat3_script.py
bot/htmltagremove.py
pdc/apps/common/migrations/0002_auto_20150512_0703.py
settings.py
base/maps/compile.py
examples/graph.py
scripts/lib/hash_reqs.py
pypsi/progress.py
CSipSimple/jni/pjsip/sources/tests/pjsua/scripts-sendto/330_srtp_prefer_rtp_savp.py
fasta_extract.py
tools/perf/tests/attr.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
third_party/catapult/tracing/tracing_build/__init__.py
djangoproject/core_splinter_tests/__init__.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
prompt_toolkit/key_binding/digraphs.py
price_monitor/api/views/EmailNotificationListView.py
lib/python3.4/site-packages/django/contrib/gis/geos/prototypes/geom.py
lib/python3.4/site-packages/django/views/decorators/csrf.py
python/desc/twinkles/registry_tools.py
djangoproject/core/views/github_hook_views.py
clean_corpus.py
scripts/external_libs/scapy-2.3.1/python3/scapy/volatile.py
rest_framework_hstore/serializers.py
boilerplate/external/apiclient/discovery.py
ironic_inspector_client/__init__.py
python/v2.5/get_change_logs_for_advertiser.py
python/v2.5/get_user_roles.py
scipy/stats/tests/test_rank.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
build/android/pylib/local/__init__.py
stores/apps/sell/countries.py
stores/apps/core/views.py
USEFUL/snippets_and_examples_Flask/example_new_user_form/app/auth/views.py
commands/cat_allocation.py
python/v2.5/share_remarketing_list_to_advertiser.py
sklearn/externals/joblib/hashing.py
scipy/weave/examples/tuple_return.py
numpy/core/tests/test_ufunc.py
scipy/sparse/linalg/tests/test_onenormest.py
tests/dmarc_parser_tests.py
lib/django/contrib/gis/geos/libgeos.py
lib/django/db/migrations/operations/__init__.py
lib/django/contrib/gis/management/commands/__init__.py
ch07/code/ch07-01_dem2postgis.py
Query:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.R'
   LIMIT 100;
Answer:
path
code/summarize_mcc.R
Rmd_files/figureCode/figure6_invertTest.R
data_analysis/crossvalidation/randomForest.R
series/process-series.R
R/td.rank.R
r_modules/dataPackages/TCGAread/inst/import/protein/createProteinMatrix.R
snpGWAS/snpLogRegression.R
code/combine_data.R
h2o-r/demos/rdemo.gbm.R
R/inference_utility.R
src/library/graphics/R/persp.R
src/library/base/R/array.R
R/MxBounds.R
src/extra/testr/filtered-test-suite/isvector/tc_isvector_3.R
src/extra/testr/filtered-test-suite/list/tc_list_8.R
src/extra/testr/filtered-test-suite/gregexpr/tc_gregexpr_3.R
src/extra/testr/filtered-test-suite/match/tc_match_5.R
src/extra/testr/filtered-test-suite/seqint/tc_seqint_2.R
src/extra/testr/filtered-test-suite/beta/tc_beta_2.R
src/extra/testr/filtered-test-suite/format/tc_format_2.R
src/extra/testr/filtered-test-suite/cumsum/tc_cumsum_12.R
src/extra/testr/filtered-test-suite/attr/tc_attr_42.R
src/extra/testr/filtered-test-suite/asvector/tc_asvector_11.R
src/extra/testr/filtered-test-suite/names/tc_names_24.R
src/extra/testr/filtered-test-suite/attributes/tc_attributes_15.R
src/library/utils/R/objects.R
src/extra/testr/filtered-test-suite/length/tc_length_11.R
src/extra/testr/filtered-test-suite/sample/tc_sample_1.R
src/extra/testr/filtered-test-suite/cosh/tc_cosh_1.R
src/extra/testr/filtered-test-suite/deparse/tc_deparse_23.R
src/extra/testr/R/S4.R
tests/testthat/test-sas.R
examples/tm_raster.R
R/run_zinba.R
R/plot-tweets.R
inst/examples/plotMaps_usingExtractMethod.R
helper/R/mxVtDep.R
src/library/datasets/data/esoph.R
src/library/methods/tests/duplicateClass.R
conf/tables/r/subsetSite.R
R/meta.R
books/Gelman_BDA2_ARM/doc/arm2/pictures/logistic.R
R/merSubstEff.R
tests/extratests/mplus/mplusFunctions.R
R/ga-api-coerce.R
Mage/R/plotLifeCycle.R
examples/sim.setting.a.R
src/library/base/R/funprog.R
R/dataBench.R
R/data.R
scripts/manuscripts/manuscript_1/variance_partitioning_script_burnin2.R
Statistical_Inference/Expectations/normalMeans.R
R/zzz.R
src/library/stats/R/acf.R
src/L0_gbm41/model_parameters.R
R/order-book-reconstruction.R
sparkr-interpreter/src/main/resources/R/pkg/inst/tests/test_includePackage.R
util/R/get_Poisson_conf_intervals.R
basic/software.R
src/extra/testr/filtered-test-suite/paste/tc_paste_7.R
src/extra/testr/filtered-test-suite/match/tc_match_18.R
R/repositories.R
src/library/base/R/builtins.R
tests/testthat/projects/overwritten_controller/example/config/application.R
R/utils.R
Biology/DoobGillespieBirthsDensityDependent.R
r_modules/dataPackages/DEMOdz/inst/import/networks/mut/go.R
src/library/graphics/R/hist.R
tests/testthat/test-cnSpec_qual.R
lib/pegasus/r/Pegasus/DAX/tests/testthat/test_executable.R
demo/lda_fgs_acgs_ppc_R.R
82.R
R/bootstrap-FDR.R
src/library/grid/R/layout.R
R/checkDays.R
src/LDA-RLFM/R/model/Gibbs_LDA.R
books/Gelman_BDA2_ARM/doc/Book_Codes/Ch.2/2.3_CI.R
src/library/utils/R/package.skeleton.R
jobs/dream4_bbsr_noprior.R
R-src/src/library/base/R/sample.R
aws-blog-sparkr-geospatial-analysis/Spark-Redshift.R
src/library/grDevices/R/windows/cairo.R
src/library/datasets/data/DNase.R
R-src/src/library/tools/R/packages.R
Financial-Risk-Modelling-and-Portfolio-Optimization-with-R/source-code/Part3Chapter2Ex1.R
plot/plot_distribution.R
 dev_tools/package_dev.R
Coursera Statistics Princeton/stats1_supplemental_Stats1.13.Lab.06.R
PracticalML/quiz3.R
R/tests/testdir_jira/runit_hex_1484_rf_ls.R
R/tests/testdir_munging/slice/runit_rbind.R
R/tests/Utils/shared_javapredict_DL.R
h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_poisson_offset.R
h2o-r/tests/testdir_algos/gbm/runit_GBM_groupsplit_smallcat.R
R/install_h2o.R
R/is_standardUnit.R
data-raw/Hunting.R
src/library/datasets/data/islands.R
src/library/base/R/findInt.R
inst/models/passing/MxEvalDefinitionVar.R
Query:
SELECT DISTINCT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.ipynb'
   LIMIT 100;
Answer:
path
exercises/06-Titanic_cross_validation.ipynb
bcomposes-examples/a01-hidden-notebook.ipynb
BinPy/examples/notebook/connectors/Linker.ipynb
docs/geoms/geom_jitter.ipynb
BatchImporter/InvertedPendulum/InvertedPendulum_SimulinkControl.ipynb
nglview/tests/notebooks/closest_waters.ipynb
nglview/tests/notebooks/test_camera.ipynb
tutorials/tutorial-2-iges.ipynb
notebooks/IntroPython.ipynb
2015_Fall/MATH-578B/Homework10/Homework 3.ipynb
tutorials/new_data_iso.ipynb
doc/Tutorials/Exploring_Data.ipynb
code/webapp/code/music_note_cloud/cloudmaker.ipynb
nbgrader/docs/source/user_guide/release/ps1/problem2.ipynb
examples/.ipynb_checkpoints/compare_kernelKmenas_vs_kmeans-checkpoint.ipynb
notebooks/.ipynb_checkpoints/ntf_movie_lens-checkpoint.ipynb
pylearn2/scripts/tutorials/softmax_regression/softmax_regression.ipynb
examples/axon_patterns.ipynb
python_interface/ipython_notebooks/FlandmarkPythonInterfaceExample.ipynb
ipynb/module2_v1.0.ipynb
notebooks/1. Introduction to PyMC3.ipynb
doc/ipython-notebooks/classification/HashedDocDotFeatures.ipynb
doc/timeout.ipynb
hash-collision-prob.ipynb
contrib/roary_plots/roary_plots.ipynb
tensorflow-tutorials/01_basics.ipynb
examples/gradev-demo.ipynb
example/pandas_validator_example_ja.ipynb
slides/commit plots.ipynb
ar1_pymc.ipynb
introduction-to-big-data-with-apache-spark/lab-4/lab4_machine_learning_student.ipynb
notebooks/1_Introduction/helloworld.ipynb
examples/.ipynb_checkpoints/remove_distortion_from_edi-checkpoint.ipynb
tensorflow/examples/udacity/4_convolutions.ipynb
Python/Blaze/blaze_test.ipynb
code/08.sklearn_stock_market.ipynb
python/ipnb/testData/outputs.ipynb
notebooks/.ipynb_checkpoints/Oscillator Regimes-checkpoint.ipynb
doc/TypeCheck.ipynb
doc/pub/mcint/ipynb/mcint.ipynb
notebooks/wradlib_workflow.ipynb
demos_ch2/demo2_4.ipynb
OCR_Example.ipynb
notebooks/02-IntroMachineLearning_update.ipynb
"4 - pandas Basics/4-7 pandas DataFrame Summary Statistics, Filtering, Dropping and adding Rows and Columns, Grouping Basics.ipynb"
chapters/statistics/notebooks/Convergence.ipynb
chapters/statistics/notebooks/maximum_posteriori.ipynb
BinPy/examples/notebook/algorithms/expression_convert_example.ipynb
h2o-docs/src/booklets/v2_2015/source/DeepLearning_Vignette.ipynb
docs/examples/Adjusting Shapes.ipynb
BatchImporter/Introduction/Introduction_ControlPID.ipynb
docs/intro.ipynb
examples/notebook/Introduction.ipynb
develop/Simple%20Demo.ipynb
Logica Proposicional/Logica proposicional.ipynb
docs/old/RCall.ipynb
deepthought/experiments/ismir2014/plots/.ipynb_checkpoints/cube_plot-checkpoint.ipynb
tlg/Parse AUTHTAB.DIR.ipynb
data/examples/python2.7/Markdown.ipynb
demos_ch3/demo3_6.ipynb
notebooks/delete repos.ipynb
doc/ipython-notebooks/gaussian_process/variational_classifier.ipynb
notebook/.ipynb_checkpoints/ex4_layer-checkpoint.ipynb
deliver/02-Visualization-and-code-organization.ipynb
notebooks/jupyter-examples/6. Exercises.ipynb
lqcontrol/lqcontrol_solutions_jl.ipynb
Lecture-15/2.Topic-Modeling.ipynb
notebooks/profiling/fst.ipynb
powerline/doc/n_day_var.ipynb
docs/tutorial/closer_look_at_viz.ipynb
bugreport/boxplot_notch/boxplot_notch.ipynb
examples/pq-dbquantity.ipynb
blog-part-3.ipynb
tests/notebook/nbformat3/Index.ipynb
All.ipynb
docs/notebooks/4-Create-model.ipynb
notebooks/scratch/onlineai.ipynb
11_hyperbolic-2.ipynb
Exercises-4.ipynb
algorithms-in-C++/data-structures_03_queues.ipynb
Extra/ELGMarkov.ipynb
introduction-to-big-data-with-apache-spark/solutions/lab1_word_count_student.ipynb
examples/ipython/alanine_dipeptide_mstis_analysis.ipynb
Examples/CesiumWidget Example KML.ipynb
lectures/lec_04.ipynb
script/SGEMM_NT/explore-n-lws.ipynb
Query:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
     ON c."id" = f."id"
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if has
Query:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
INNER JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  ON c."sample_path" = f."path" AND c."sample_repo_name" = f."repo_name"
LIMIT 100;
Answer:
Too long, hard cut:
content
""
"Awful TV show. Cheap cheesy effects and the dumbest most repetitive plots in all of TVdom. Can't hold a candle to any of the movie versions all of which are far, far better. Perhaps the only draw of this show is the fact that in the first season it featured a little bit of nudity. Nice gimmick, didn't ABC do that years ago?"
".. _install:

############
Installation
############

Ce chapitre décrit les différentes étapes de mise en place d'une plate-forme de supervision basée sur Centreon.

La plate-forme de supervision peut-être installée de plusieurs manières. Cependant, **nous vous recommandons vivement d'utiliser Centreon ISO (ex CES) pour installer votre plate-forme**. Profitez ainsi de nos travaux d'industrialisation de l'installation et de la mise à jour de l'environnement. Profitez également des optimisations installées en standard par le système.
L'installation de Centreon peut être effectuée à partir des sources (tar.gz) mais le travail est plus complexe. De plus l'installeur ne sera supporté que par la communauté.

Avant toute installation, veillez à bien suivre les pré-requis d'installation et de dimensionnement (ressources CPU, mémoire, disques, partitionnement, etc...). Prenez également soin de bien choisir le type d'architecture qu'il convient de mettre en place pour vos besoins. 
Enfin, vous pourrez procéder à l'installation de la plate-forme.

.. toctree::
   :maxdepth: 2

   prerequisites
   architecture/index
   installation/index
   downloads
   01a
   from_packages
   from_sources
"
"# Emercoin Instance

This Microsoft Azure template deploys a single Emercoin client which will connect to the public Emercoin network.

[![Deploy to Azure](http://azuredeploy.net/deploybutton.png)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2Femercoin-ubuntu%2Fazuredeploy.json)
<a href=""http://armviz.io/#/?load=https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2Fazure-quickstart-templates%2Fmaster%2Femercoin-ubuntu%2Fazuredeploy.json"" target=""_blank"">
    <img src=""http://armviz.io/visualizebutton.png""/>
</a>

Once your deployment is complete you will be able to connect to the Emercoin public network.

![Emercoin-Azure](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/emercoin-ubuntu/images/emercoin.png)

# Template Parameters
When you launch the installation of the VM, you need to specify the following parameters:
* `vmDnsPrefix`: this is the public DNS name for the VM that you will use interact with your console. You just need to specify an unique name.
* `adminUsername`: self-explanatory. This is the account you will use for connecting to the node
* `adminPassword`: self-explanatory. Be aware that Azure requires passwords to have One upper case, one lower case, a special character, and a number
* `vmSize`: The type of VM that you want to use for the node. The default size is D1_v2 but you can change that if you expect to run workloads that require more RAM or CPU resources.

# Emercoin Deployment Walkthrough
1. Get your node's IP
 1. browse to https://portal.azure.com

 2. then click browse all, followed by ""resource groups"", and choose your resource group

 3. then expand your resources, and public ip address of your node.

2. Connect to your node
 1. SSH to the public ip of your node as the user you specified for `adminUsername`, enter your `adminPassword`
 2. Try to use the cli-client by `emc help` or `emc getinfo`
 3. Point your browser to the public ip of your node, sign in with `adminUsername` and `adminPassword` specified before (note that browser may show you a warning of bad certificate - it's OK, you may replace the self-signed certificates by yours at /etc/ssl/emc/emcweb*)
"
"b3e.editor.ShortcutManager = function(editor) {
  ""use strict"";

  this._applySettings = function(settings) {};
};"
""
"@ECHO OFF

REM Command file for Sphinx documentation

if ""%SPHINXBUILD%"" == """" (
	set SPHINXBUILD=sphinx-build
)
set BUILDDIR=build
set SRCDIR=src
set ALLSPHINXOPTS=-d %BUILDDIR%/doctrees %SPHINXOPTS% %SRCDIR%
set I18NSPHINXOPTS=%SPHINXOPTS% %SRCDIR%
if NOT ""%PAPER%"" == """" (
	set ALLSPHINXOPTS=-D latex_paper_size=%PAPER% %ALLSPHINXOPTS%
	set I18NSPHINXOPTS=-D latex_paper_size=%PAPER% %I18NSPHINXOPTS%
)

if ""%1"" == """" goto help

if ""%1"" == ""help"" (
	:help
	echo.Please use `make ^<target^>` where ^<target^> is one of
	echo.  html       to make standalone HTML files
	echo.  dirhtml    to make HTML files named index.html in directories
	echo.  singlehtml to make a single large HTML file
	echo.  pickle     to make pickle files
	echo.  json       to make JSON files
	echo.  htmlhelp   to make HTML files and a HTML help project
	echo.  qthelp     to make HTML files and a qthelp project
	echo.  devhelp    to make HTML files and a Devhelp project
	echo.  epub       to make an epub
	echo.  latex      to make LaTeX files, you can 
Query:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%'
LIMIT 100;
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"cmake_minimum_required(VERSION 2.8.3)
project(socketcan_interface)

find_package(catkin REQUIRED
  COMPONENTS
    class_loader
)

find_package(Boost REQUIRED
  COMPONENTS
    chrono
    system
    thread
)

catkin_package(
  INCLUDE_DIRS
    include
  LIBRARIES
    socketcan_interface_string
  CATKIN_DEPENDS
  DEPENDS
    Boost
)

include_directories(
  include
  ${Boost_INCLUDE_DIRS}
  ${catkin_INCLUDE_DIRS}
)

# socketcan_interface_string
add_library(socketcan_interface_string
    src/string.cpp
)

# socketcan_dump
add_executable(socketcan_dump
  src/candump.cpp
)
target_link_libraries(socketcan_dump
   socketcan_interface_string
   ${catkin_LIBRARIES}
   ${Boost_LIBRARIES}
)

# socketcan_interface_plugin
add_library(socketcan_interface_plugin
  src/socketcan_interface_plugin.cpp
)
target_link_libraries(socketcan_interface_plugin
  ${catkin_LIBRARIES}
  ${Boost_LIBRARIES}
)

install(
  TARGETS
    socketcan_dump
    socketcan_interface_plugin
    socketcan_interface_string
  ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
  LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
  RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
)

install(
  DIRECTORY
    include/${PROJECT_NAME}/
  DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}
  FILES_MATCHING PATTERN ""*.h""
)

install(
  FILES
    socketcan_interface_plugin.xml
  DESTINATION ${CATKIN_PACKAGE_SHARE_DESTINATION}
)
if(CATKIN_ENABLE_TESTING)

  catkin_add_gtest(${PROJECT_NAME}-test_dummy_interface
    test/test_dummy_interface.cpp
  )
  target_link_libraries(${PROJECT_NAME}-test_dummy_interface
    socketcan_interface_string
    ${catkin_LIBRARIES}
  )
endif()
"
"// Includes
#include ""stdafx.h""
#include ""Synergy Editor.h""
#include ""ChildFrm.h""
#include ""Doc.h""
#include ""View.h""
#include ""Help.h""
#include ""HTMLDocument.h""
#include ""Keywords.h""
#include ""Utilities.h""
#include ""Settings.h""
#include ""ReadProject.h""
#include ""afxwin.h""
#include ""Variables.h""
#include ""direct.h""
#include ""io.h""
//#include ""adpcore.h""

// Globals
int g_uTriggerIntroOn = 0;
bool g_bIsNETBOOKVersion = false;

// Prototype for associated GETDXVER.CPP (or empty true in MAIN.CPP if compiler)
DWORD g_dwDirectXVersion = 0;
TCHAR g_strDirectXVersion[10] = { 0 };
HRESULT GetDXVersion( DWORD* pdwDirectXVersion, TCHAR* strDirectXVersion, int cchDirectXVersion );

// Message Pump
BEGIN_MESSAGE_MAP(App, CWinApp)
	ON_COMMAND(ID_APP_ABOUT, OnAppAbout)
	ON_COMMAND_EX_RANGE(ID_FILE_MRU_FILE1, ID_FILE_MRU_FILE16, OnOpenRecentFile)
	// Standard file based document commands
	ON_COMMAND(ID_FILE_NEW, CWinApp::OnFileNew)
	ON_COMMAND(ID_FILE_OPEN, CWinApp::OnFileOpen)
	// Standard print setup command
	ON_COMMAND(ID_FILE_PRINT_SETUP, CWinApp::OnFilePrintSetup)
	ON_COMMAND(ID_HELP_DARKBASICPROFESSIONALHELP, OnHelp)
END_MESSAGE_MAP()

// App construction
App::App() :
	CBCGPWorkspace (TRUE /* m_bResourceSmartUpdate */)
{
	// TODO: add construction code here,
	// Place all significant initialization in InitInstance
}

// The one and only App object
App theApp;

// App initialization
BOOL App::InitInstance()
{
	/*
	// U75 - 171109 - check if special NETBOOK version
	g_bIsNETBOOKVersion = false;
	char pOldDir [ _MAX_PATH ];
	_getcwd ( pOldDir, _MAX_PATH );
	createdSettings = Settings::LoadSettings();
	CString pCertFolder = Settings::DBPLocation + L""Compiler\\certificates\\"";
	SetCurrentDirectory ( pCertFolder );
	_finddata_t filedata;
	long hInternalFile = _findfirst(""*.*"", &filedata);
	if(hInternalFile!=-1L)
	{
		int FileReturnValue = 0;
		bool bContainsDBPString = false;
		while ( FileReturnValue==0 && g_bIsNETBOOKVersion==false )
		{
			if( !(filedata.attrib & _A_SUBDIR) )
			{
				char pFilename[256];
				strcpy ( pFilename, filedata.name );
				if ( pFilename )
					if ( stricmp ( pFilename, ""a3330fee1fd9f540f18ee37c331c46ea"" )==NULL )
						g_bIsNETBOOKVersion = true;
			}
			FileReturnValue = _findnext(hInternalFile, &filedata);
		}
		_findclose(hInternalFile);
		hInternalFile=NU
Query:
-- Corrected SQL 2
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py' AND c."content" ILIKE '%import %'
LIMIT 100
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------
Query:
-- Corrected SQL 3
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.R' AND c."content" ILIKE '%library(%'
LIMIT 100
Answer:
Too long, hard cut:
content
"library(raster)
library(data.table)
library(microbenchmark)
library(SpaDES)

N=1e4
sim <- new.env(parent = .GlobalEnv)
sim <- new.env()
sim$landscapeDT = data.table(test=1:(N^2), hello = sample(1:(N^2),(N^2)))
sim$ras <- raster(x=extent(0,N,0,N), res=1)
sim$ras[] = sim$landscapeDT$test


landscapeDT = copy(sim$landscapeDT)
ras <- raster(x=extent(0,N,0,N), res=1)
ras[] = -sim$landscapeDT$test

vec <- getValues(ras)

ras2 <- 1:10

# Note return invisible empty ... means no copy is made. The object is actually modified in place
#  as long as it is the same name, i.e., sim, as the environment name
changeValDT  <- function(sim, index, val) {
  sim$landscapeDT[index,hello:=val]
  invisible()
}

changeValDTNoSim  <- function(sim, index, val) {
  landscapeDT[index,hello:=val]
  invisible()
}

changeValRas  <- function(sim, index, val) {
  sim$ras[index]  <- val
  invisible()
}

index=sample(N^2, 5); print(index)

# Note - exact same time, meaning no copying is happening
microbenchmark(times=2000,
               new=changeValDT(sim, index=index, val=index),
               old=#{landscape <- getGlobal(""landscape""); 
                landscapeDT[index,hello:=index],
               alt=changeValDTNoSim(sim, index=index, val=index)
               # assignGlobal(""landscape"")}
               )
print(sim$landscapeDT[index,])
print(landscapeDT[index,])
print(landscapeDT[index,])

# Note - exact same time, meaning no copying is happening
microbenchmark(times=10,
               changeValRas(sim, index, index),
               ras[index]<-index,
               vec[index] <- index)

sim = new.env()
microbenchmark(times=100,
               makeEnv={sim$ras <- ras; sim$landscape <- landscape},
               makeList={sim2=list(ras, landscape)})

names(sim$ras) <- ""Fires""
setColors(sim$ras) <- c(""white"", rev(heat.colors(9)))
sim$ras <- setValues(sim$ras, 1)

"
"library(sptrans)
olhovivo_login()
"
"#!/usr/bin/env Rscript

library(edgeR)
library(getopt)

filterCountsFile <- function(inputFile, countsPerMillion, numSamples, outputFile) {
    counts <- read.table(inputFile, head=T, row.names=1)
    keep <- rowSums(cpm(counts)>countsPerMillion) >= numSamples
    output <- counts[keep,]
    write.table(output, outputFile,  quote=FALSE, sep=""\t"",
        row.names=TRUE, col.names=NA)
}

parseOptions <- function(optSpec) {
    opts <- getopt(optSpec)

    if (!is.null(opts$help)) {
        cat(getopt(optSpec, usage=TRUE))
        q(status=0)
    }

    required <- optSpec[optSpec[,3] == ""1"", 1]
    for (req in required) {
        if (is.null(opts[[req]])) {
            cat(paste(""Required option"", req, ""missing!\n""))
            q(status=1)
        }
    }

    opts
}

optSpec = matrix(c(
    ""help"", ""h"", 0, NA,
    ""input-file"", ""i"", 1, ""character"",
    ""counts-per-million"", ""c"", 1, ""integer"",
    ""num-samples"", ""n"", 1, ""integer"",
    ""output-file"", ""o"", 1, ""character""
    ), byrow=TRUE, ncol=4)

opts <- parseOptions(optSpec)

dge <- filterCountsFile(opts$`input-file`, opts$`counts-per-million`, opts$`num-samples`, opts$`output-file`)
"
"library(shogun)

fm_train_real <- t(as.matrix(read.table('../data/fm_train_real.dat')))
fm_test_real <- t(as.matrix(read.table('../data/fm_test_real.dat')))

# poly
print('Poly')

feats_train <- RealFeatures()
dummy <- feats_train$set_feature_matrix(fm_train_real)
feats_test <- RealFeatures()
dummy <- feats_test$set_feature_matrix(fm_test_real)
degree <- as.integer(4)
inhomogene <- FALSE

kernel <- PolyKernel(
	feats_train, feats_train, degree, inhomogene)

km_train <- kernel$get_kernel_matrix()
dump <- kernel$init(feats_train, feats_test)
km_test <- kernel$get_kernel_matrix()
"
"library(PLSR)
library(RUnit)
library(DEMOdz)
library(TCGAgbm)
Sys.setlocale(""LC_ALL"", ""C"")

if(!exists(""marker.genes.545"")){
    print(load(system.file(package=""TCGAgbm"", ""extdata"", ""genesets.RData"")))
    marker.genes.545 <- genesets$marker.genes.545
    tcga.GBM.classifiers <- genesets$tcga.GBM.classifiers
    }
#----------------------------------------------------------------------------------------------------
runTests <- function()
{
  test_constructor()
  test_summarizePatientNumericFactors()
  test_summarizePatientNumericFactors_TCGAgbm_poorPatientOverlap()
  test_summarizePatientNumericFactors_DEMOdz()
  test_createClassificationMatrix()
  test_createClassificationMatrix.1factor()
  test_createClassificationMatrix.2factors()
  test_createBigClassificationMatrix()
  test_calculateTiny.ageDxDemo()
  #test_calculateTiny.ageDxAndSurvival.demo()
  test_calculateSmall.ageDxOnly()
  test_calculateSmall.survivalOnly()
  test_calculateSmall()
  test_calculateBig()
  test_changeDataSetsAndCalculateSmall()

} # runTests
#----------------------------------------------------------------------------------------------------
test_constructor = function()
{
   printf(""--- test_constructor"")
   demoDz <- DEMOdz()
   mrna.datasets <- sort(grep(""mtx.mrna"", manifest(demoDz)$
Query:
-- Corrected SQL 4
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.ipynb' AND c."content" ILIKE '%import %'
LIMIT 100
Answer:
Too long, hard cut:
content
"{
 ""cells"": [
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""<!-- dom:TITLE: Introduction to Monte Carlo methods -->\n"",
    ""# Introduction to Monte Carlo methods\n"",
    ""<!-- dom:AUTHOR: Morten Hjorth-Jensen   Email morten.hjorth-jensen@fys.uio.no at Department of Physics and Center of Mathematics for Applications, University of Oslo & National Superconducting Cyclotron Laboratory, Michigan State University -->\n"",
    ""<!-- Author: --> **Morten Hjorth-Jensen   Email morten.hjorth-jensen@fys.uio.no**, Department of Physics and Center of Mathematics for Applications, University of Oslo and National Superconducting Cyclotron Laboratory, Michigan State University\n"",
    ""\n"",
    ""Date: **Fall 2015**\n"",
    ""\n"",
    ""## Monte Carlo methods, plan for the lectures\n"",
    ""1. First week: intro, MC integration and probability distribution functions (PDFs)\n"",
    ""\n"",
    ""2. Second week: More on integration, PDFs, MC integration and random walks.\n"",
    ""\n"",
    ""3. Third week: random walks and statistical physics.\n"",
    ""\n"",
    ""4. Fourth week: Statistical physics and the Ising and Potts models\n"",
    ""\n"",
    ""5. Fifth week: Most likely quantum Monte Carlo\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Monte Carlo: Enhances algorithmic thinking!\n"",
    ""* Be able to generate random variables following a given probability distribution function PDF\n"",
    ""\n"",
    ""* Find a probability distribution function (PDF)\n"",
    ""\n"",
    ""* Sampling rule for accepting a move\n"",
    ""\n"",
    ""* Compute standard deviation and other expectation values\n"",
    ""\n"",
    ""* Techniques for improving errors\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Domains and probabilities\n"",
    ""Consider the following simple example, namely the tossing of a dice, resulting in  the following possible values""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{2,3,4,5,6,7,8,9,10,11,12\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""These values are called the *domain*. \n"",
    ""To this domain we have the corresponding *probabilities*""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{1/36,2/36/3/36,4/36,5/36,6/36,5/36,4/36,3/36,2/36,1/36\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""## Monte Carlo methods, tossing a dice\n"",
    ""The numbers in the domain are the outcomes of the physical process tossing the dice.\n"",
    ""We cannot tell beforehand whether the outcome is 3 or 5 or any other number in this domain.\n"",
    ""This defines the randomness of the outcome, or unexpectedness or any other synonimous word which\n"",
    ""encompasses the uncertitude of the final outcome. \n"",
    ""\n"",
    ""The only thing we can tell beforehand\n"",
    ""is that say the outcome 2 has a certain probability.  \n"",
    ""If our favorite hobby is to  spend an hour every evening throwing dice and \n"",
    ""registering the sequence of outcomes, we will note that the numbers in the above domain""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{2,3,4,5,6,7,8,9,10,11,12\\},\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""appear in a random order. After 11 throws the results may look like""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""\\{10,8,6,3,6,9,11,8,12,4,5\\}.\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""## Stochastic variables\n"",
    ""\n"",
    ""**Random variables are characterized by a domain which contains all possible values that the random value may take. This domain has a corresponding PDF**.\n"",
    ""\n"",
    ""\n"",
    ""\n"",
    ""## Stochastic variables and the main concepts, the discrete case\n"",
    ""There are two main concepts associated with a stochastic variable. The\n"",
    ""*domain* is the set $\\mathbb D = \\{x\\}$ of all accessible values\n"",
    ""the variable can assume, so that $X \\in \\mathbb D$. An example of a\n"",
    ""discrete domain is the set of six different numbers that we may get by\n"",
    ""throwing of a dice, $x\\in\\{1,\\,2,\\,3,\\,4,\\,5,\\,6\\}$.\n"",
    ""\n"",
    ""The *probability distribution function (PDF)* is a function\n"",
    ""$p(x)$ on the domain which, in the discrete case, gives us the\n"",
    ""probability or relative frequency with which these values of $X$\n"",
    ""occur""
   ]
  },
  {
   ""cell_type"": ""markdown"",
   ""metadata"": {},
   ""source"": [
    ""$$\n"",
    ""p(x) = \\mathrm{Prob}(X=x).\n"",
    ""$$""
   ]
  },
  {
   ""cell_type"": ""markdown"
Query:
-- Corrected SQL 5
SELECT DISTINCT c."sample_repo_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
LIMIT 100
Answer:
sample_repo_name
cloudmatrix/esky
vmesel/GeoMath
lsaffre/lino
gunnery/gunnery
jvanasco/metadata_parser
poldrack/myconnectome
olhoneles/olhoneles
openstack/nova
Nymphet/sexinsex-crawler
D-L/SimpleBookMarks
grundgruen/powerline
omab/psa-recovery
n3ko1/WrapMap
google/grr
DarkFenX/Pyfa
rocky/python3-trepan
piiswrong/dec
openconnectome/m2g
RealTimeWeb/blockpy
funkandwagnalls/pythonpentest
pyQode/pyqode.core
exaile/exaile
openstack/monasca-agent
feuervogel/django-taggit-templatetags
hpcugent/hanythingondemand
wreckage/django-happenings
openstack/rack
alexforencich/python-ivi
udp/json-parser
maxcutler/python-wordpress-xmlrpc
ejeschke/ginga
LinuxCNC/linuxcnc
networkx/networkx
drvink/epanos
wmayner/pyphi
mbuesch/awlsim
espadrine/opera
tgrosinger/aenea-grammars
openstack/cinder
ceph/ceph-installer
croach/django-simple-rest
svpcom/hyperloglog
dropbox/changes
openstack/python-neutronclient
OpenBazaar/OpenBazaar-Server
bdoin/GCompris
hydralabs/pyamf
golismero/golismero
edx/edx-platform
mushorg/conpot
mila-udem/fuel
robweber/xbmcbackup
milkbikis/powerline-shell
rusthon/Rusthon
GeoNode/geonode
pazz/alot
KhronosGroup/COLLADA-CTS
jdapena/swftools
calmofthestorm/aenea
pyca/cryptography
ARM-software/workload-automation
ahmetcemturan/SFACT
Stanford-Online/edx-platform
crossbario/crossbarexamples
nsnam/ns-3-dev-git
apache/stratos
mne-tools/mne-python
PyTables/PyTables
j5shi/Thruster
mollstam/UnrealPy
pybuilder/pybuilder
cgstudiomap/cgstudiomap
cjones/madcow
rec/echomesh
biocore/qiime
viewfinderco/viewfinder
mdaniel/virtualbox-org-svn-vbox-trunk
SciTools/iris
openhatch/oh-mainline
OCA/l10n-spain
spulec/moto
Kitware/VTK
openfun/edx-platform
gem5/gem5
bugvm/bugvm-studio
chyyuu/ucore_os_lab
kovidgoyal/calibre
basicthinker/ThyNVM
goace/personal-file-sharing-center
grap/odoo-addons-misc
Ulauncher/Ulauncher
pegasus-isi/pegasus
openstack/keystone
SekoiaLab/Fastir_Collector
openstack/murano
harrymvr/absorbing-centrality
jacinda/pycon_library
MartinPyka/Parametric-Anatomical-Modeling
qdqmedia/wiggum
openstack/fuel-plugin-midonet
Query:
-- Corrected SQL 6
SELECT c."sample_repo_name", COUNT(*) AS "python_file_count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
GROUP BY c."sample_repo_name"
ORDER BY "python_file_count" DESC
LIMIT 100
Answer:
sample_repo_name,python_file_count
anhstudios/swganh,11
mollstam/UnrealPy,7
bugvm/bugvm-studio,5
Cadasta/cadasta-platform,4
JetBrains/intellij-community,4
qunaibit/zippy-mirror,4
rocky/python3-trepan,3
espadrine/opera,3
golismero/golismero,3
samabhi/pstHealth,3
yade/trunk,3
ArduPilot/MissionPlanner,3
dimagi/commcare-hq,3
dropbox/changes,2
Moguri/BlenderPanda,2
django/django-formtools,2
SekoiaLab/Fastir_Collector,2
mbuesch/awlsim,2
exaile/exaile,2
junalmeida/Sick-Beard,2
nuagenetworks/monolithe,2
linkedin/Zopkio,2
rizar/attention-lvcsr,2
MissionCriticalCloud/cosmic,2
openstack/rack,2
sassoftware/conary,2
aliyun/oss-ftp,2
h2oai/h2o-2,2
openhatch/oh-mainline,2
GoogleCloudPlatform/gcloud-python,2
openstack/nova,2
kret0s/gnuhealth-live,2
otsaloma/gaupol,2
SeleniumHQ/selenium,2
lsaffre/lino,2
DarkFenX/Pyfa,2
viewfinderco/viewfinder,2
getsentry/sentry,2
hazelcast/hazelcast-python-client,2
bokeh/bokeh,2
moesy/AWS-Lambda-ML-Microservice-Skeleton,2
MITRECND/crits,2
harmy/kbengine,2
zeromq/pyzmq,2
wummel/linkchecker,2
PacificBiosciences/pypeFLOW,2
ngageoint/scale,2
edx/edx-platform,2
thiago-silva/memetalk,2
IONISx/edx-platform,2
openstack/monasca-agent,2
OpenBazaar/OpenBazaar-Server,2
inasafe/inasafe,2
edx/ecommerce,2
django/django,2
autotest/autotest,2
Stanford-Online/edx-platform,2
Datera/cinder,2
hakobera/luigi-bigquery,2
cloudera/hue,2
cgstudiomap/cgstudiomap,2
openconnectome/m2g,2
openstack/vitrage,2
openstack/cinder,2
sfepy/sfepy,2
vlegoff/tsunami,2
rec/echomesh,2
biocore/qiime,2
feist/pcs,1
PySide/pyside2,1
Ulauncher/Ulauncher,1
nfco/netforce,1
sockeye44/instavpn,1
openstack/third-party-ci-tools,1
Yelp/fullerite,1
egaxegax/django-egaxegax,1
wmayner/pyphi,1
d-ai/Sourcepawn,1
marsyas/marsyas,1
Microsoft/PTVS,1
aut-sepanta/Sepanta3,1
gurock/testrail-api,1
piiswrong/dec,1
hpcugent/hanythingondemand,1
regreg/regreg,1
feuervogel/django-taggit-templatetags,1
oVirt/vdsm,1
iridium-browser/iridium-browser-ubuntu,1
mapzen/TileStache,1
drvink/epanos,1
LinuxCNC/linuxcnc,1
bq/bitbloq-offline,1
MyRobotLab/pyrobotlab,1
scrapinghub/splash,1
azatoth/scons,1
Flexget/Flexget,1
statsmodels/statsmodels,1
etetoolkit/ete,1
wreckage/django-happenings,1
RealTimeWeb/blockpy,1
Query:
-- Corrected SQL 7
SELECT c."size"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py'
LIMIT 100
Answer:
size
975
1395
1448
44812
3942
1636
184
285
6556
2189
3459
1445
17043
17923
2535
23052
2807
473
951
4259
577
226
24133
1739
26622
669
1320
3528
2610
1537
4499
2101
385
3835
1681
1937
12907
13943
638
27648
775
11355
1640
125539
240
4015
5311
761
590
18432
2147
14896
9381
5214
544
483
332
3519
451
2632
2736
8277
5111
3718
73117
7005
5349
8333
26113
2276
1057
2289
1138
1037
317
22903
317
203
2058
1654
2887
17115
836
430
22259
6933
478
3961
26343
1268
3114
3519
2506
807
545
1473
62274
3376
4641
41
Query:
-- Corrected SQL 8
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.py' AND c."content" ILIKE '%from % import %'
LIMIT 100
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------
Query:
-- Corrected SQL 9
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE c."sample_path" LIKE '%.R' AND c."content" ILIKE '%require(%'
LIMIT 100
Answer:
Too long, hard cut:
content
"setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$""f"")))
source(""../../../scripts/h2o-r-test-setup.R"")
### This tests observation weights in glm ######




test_weights_by_row_duplication <- function() {
  
  require(testthat)
  
  #create data
  print(""create synthetic data with a numeric outcome"")
  set.seed(45541)
  n <- 100  #Number of training observations (also nrow(test) = n)
  p <- 20  #Number of features
  x <- matrix(rnorm(n*p), n, p)
  y <- rnorm(n)
  
  # Training data with weights
  # draw some random weights ~ Poisson, add 'x1' weight col and y to df, hdf
  set.seed(1234)
  x1 <- rpois(n, rep(2, n)) + 1  #Random integer-valued (>=1) weights
  df <- data.frame(x, x1, y)  #design matrix with weight and outcome cols
  hdf <- as.h2o(df, destination_frame = ""hdf"")  #for h2o
  
  # Training data (weights == 1.0 with repeated rows instead of weights)
  rep_idxs <- unlist(sapply(1:n, function(i) rep(i, df$x1[i])))
  rdf <- df[rep_idxs,]  #repeat rows
  rdf$x1 <- 1  #set weights back to 1.0
  rhdf <- as.h2o(rdf, destination_frame = ""rhdf"")  #for h2o
  
  ## for glmnet
  #df <- as.matrix(df)
  #rdf <- as.matrix(rdf)
  
  # Test data
  set.seed(2641)
  newx <- matrix(rnorm(n*p), n, p)
  newy <- rnorm(n)
  
  x1 <- rep(1, n)
  valid1 <- data.frame(newx, x1, y = newy)
  val1 <- as.h2o(valid1, destination_frame = ""val1"")
  valid1 <- as.matrix(valid1)
  
  print(""build models with weights vs repeated rows with h2o and lambda!=0"")
  hh1 <- h2o.gbm(x = 1:20, y = ""y"", 
                 training_frame = hdf, 
                 validation_frame = val1, 
                 ntrees = 50, 
                 weights_column = ""x1"")
  hh2 <- h2o.gbm(x = 1:20, y = ""y"", 
                 training_frame = rhdf, 
                 validation_frame = val1,
                 ntrees = 50, 
                 weights_column = ""x1"")
  
  print(""compare results"")
  expect_equal(hh1@model$training_metrics@metrics$MSE, 
               hh2@model$training_metrics@metrics$MSE,
               tolerance = 1e-6)
  expect_equal(hh1@model$training_metrics@metrics$r2, 
               hh2@model$training_metrics@metrics$r2,
               tolerance = 1e-6)

  expect_equal(hh1@model$validation_metrics@metrics$MSE,
               hh2@model$validation_metrics@metrics$MSE,
               tolerance = 1e-6)
  expect_equal(hh1@model$validation_metrics@metrics$r2,
               hh2@model$validation_metrics@metrics$r2,
               tolerance = 1e-6)
  
    
  #predictions
  print(""compare predictions"")
  ph1 <- as.data.frame(h2o.predict(object = hh1, newdata = val1))
  ph2 <- as.data.frame(h2o.predict(object = hh2, newdata = val1))
  mse1 <- mean((ph1$predict - newy)^2)
  mse2 <- mean((ph2$predict - newy)^2)
  #expect_equal(mse1, mse2)  #1.49 - 1.46 == 0.0291
  
  
  
}


doTest(""GBM weight Test: GBM w/ weights test by row duplication"", test_weights_by_row_duplication)
"
"## This script checks whether the reads in each splitted file belong to the same barcode
## and whether the file name contains the corresponding sample id name.
## Note, this script is a memory hog. 
## It was used to check the idemp output for one of our miseq runs.
##source(""http://bioconductor.org/biocLite.R"")
##biocLite(""ShortRead"")
require(ShortRead)

#### Files original and decoded ####
barcodeFile=""barcode_sample.txt""
I1File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_I1_001.fastq.gz""
R1File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_R1_001.fastq.gz"" 
R2File=""~/projects/141104_M03249_0008_000000000-ABY6R/Data/Intensities/BaseCalls/Undetermined_S0_L001_R2_001.fastq.gz""
decodedFile=list.files(""./"", pattern=""*.fastq.gz$"")

#### Read in barcode table, sequence names and barcodes ####
firstLine=readLines(""barcode_sample.txt"",1)
barcodeTable=read.table(barcodeFile, header=grepl(""arcode"", firstLine))
### Read in seqquence names and barcodes ###
barcodeReads=readFastq(I1File)
I1Names=unname(sapply(as.character(id(barcodeReads)), function(x) (strsplit(x,"" "")[[1]][1] ) ) )
I1Barcodes=as.character(sread(barcodeReads))

#### Loop through decoded files ####
for( i in 1:length(decodedFile) ) {
  cat(i, "" of "", length(decodedFile), ""\n"")
  cat(decodedFile[i],""\n"")
  decodedReads=readFastq(decodedFile[i])
  seqNames=unname(sapply(as.character(id(decodedReads)), function(x) (strsplit(x,"" "")[[1]][1] ) ) )
  seqBarcodes=as.character(sread(decodedReads))
  
  idx=match(seqNames, I1Names)
  #table(I1Barcodes[idx])
  #sort(table(I1Barcodes[idx]), decreasing=T)
  bctable=sort(table(I1Barcodes[idx]), decreasing=T)
  if ( length(bctable)> 1000 ) {
    message(length(bctable), "" barcodes found out of "", length(seqNames), "" reads."") 
  }
  if ( length( grep(names(bctable)[1], barcodeTable[,1]) )==0 ) {
    message(""most frequent barcode "",names(bctable)[1], "" not matched."") 
    next
  }
  if ( length(bctable)> 1000 ) n
Query:
-- Corrected SQL 10
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
WHERE (c."sample_path" LIKE '%.py' OR c."sample_path" LIKE '%.ipynb' OR c."sample_path" LIKE '%.R')
  AND c."content" ILIKE '%import %'
LIMIT 100
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
# Copyright (C) 2015 mulhern <amulhern@redhat.com>

# This library is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation; either version 2.1 of the License, or (at your
# option) any later version.

# This library is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License
# for more details.

# You should have received a copy of the GNU Lesser General Public License
# along with this library; if not, write to the Free Software Foundation,
# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

""""""
    pyudev._ctypeslib
    =================

    Wrappers for libraries.

    .. moduleauthor::  mulhern  <amulhern@redhat.com>
""""""

from . import libc
from . import libudev
"
"# Copyright 2015 Cisco Systems, Inc.
# All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from neutronclient.common import extension


class PolicyProfile(extension.NeutronClientExtension):
    resource = 'policy_profile'
    resource_plural = '%ss' % resource
    object_path = '/%s' % resource_plural
    resource_path = '/%s/%%s' % resource_plural
    versions = ['2.0']
    allow_names = True


class PolicyProfileList(extension.ClientExtensionList, PolicyProfile):
    """"""List policy profiles that belong to a given tenant.""""""

    shell_command = 'cisco-policy-profile-list'
    list_columns = ['id', 'name']
    pagination_support = True
    sorting_support = True


class PolicyProfileShow(extension.ClientExtensionShow, PolicyProfile):
    """"""Show information of a given policy profile.""""""

    shell_command = 'cisco-policy-profile-show'
"
"# -*- coding: utf-8 -*-
# Copyright (c) 2015, Alex Grigorevskiy
# Licensed under the BSD 3-clause license (see LICENSE.txt)
""""""
Test module for state_space_main.py
""""""

import unittest
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

import GPy.models.state_space_setup as ss_setup
import GPy.models.state_space_main as ssm

def generate_x_points(points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates (sorted) points on the x axis.
    
    Input:
    ---------------------------
        points_num: int
            How many points to generate
        x_interval: tuple (a,b)
            On which interval to generate points
        random: bool
            Regular points or random
    
    Output:
    ---------------------------
        x_points: np.array
            Generated points
    """"""
    
    x_interval = np.asarray( x_interval )

    if random:
        x_points = np.random.rand(points_num) * ( x_interval[1] - x_interval[0] ) + x_interval[0]
        x_points = np.sort( x_points )
    else:
        x_points = np.linspace(x_interval[0], x_interval[1], num=points_num )        

    return x_points

def generate_sine_data(x_points=None, sin_period=2.0, sin_ampl=10.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates sinusoidal data.
    
    Input:
    --------------------------------
    
    x_points: np.array
        Previously generated X points
    sin_period: float
        Sine period    
    sin_ampl: float
        Sine amplitude
    noise_var: float 
        Gaussian noise variance added to the sine function
    plot: bool
        Whether to plot generated data
    
    (if x_points is None, the the following parameters are used to generate
    those. They are the same as in 'generate_x_points' function)        
    
    points_num: int
    
    x_interval: tuple (a,b)
    
    random: bool
    """"""    
    
    sin_function = lambda xx: sin_ampl * np.sin( 2*np.pi/sin_period * xx )
    
    if x_points is None:
        x_points = generate_x_points(points_num, x_interval, random)

    y_points = sin_function( x_points ) + np.random.randn( len(x_points) ) * np.sqrt(noise_var)

    if plot:
        pass
    
    return x_points, y_points
    
def generate_linear_data(x_points=None, tangent=2.0, add_term=1.0, noise_var=2.0,
                        plot = False, points_num=100, x_interval = (0, 20), random=True):
    """"""
    Function generates linear data.
    
    Input:
    -------
End Exploring Related Columns
Task: What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?

Please answer only one complete SQL in snowflake dialect in ```sql``` format.
Usage example: ```sql
SELECT "COLUMN_NAME" FROM DATABASE.SCHEMA.TABLE WHERE ... ``` (Adjust "DATABASE", "SCHEMA", and "TABLE" to match actual names, ensure all column names are enclosed in double quotations)
Follow the answer format like: Based on common usage patterns in Python, R, and IPython script files on GitHub, the second most frequently used module is likely `pandas`. Here is the answer in the requested format:

```csv
Module,Frequency
pandas,frequency:int
(Attention: answer in one row)
```

**Explanation:** `pandas` is a widely-used library for data manipulation and analysis in Python. It is frequently imported in scripts and notebooks across various repositories, especially those related to data science and machine learning..
Here are some useful tips for answering:
When performing a UNION operation on many tables, ensure that all table names are explicitly listed. Union first and then add condition and selection. e.g. SELECT "col1", "col2" FROM (TABLE1 UNION ALL TABLE2) WHERE ...; Don't write sqls as (SELECT col1, col2 FROM TABLE1 WHERE ...) UNION ALL (SELECT col1, col2 FROM TABLE2 WHERE ...); Don't use ['-- Include all', '-- Omit', '-- Continue', '-- Union all', '-- ...', '-- List all', '-- Replace this', '-- Each table', '-- Add other'] to omit any table. Table names here: ({database name: {schema name: {table name}}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_FILES', 'SAMPLE_REPOS']}}

For string-matching scenarios, if the string is decided, don't use fuzzy query, and avoid using REGEXP. e.g. Get the object's title contains the word "book"
However, if the string is not decided, you may use fuzzy query and ignore upper or lower case. e.g. Get articles that mention "education".
When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.
When using ORDER BY, if there are duplicate values in the primary sort column, sort by an additional column as a secondary criterion.Keep all decimals to four decimal places.
Be careful of information in nested columns. e.g. When it comes to completed purchase, `hits.eCommerceAction.action_type` Indicates the type of ecommerce action and '6' represents completed purchases.
Be careful one country may have different country_name and country_region in different columns in a database.
Don't be misled by examples. For instance, a question related to Android development on StackOverflow might include tags like 'android-layout,' 'android-activity,' or 'android-intent.' However, you should not limit your analysis to just these three tags; instead, consider all tags related to Android: "tags" LIKE '%android%'.

2025-01-23 03:46:02 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:46:02 - root - INFO - Based on common usage patterns in Python and R scripts on GitHub, the second most frequently used module is likely `numpy`. Here is the answer as per the requested format:

```sql
WITH py_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.py'
),
py_lines AS (
    SELECT value AS line
    FROM py_contents,
    LATERAL FLATTEN(input => lines)
),
py_modules AS (
    SELECT
        COALESCE(
            REGEXP_SUBSTR(line, '^\\s*from\\s+([\\w\\.]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(line, '^\\s*import\\s+([\\w\\.]+)', 1, 1, 'i', 1)
        ) AS module_name
    FROM py_lines
    WHERE line ILIKE '%import%'
),
extracted_py_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM py_modules
    WHERE module_name IS NOT NULL
),
r_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.R'
),
r_lines AS (
    SELECT value AS line
    FROM r_contents,
    LATERAL FLATTEN(input => lines)
),
r_modules AS (
    SELECT
        REGEXP_SUBSTR(line, '^\\s*(?:library|require)\\(\\s*([\\w\\.]+)\\s*\\)', 1, 1, 'i', 1) AS module_name
    FROM r_lines
    WHERE line ILIKE '%library(%' OR line ILIKE '%require(%'
),
extracted_r_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM r_modules
    WHERE module_name IS NOT NULL
),
all_modules AS (
    SELECT module_name FROM extracted_py_modules
    UNION ALL
    SELECT module_name FROM extracted_r_modules
)
SELECT 'Module,Frequency'
UNION ALL
SELECT module_name || ',' || frequency::VARCHAR
FROM (
    SELECT module_name, COUNT(*) AS frequency,
           ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS rn
    FROM all_modules
    GROUP BY module_name
)
WHERE rn = 2;
```

This query extracts module names from Python `.py` files and R `.R` files by parsing the `import`, `from`, `library()`, and `require()` statements. It then counts the occurrences of each module and selects the second most frequently used one.

**Explanation:**
- The query starts by selecting the content of all `.py` and `.R` files from the `SAMPLE_CONTENTS` table.
- It splits the content into individual lines using `SPLIT(c."content", '\n')`.
- For Python files, it extracts module names from lines that contain `import` or `from` statements using regular expressions.
- For R files, it extracts module names from lines that contain `library()` or `require()` statements.
- It combines the module names from both languages and counts their occurrences.
- Finally, it selects the module with the second highest frequency.

---

**Note:** The actual second most frequently imported module may vary depending on the dataset contents. The assumption here is based on common patterns where `numpy` is a widely used module in Python.
2025-01-23 03:46:02 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:46:02 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:46:02 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:46:04 - snowflake.connector.cursor - INFO - Number of results in first chunk: 2
2025-01-23 03:46:04 - snowflake.connector.connection - INFO - closed
2025-01-23 03:46:04 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:46:05 - root - INFO - itercount: 1
2025-01-23 03:46:05 - root - INFO - 0
2025-01-23 03:46:05 - root - INFO - results: 
"'MODULE,FREQUENCY'"
"maec,1"
"Module,Frequency"


2025-01-23 03:46:05 - root - INFO - Please check the answer again by reviewing What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?, reviewing Relevant Tables and Columns and Possible Conditions and then give the final SQL query. Don't output other queries. If you think the answer is right, just output the current SQL.
Keep all decimals to four decimal places.
The answer format should be like: Based on common usage patterns in Python, R, and IPython script files on GitHub, the second most frequently used module is likely `pandas`. Here is the answer in the requested format:

```csv
Module,Frequency
pandas,frequency:int
(Attention: answer in one row)
```

**Explanation:** `pandas` is a widely-used library for data manipulation and analysis in Python. It is frequently imported in scripts and notebooks across various repositories, especially those related to data science and machine learning. The answer should match the number of rows, the column name of the format and the filled values in the format (e.g. filled year or month). Don't output extra rows or nested rows!
Current snswer: 
"'MODULE,FREQUENCY'"
"maec,1"
"Module,Frequency"
Current sql:
WITH py_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.py'
),
py_lines AS (
    SELECT value AS line
    FROM py_contents,
    LATERAL FLATTEN(input => lines)
),
py_modules AS (
    SELECT
        COALESCE(
            REGEXP_SUBSTR(line, '^\\s*from\\s+([\\w\\.]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(line, '^\\s*import\\s+([\\w\\.]+)', 1, 1, 'i', 1)
        ) AS module_name
    FROM py_lines
    WHERE line ILIKE '%import%'
),
extracted_py_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM py_modules
    WHERE module_name IS NOT NULL
),
r_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.R'
),
r_lines AS (
    SELECT value AS line
    FROM r_contents,
    LATERAL FLATTEN(input => lines)
),
r_modules AS (
    SELECT
        REGEXP_SUBSTR(line, '^\\s*(?:library|require)\\(\\s*([\\w\\.]+)\\s*\\)', 1, 1, 'i', 1) AS module_name
    FROM r_lines
    WHERE line ILIKE '%library(%' OR line ILIKE '%require(%'
),
extracted_r_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM r_modules
    WHERE module_name IS NOT NULL
),
all_modules AS (
    SELECT module_name FROM extracted_py_modules
    UNION ALL
    SELECT module_name FROM extracted_r_modules
)
SELECT 'Module,Frequency'
UNION ALL
SELECT module_name || ',' || frequency::VARCHAR
FROM (
    SELECT module_name, COUNT(*) AS frequency,
           ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS rn
    FROM all_modules
    GROUP BY module_name
)
WHERE rn = 2;When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.

2025-01-23 03:46:57 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:46:57 - root - INFO - ```sql
WITH py_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.py' OR f."path" LIKE '%.ipynb'
),
py_lines AS (
    SELECT value AS line
    FROM py_contents,
    LATERAL FLATTEN(input => lines)
),
py_modules AS (
    SELECT
        COALESCE(
            REGEXP_SUBSTR(line, '^\\s*from\\s+([\\w\\.]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(line, '^\\s*import\\s+([\\w\\.]+)', 1, 1, 'i', 1)
        ) AS module_name
    FROM py_lines
    WHERE line ILIKE '%import %'
),
extracted_py_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM py_modules
    WHERE module_name IS NOT NULL
),
r_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.R'
),
r_lines AS (
    SELECT value AS line
    FROM r_contents,
    LATERAL FLATTEN(input => lines)
),
r_modules AS (
    SELECT
        REGEXP_SUBSTR(line, '^\\s*(?:library|require)\\(\\s*([\\w\\.]+)', 1, 1, 'i', 1) AS module_name
    FROM r_lines
    WHERE line ILIKE '%library(%' OR line ILIKE '%require(%'
),
extracted_r_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM r_modules
    WHERE module_name IS NOT NULL
),
all_modules AS (
    SELECT module_name FROM extracted_py_modules
    UNION ALL
    SELECT module_name FROM extracted_r_modules
)
SELECT 'Module,Frequency'
UNION ALL
SELECT module_name || ',' || Frequency::VARCHAR
FROM (
    SELECT module_name, COUNT(*) AS Frequency,
        ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC NULLS LAST, module_name) AS rn
    FROM all_modules
    GROUP BY module_name
) sub
WHERE rn = 2;
```
2025-01-23 03:46:57 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:46:57 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:46:57 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:46:59 - snowflake.connector.cursor - INFO - Number of results in first chunk: 2
2025-01-23 03:46:59 - snowflake.connector.connection - INFO - closed
2025-01-23 03:46:59 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:47:00 - root - INFO - itercount: 2
2025-01-23 03:47:00 - root - INFO - 0
2025-01-23 03:47:00 - root - INFO - results: 
"'MODULE,FREQUENCY'"
"copy,1"
"Module,Frequency"


2025-01-23 03:47:00 - root - INFO - Please check the answer again by reviewing What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?, reviewing Relevant Tables and Columns and Possible Conditions and then give the final SQL query. Don't output other queries. If you think the answer is right, just output the current SQL.
Keep all decimals to four decimal places.
The answer format should be like: Based on common usage patterns in Python, R, and IPython script files on GitHub, the second most frequently used module is likely `pandas`. Here is the answer in the requested format:

```csv
Module,Frequency
pandas,frequency:int
(Attention: answer in one row)
```

**Explanation:** `pandas` is a widely-used library for data manipulation and analysis in Python. It is frequently imported in scripts and notebooks across various repositories, especially those related to data science and machine learning. The answer should match the number of rows, the column name of the format and the filled values in the format (e.g. filled year or month). Don't output extra rows or nested rows!
Current snswer: 
"'MODULE,FREQUENCY'"
"copy,1"
"Module,Frequency"
Current sql:
WITH py_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.py' OR f."path" LIKE '%.ipynb'
),
py_lines AS (
    SELECT value AS line
    FROM py_contents,
    LATERAL FLATTEN(input => lines)
),
py_modules AS (
    SELECT
        COALESCE(
            REGEXP_SUBSTR(line, '^\\s*from\\s+([\\w\\.]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(line, '^\\s*import\\s+([\\w\\.]+)', 1, 1, 'i', 1)
        ) AS module_name
    FROM py_lines
    WHERE line ILIKE '%import %'
),
extracted_py_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM py_modules
    WHERE module_name IS NOT NULL
),
r_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.R'
),
r_lines AS (
    SELECT value AS line
    FROM r_contents,
    LATERAL FLATTEN(input => lines)
),
r_modules AS (
    SELECT
        REGEXP_SUBSTR(line, '^\\s*(?:library|require)\\(\\s*([\\w\\.]+)', 1, 1, 'i', 1) AS module_name
    FROM r_lines
    WHERE line ILIKE '%library(%' OR line ILIKE '%require(%'
),
extracted_r_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM r_modules
    WHERE module_name IS NOT NULL
),
all_modules AS (
    SELECT module_name FROM extracted_py_modules
    UNION ALL
    SELECT module_name FROM extracted_r_modules
)
SELECT 'Module,Frequency'
UNION ALL
SELECT module_name || ',' || Frequency::VARCHAR
FROM (
    SELECT module_name, COUNT(*) AS Frequency,
        ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC NULLS LAST, module_name) AS rn
    FROM all_modules
    GROUP BY module_name
) sub
WHERE rn = 2;
2025-01-23 03:47:41 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-02-15-preview "HTTP/1.1 200 OK"
2025-01-23 03:47:41 - root - INFO - ```sql
WITH py_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.py' OR f."path" LIKE '%.ipynb'
),
py_lines AS (
    SELECT value AS line
    FROM py_contents,
    LATERAL FLATTEN(input => lines)
),
py_modules AS (
    SELECT
        COALESCE(
            REGEXP_SUBSTR(line, '^\\s*from\\s+([\\w\\.]+)', 1, 1, 'i', 1),
            REGEXP_SUBSTR(line, '^\\s*import\\s+([\\w\\.]+)', 1, 1, 'i', 1)
        ) AS module_name
    FROM py_lines
    WHERE line ILIKE '%import %'
),
extracted_py_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM py_modules
    WHERE module_name IS NOT NULL
),
r_contents AS (
    SELECT
        SPLIT(c."content", '\n') AS lines
    FROM
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    INNER JOIN
        "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE
        f."path" LIKE '%.R'
),
r_lines AS (
    SELECT value AS line
    FROM r_contents,
    LATERAL FLATTEN(input => lines)
),
r_modules AS (
    SELECT
        REGEXP_SUBSTR(line, '^\\s*(?:library|require)\\(\\s*([\\w\\.]+)', 1, 1, 'i', 1) AS module_name
    FROM r_lines
    WHERE line ILIKE '%library(%' OR line ILIKE '%require(%'
),
extracted_r_modules AS (
    SELECT LOWER(module_name) AS module_name
    FROM r_modules
    WHERE module_name IS NOT NULL
),
all_modules AS (
    SELECT module_name FROM extracted_py_modules
    UNION ALL
    SELECT module_name FROM extracted_r_modules
),
module_counts AS (
    SELECT module_name, COUNT(*) AS Frequency
    FROM all_modules
    GROUP BY module_name
),
ordered_modules AS (
    SELECT module_name, Frequency,
        ROW_NUMBER() OVER (ORDER BY Frequency DESC NULLS LAST, module_name) AS rn
    FROM module_counts
)
SELECT 'Module,Frequency'
UNION ALL
SELECT module_name || ',' || Frequency::VARCHAR
FROM ordered_modules
WHERE rn = 2;
```
2025-01-23 03:47:41 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-23 03:47:41 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-23 03:47:41 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-23 03:47:43 - snowflake.connector.cursor - INFO - Number of results in first chunk: 2
2025-01-23 03:47:43 - snowflake.connector.connection - INFO - closed
2025-01-23 03:47:43 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-23 03:47:43 - root - INFO - itercount: 3
2025-01-23 03:47:43 - root - INFO - 0
2025-01-23 03:47:43 - root - INFO - Total iteration counts: 3
