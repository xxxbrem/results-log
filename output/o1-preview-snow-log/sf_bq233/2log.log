2025-01-25 07:45:23 - openai._base_client - INFO - Retrying request to /chat/completions in 0.492978 seconds
2025-01-25 07:46:13 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:46:13 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:13 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:13 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:15 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-25 07:46:15 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:15 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:15 - root - INFO - SQL:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
Results:
Too long, hard cut:
path
fixtures/strict-par-success/fun1_to_proc_par2.ll
tests/failure/wrong_order_par_seq_middle.t/wrong_order_par_seq_middle.ll
fixtures/strict-par-success/layout_case.ll
fixtures/strict-par-success/merger_loli_Sort.ll
fixtures/failure/infer_recv.ll
fixtures/strict-par-success/parallel_assoc_tensor3_flat.ll
fixtures/strict-par-failure/ten_loli_par.ll
fixtures/compile/my_loli.ll
fixtures/failure/dead_lock_tensor2_tensor2.ll
fixtures/sequence/par_ten_ten_v1.ll
tests/failure/reusedParChannel.t/reusedParChannel.ll
Demo/Pods/Headers/Private/ReactiveCocoa/ReactiveCocoa/RACSerialDisposable.h
Demo/Pods/Headers/Private/ReactiveCocoa/UISlider+RACSignalSupport.h
libsel4/tools/xmllint.sh
themes/themes-available/Vautour/images/logo_thruk_mid.png
applications/solvers/incompressible/windEnergy/pisoFoamTurbine/Make/options
applications/solvers/incompressible/windEnergy/windPlantSolver/pEqn.H
cloudsqlip/Godeps/_workspace/src/github.com/prometheus/procfs/fixtures/26232/fd/1
examples/2D/two_dimensional_riemann_problems/kt-c03/off/OFF
test/zdtm/static/mntns_root_bind02.c
fastlane/metadata/fr-FR/marketing_url.txt
Pods/Headers/Private/CocoaLumberjack/DDContextFilterLogFormatter.h
Pods/Headers/Private/SVGKit/SVGKParserStyles.h
Pods/Headers/Public/SVGKit/SVGLength.h
Pods/Headers/Public/KSRefresh/UIScrollView+KS.h
Pods/Headers/Private/FMDB/FMDatabasePool.h
plugins/20_events/post-build-dockerfile
Pods/Headers/Public/KIF/CGGeometry-KIFAdditions.h
third_party/java/jetty/lib/jetty-servlet.jar
GYnews/Pods/Headers/Public/Masonry/MASConstraintMaker.h
test/05-diff/source/etc/symlink-modified.conf
examples/networking/neighbor_sharing/simulation.py
static/js/native.history.js
fast
deps/openssl/include/openssl/ui_compat.h
icons/CubLinux/actions/22/gnome-logout.png
icons/CubLinux/actions/16/window_new.png
icons/CubLinux/status/22/gpm-battery-040.png
icons/CubLinux/status/Scalable/notification-gpm-battery-020.svg
icons/CubLinux/status/48/battery_two_thirds.png
icons/CubLinux/places/128/gnome-fs-dav.png
icons/CubLinux/actions/22/rhythmbox-set-star.png
icons/CubLinux/places/128/gnome-fs-smb.png
icons/CubLinux/actions/22/gtk-find-and-replace.png
icons/CubLinux/actions/22/filesave.png
icons/CubLinux/actions/24/back.png
icons/chromiumos/cursors/sailboat
icons/CubLinux/status/96/ac-adapter.png
icons/CubLinux/stock/64/stock_person-panel.png
icons/CubLinux/places/96/folder-images.png
icons/CubLinux/actions/24/rotate.png
icons/CubLinux/status/22/battery-good-charging-symbolic.png
icons/CubLinux/status/128/notification-battery-020.png
icons/CubLinux/actions/16/stock_search.png
icons/CubLinux/actions/22/gtk-properties.png
icons/CubLinux/places/scalable/gnome-fs-directory-accept.svg
Debian Packages/chromiumos-cursor-theme/extract/usr/share/icons/chromiumos/cursors/hand1
rekaz/Zaker-C/Pods/Headers/Public/SDWebImage/SDWebImageManager.h
notanos/system/data/icons/buuf_watercolor/apps/kig.png
notanos/system/data/icons/buuf_watercolor/status/dialog-error.png
notanos/system/data/icons/buuf_watercolor/actions/edit-cut.png
notanos/system/data/icons/buuf_watercolor/actions/system-shutdown.png
notanos/system/data/icons/buuf_watercolor/devices/audio-input-microphone.png
PopMenu/Pods/Headers/Private/pop/POPAnimationRuntime.h
bin/yap_summary_call
test-apps/test-app-with-history-fallback/libs
build/prepare-commit-msg
man/std::wfstream::close.3
"man/std::u32string::rend,std::basic_string::crend.3"
"man/std::wstreambuf::pubimbue,std::basic_streambuf::imbue.3"
node_modules/html-webpack-plugin/node_modules/html-minifier/node_modules/.bin/uglifyjs
third_party/openssl/linux-x86_64/include/openssl/ecdsa.h
Pods/Headers/Private/FBSDKCoreKit/FBSDKUtility.h
Pods/Headers/Public/ParseTwitterUtils/PF_Twitter.h
Pods/Headers/Private/FBSDKCoreKit/FBSDKMutableCopying.h
Pods/Headers/Private/Parse/PFURLSessionDataTaskDelegate.h
Pods/Headers/Private/Parse/PFRESTObjectBatchCommand.h
Pods/Headers/Private/Parse/PFErrorUtilities.h
Pods/Headers/Private/Parse/PFPushUtilities.h
Pods/Headers/Private/ParseFacebookUtilsV4/PFFacebookUtils.h
Pods/Headers/Private/Parse/PFSession.h
Pods/Headers/Public/YYText/YYTextMagnifier.h
Pods/Headers/Private/MagicalRecord/NSPersistentStoreCoordinator+MagicalRecord.h
Library/ENV/4.3/g++-4.9
百思不得姐项目/Pods/Headers/Private/SVProgressHUD/SVRadialGradientLayer.h
DemoTests/Pods/Headers/Private/ZXingObjC/ZXMultiDetector.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceSubscribedSkuCollectionFetcher.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceReminder.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Private/MSGraph-SDK-iOS/MSGraphServiceAssignedLicense.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDirectoryRoleOperations.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDevice.h
S

2025-01-25 07:46:15 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:15 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:15 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:16 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-25 07:46:16 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:17 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:17 - root - INFO - SQL:
SELECT COUNT(*) AS "total_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES";
Results:
total_files
524077

2025-01-25 07:46:17 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:17 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:17 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:18 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-25 07:46:18 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:18 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:19 - root - INFO - SQL:
SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path") + 1) AS "extension"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
Results:
extension
ll
t/wrong_order_par_seq_middle.ll
t/reusedParChannel.ll
sh
conf
icons/chromiumos/cursors/sailboat
test-apps/test-app-with-history-fallback/libs
3/g++-4.9
iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceSubscribedSkuCollectionFetcher.h
iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDevice.h
iOS.Swift.SendMail/Pods/Headers/Private/MSGraph-SDK-iOS/MSGraphServiceConversationCollectionFetcher.h
0.2d/include/openssl/pqueue.h
benchmarks/svm_on_grid/random_hyperopt_august2013_mod
ico
core/dslmcode/stacks/blog/sites/all/libraries/jmol
bin/jslint
example/share/templates
framework/Versions/Current
tests/virhostcpudata/linux-subcores2/node/node16/cpu102
applications/solvers/incompressible/windEnergy/pisoFoamTurbine/Make/options
iOS.Objective-C.SendMail/Pods/Headers/Public/ADALiOS/ADLogger.h
php
domains/grades
core/dslmcode/stacks/innovate/sites/default
system/boot/ix86/vmxboot/suse-tumbleweed/root/include
dtsi
S
legacyLibraryProject/PlayerHater/res
md
Debian Packages/chromiumos-cursor-theme/extract/usr/share/icons/chromiumos/cursors/hand1
oasis.opendocument.draw.template.svg
0.1.jar.LICENSE
adoc
tests/ostree-grub-generator
3/g++-4.5
tests/module-loading/sim_encoder/num_chan=1/PIN_NAME_REGEX
arch/arm64/boot/dts/include/dt-bindings
src/libc/locale/locale/et_EE/LC_MONETARY
events/display-state-change/25_create_xineliboutput_config
chroot-bin/startkde
alternatives/traceproto
bin/os-name
1-DVD-i586-Build0091-textmode/.thumbs/installation_overview-1.png
tmp/vendor/@angular/compiler/src/output/output_interpreter.js
tmp/vendor/rxjs/Observable.js
data/licenses/container-interop
package/feeds/packages/libexif
package/feeds/management/libnetconf
Examples/CocosBuilderExample/libs/CocosDenshion
oasis.opendocument.presentation.svg
8.3-1.yaml.erb
0.1/mpn/toom_eval_pm1.c
9.32_eabi/gcc-linaro-4.5-2011.02-0-final/arm-openwrt-linux-uclibcgnueabi/libstdc++-v3/include/debug/safe_iterator.tcc
6/copy_reg.py
src/journal-remote/Makefile
src/system-update-generator/Makefile
svg
core/dslmcode/stacks/media/scripts
core/dslmcode/stacks/blog/sites/all/libraries/google-api-cors-upload
128x128
infer/lib/wrappers/clang++
bin/which
config
core/dslmcode/stacks/discuss/sites/all/libraries/chosen
BaragonUI/vendor/vexCss
erai-30km
INSTALL
stampede-agent/files/opt/virsh
src/libc/locale/locale/is_IS/LC_NUMERIC
src/libc/locale/locale/ja_JP/LC_CTYPE
osg
ive
2.12/lib/auth/.libs/libgnutls_auth.la
framework/Headers
3.2.jar
sun.xml.calc.png
1-DVD-i586-Build0091-kde/ooffice-5.png
xctemplate/Sleipnir.framework/Versions/Current
tmp/vendor/@angular/compiler/src/output/ts_emitter.js
tmp/vendor/@angular/core/src/application_tokens.js
tmp/vendor/angularfire2/providers/auth.js
tmp/vendor/@angular/common/src/forms.js
tmp/vendor/@angular/http/esm/src/backends/browser_jsonp.js
theme/Numix-Cursor/cursors/wait
Library/Aliases/boot2docker-cli
com/mattermost/rsc/tmp/google
deps/protobuf/protobuf
package/feeds/oldpackages/nuttcp
package/feeds/oldpackages/updatedd
vgtest
1-DVD-i586-Build0091-textmode/mtab-1.png
 15-UI-test/Pods/BuildHeaders/Expecta/EXPMatchers+beLessThanOrEqualTo.h
m
package/feeds/oldpackages/libdlna
de_DE.php
bin/har-validator
woff
1.txt
bin/mocha

2025-01-25 07:46:19 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:19 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:19 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:20 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-25 07:46:20 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:20 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:20 - root - INFO - SQL:
SELECT COUNT(*) AS "python_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.py';
Results:
python_files
8963

2025-01-25 07:46:20 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:20 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:20 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:22 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-25 07:46:22 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:22 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:22 - root - INFO - SQL:
SELECT COUNT(*) AS "r_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.r';
Results:
r_files
184

2025-01-25 07:46:22 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:22 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:22 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:24 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:46:24 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:24 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:46:24 - root - INFO - SQL:
SELECT sc."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
     ON sc."id" = sf."id"
   WHERE LOWER(sf."path") LIKE '%.py'
   LIMIT 100;
Results:
Too long, hard cut:
content
"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API repre

2025-01-25 07:46:24 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:46:24 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:46:24 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:46:26 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-25 07:46:26 - snowflake.connector.connection - INFO - closed
2025-01-25 07:46:26 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:47:12 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:47:12 - root - INFO - ```sql
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
LIMIT 100;
```
2025-01-25 07:47:12 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:47:12 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:47:12 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:47:14 - snowflake.connector.cursor - INFO - Number of results in first chunk: 11
2025-01-25 07:47:15 - snowflake.connector.connection - INFO - closed
2025-01-25 07:47:15 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:48:15 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:48:15 - root - INFO - SQL:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
LIMIT 100;
Results:
Too long, hard cut:
path,content
fs/xfs/xfs_itable.c,"/*
 * Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
 * All Rights Reserved.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write the Free Software Foundation,
 * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */
#include ""xfs.h""
#include ""xfs_fs.h""
#include ""xfs_types.h""
#include ""xfs_bit.h""
#include ""xfs_log.h""
#include ""xfs_inum.h""
#include ""xfs_trans.h""
#include ""xfs_sb.h""
#include ""xfs_ag.h""
#include ""xfs_mount.h""
#include ""xfs_bmap_btree.h""
#include ""xfs_alloc_btree.h""
#include ""xfs_ialloc_btree.h""
#include ""xfs_dinode.h""
#include ""xfs_inode.h""
#include ""xfs_ialloc.h""
#include ""xfs_itable.h""
#include ""xfs_error.h""
#include ""xfs_btree.h""
#include ""xfs_trace.h""

STATIC int
xfs_internal_inum(
	xfs_mount_t	*mp,
	xfs_ino_t	ino)
{
	return (ino == mp->m_sb.sb_rbmino || ino == mp->m_sb.sb_rsumino ||
		(xfs_sb_version_hasquota(&mp->m_sb) &&
		 (ino == mp->m_sb.sb_uquotino || ino == mp->m_sb.sb_gquotino)));
}

/*
 * Return stat information for one inode.
 * Return 0 if ok, else errno.
 */
int
xfs_bulkstat_one_int(
	struct xfs_mount	*mp,		/* mount point for filesystem */
	xfs_ino_t		ino,		/* inode to get data for */
	void __user		*buffer,	/* buffer to place output in */
	int			ubsize,		/* size of buffer */
	bulkstat_one_fmt_pf	formatter,	/* formatter, copy to user */
	int			*ubused,	/* bytes used by me */
	int			*stat)		/* BULKSTAT_RV_... */
{
	struct xfs_icdinode	*dic;		/* dinode core info pointer */
	struct xfs_inode	*ip;		/* incore inode pointer */
	struct xfs_bstat	*buf;		/* return buffer */
	int			error = 0;	/* error value */

	*stat = BULKSTAT_RV_NOTHING;

	if (!buffer || xfs_internal_inum(mp, ino))
		return XFS_ERROR(EINVAL);

	buf = kmem_alloc(sizeof(*buf), KM_SLEEP | KM_MAYFAIL);
	if (!buf)
		return XFS_ERROR(ENOMEM);

	error = xfs_iget(mp, NULL, ino,
			 (XFS_IGET_DONTCACHE | XFS_IGET_UNTRUSTED),
			 XFS_ILOCK_SHARED, &ip);
	if (error) {
		*stat = BULKSTAT_RV_NOTHING;
		goto out_free;
	}

	ASSERT(ip != NULL);
	ASSERT(ip->i_imap.im_blkno != 0);

	dic = &ip->i_d;

	/* xfs_iget returns the following without needing
	 * further change.
	 */
	buf->bs_nlink = dic->di_nlink;
	buf->bs_projid_lo = dic->di_projid_lo;
	buf->bs_projid_hi = dic->di_projid_hi;
	buf->bs_ino = ino;
	buf->bs_mode = dic->di_mode;
	buf->bs_uid = dic->di_uid;
	buf->bs_gid = dic->di_gid;
	buf->bs_size = dic->di_size;
	buf->bs_atime.tv_sec = dic->di_atime.t_sec;
	buf->bs_atime.tv_nsec = dic->di_atime.t_nsec;
	buf->bs_mtime.tv_sec = dic->di_mtime.t_sec;
	buf->bs_mtime.tv_nsec = dic->di_mtime.t_nsec;
	buf->bs_ctime.tv_sec = dic->di_ctime.t_sec;
	buf->bs_ctime.tv_nsec = dic->di_ctime.t_nsec;
	buf->bs_xflags = xfs_ip2xflags(ip);
	buf->bs_extsize = dic->di_extsize << mp->m_sb.sb_blocklog;
	buf->bs_extents = dic->di_nextents;
	buf->bs_gen = dic->di_gen;
	memset(buf->bs_pad, 0, sizeof(buf->bs_pad));
	buf->bs_dmevmask = dic->di_dmevmask;
	buf->bs_dmstate = dic->di_dmstate;
	buf->bs_aextents = dic->di_anextents;
	buf->bs_forkoff = XFS_IFORK_BOFF(ip);

	switch (dic->di_format) {
	case XFS_DINODE_FMT_DEV:
		buf->bs_rdev = ip->i_df.if_u2.if_rdev;
		buf->bs_blksize = BLKDEV_IOSIZE;
		buf->bs_blocks = 0;
		break;
	case XFS_DINODE_FMT_LOCAL:
	case XFS_DINODE_FMT_UUID:
		buf->bs_rdev = 0;
		buf->bs_blksize = mp->m_sb.sb_blocksize;
		buf->bs_blocks = 0;
		break;
	case XFS_DINODE_FMT_EXTENTS:
	case XFS_DINODE_FMT_BTREE:
		buf->bs_rdev = 0;
		buf->bs_blksize = mp->m_sb.sb_blocksize;
		buf->bs_blocks = dic->di_nblocks + ip->i_delayed_blks;
		break;
	}
	xfs_iunlock(ip, XFS_ILOCK_SHARED);
	IRELE(ip);

	error = formatter(buffer, ubsize, ubused, buf);

	if (!error)
		*stat = BULKSTAT_RV_DIDONE;

 out_free:
	kmem_free(buf);
	return error;
}

/* Return 0 on success or positive error */
STATIC int
xfs_bulkstat_one_fmt(
	void			__user *ubuffer,
	int			ubsize,
	int			*ubused,
	const xfs_bstat_t	*buffer)
{
	if (ubsize < sizeof(*buffer))
		return XFS_ERROR(ENOMEM);
	if (copy_to_user(ubuffer, buffer, sizeof(*buffer)))
		return XFS_ERROR(EFAULT);
	if (ubused)
		*ubused = sizeof(*buffer);
	return 0;
}

int
xfs_bulkstat_one(
	xfs_mount_t	*mp,		/* mount point for filesystem */
	xfs_ino_t	ino,		/* inode number to get data for */
	void		__user *buffer,	/* buffer to place output in */
	int		ubsize,		/* size of buffer */
	int		*ubused,	/* bytes used by me */
	int		*stat)		/* BULKSTAT_RV_... */
{
	return xfs_bulkstat_one_int(mp, ino, buffer, ubsize,
				    xfs_bulkstat_one_fmt, ubused, stat);
}

#define

2025-01-25 07:48:15 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:48:15 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:48:15 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:48:17 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:48:17 - snowflake.connector.connection - INFO - closed
2025-01-25 07:48:17 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:48:17 - root - INFO - SQL:
SELECT sf."path"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sf."id" = sc."id"
WHERE LOWER(sf."path") LIKE '%.py'
LIMIT 100
Results:
path
rekall-core/rekall/plugins/linux/iomem.py
examples/zeromq_receiver.py
corehq/sql_proxy_accessors/migrations/0006_get_case_by_external_id.py
maec/utils/merge.py

2025-01-25 07:48:17 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:48:17 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:48:17 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:48:19 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-25 07:48:19 - snowflake.connector.connection - INFO - closed
2025-01-25 07:48:19 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:49:07 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:49:07 - root - INFO - ```sql
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
```
2025-01-25 07:49:07 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:49:07 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:49:07 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:49:08 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:49:08 - snowflake.connector.connection - INFO - closed
2025-01-25 07:49:09 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:49:46 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:49:46 - root - INFO - SQL:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
Results:
Too long, hard cut:
path,content
maec/utils/merge.py,"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if hash_value in binned_subjects:
                                binned_subjects[hash_value].append(malware_subject)
                            else:
                                binned_subjects[hash_value] = [malware_subject]
    return binned_subjects

def merge_entities(entity_list):
    '''Merge a list of MAEC/CybOX entities'''
    dict_list = [x.to_dict() for x in entity_list]
    output_dict = dict_merge({}, *dict_list)
    return output_dict

def deduplicate_vocabulary_list(entity_list, value_name = ""value""): # TODO: Move this t

2025-01-25 07:49:46 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:49:46 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:49:46 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:49:48 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:49:48 - snowflake.connector.connection - INFO - closed
2025-01-25 07:49:48 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:49:48 - root - INFO - SQL:
SELECT sc."content" AS "import_line"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100
Results:
Too long, hard cut:
import_line
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if hash_value in binned_subjects:
                                binned_subjects[hash_value].append(malware_subject)
                            else:
                                binned_subjects[hash_value] = [malware_subject]
    return binned_subjects

def merge_entities(entity_list):
    '''Merge a list of MAEC/CybOX entities'''
    dict_list = [x.to_dict() for x in entity_list]
    output_dict = dict_merge({}, *dict_list)
    return output_dict

def deduplicate_vocabulary_list(entity_list, value_name = ""value""): # TODO: Move this to the deduplicator mo

2025-01-25 07:49:48 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:49:48 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:49:48 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:49:50 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-25 07:49:50 - snowflake.connector.connection - INFO - closed
2025-01-25 07:49:50 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:50:26 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:50:26 - root - INFO - ```sql
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
LIMIT 100;
```
2025-01-25 07:50:26 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:50:26 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:50:26 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:50:28 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:50:28 - snowflake.connector.connection - INFO - closed
2025-01-25 07:50:28 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:51:20 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:51:20 - root - INFO - SQL:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
LIMIT 100;
Results:
Too long, hard cut:
path,content
examples/zeromq_receiver.py,"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
rekall-core/rekall/plugins/linux/iomem.py,"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
corehq/sql_proxy_accessors/migrations/0006_get_case_by_external_id.py,"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
maec/utils/merge.py,"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, outp

2025-01-25 07:51:20 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:51:20 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:51:20 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:51:21 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-25 07:51:21 - snowflake.connector.connection - INFO - closed
2025-01-25 07:51:22 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:51:22 - root - INFO - SQL:
SELECT COUNT(*) AS "python_files_with_content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sf."id" = sc."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py'
Results:
python_files_with_content
4

2025-01-25 07:51:22 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:51:22 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:51:22 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:51:23 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-01-25 07:51:23 - snowflake.connector.connection - INFO - closed
2025-01-25 07:51:24 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:51:43 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:51:43 - root - INFO - ```sql
SELECT
  CASE
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'Python'
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN 'R'
  END AS "Language",
  REGEXP_SUBSTR(
    sc."content",
    CASE
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'import\\s+([a-zA-Z0-9_\\.]+)'
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN '(library|require)\\(([^\\)]+)\\)'
    END,
    1, 1, 'e', 1
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
GROUP BY 1, 2
ORDER BY 1, 3 DESC;
```
2025-01-25 07:51:43 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:51:43 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:51:43 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:51:45 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:51:45 - snowflake.connector.connection - INFO - closed
2025-01-25 07:51:45 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:52:41 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:52:41 - root - INFO - SQL:
SELECT
  CASE
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'Python'
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN 'R'
  END AS "Language",
  REGEXP_SUBSTR(
    sc."content",
    CASE
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'import\\s+([a-zA-Z0-9_\\.]+)'
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN '(library|require)\\(([^\\)]+)\\)'
    END,
    1, 1, 'e', 1
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
GROUP BY 1, 2
ORDER BY 1, 3 DESC;
Results:
Language,Module_or_Library,Count
Python,unicode_literals,1
Python,common,1
Python,deepcopy,1
Python,quickstart,1

2025-01-25 07:52:41 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:52:41 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:52:41 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:52:43 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:52:43 - snowflake.connector.connection - INFO - closed
2025-01-25 07:52:43 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:52:43 - root - INFO - SQL:
SELECT DISTINCT
  REGEXP_SUBSTR(sc."content", 'import\\s+([a-zA-Z0-9_\\.]+)', 1, 1, 'e', 1) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Results:
module
unicode_literals
common
deepcopy
quickstart

2025-01-25 07:52:43 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:52:43 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:52:43 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:52:45 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-25 07:52:45 - snowflake.connector.connection - INFO - closed
2025-01-25 07:52:45 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:53:19 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:53:19 - root - INFO - ```sql
SELECT DISTINCT
  REGEXP_SUBSTR(sc."content", 'import\\s+([a-zA-Z0-9_\\.]+)', 1, 1, '', 1) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
```
2025-01-25 07:53:19 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:53:19 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:53:19 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:53:20 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:53:20 - snowflake.connector.connection - INFO - closed
2025-01-25 07:53:21 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:22 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:54:22 - root - INFO - SQL:
SELECT DISTINCT
  REGEXP_SUBSTR(sc."content", 'import\\s+([a-zA-Z0-9_\\.]+)', 1, 1, '', 1) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
Results:
module
deepcopy
common
unicode_literals
quickstart

2025-01-25 07:54:22 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:22 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:22 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:24 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:54:24 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:24 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:25 - root - INFO - SQL:
SELECT
  REGEXP_SUBSTR(
    sc."content",
    'from\\s+[a-zA-Z0-9_\\.]+\\s+import\\s+[a-zA-Z0-9_\\*]+',
    1, 1, 'i'
  ) AS "from_import_line"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Results:
from_import_line
from __future__ import unicode_literals
from diesel import quickstart
from copy import deepcopy
from rekall.plugins.linux import common

2025-01-25 07:54:25 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:25 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:25 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:26 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:54:26 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:26 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:27 - root - INFO - SQL:
SELECT DISTINCT
  REGEXP_SUBSTR(
    sc."content",
    'import\\s+([a-zA-Z0-9_\\.]+)',
    1, 1, 'i', 1
  ) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Results:
module
common
deepcopy
quickstart
unicode_literals

2025-01-25 07:54:27 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:27 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:27 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:28 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-01-25 07:54:28 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:28 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:28 - root - INFO - SQL:
SELECT sf."path"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
WHERE sf."path" ILIKE '%.py'
LIMIT 100
Results:
Too long, hard cut:
path
push_notifications/migrations/0002_auto_20160106_0850.py
Maximum_Subarray/Python/shivam5992/kadane.py
Caesar_Cipher/Python/kennyledet/caesar_cipher.py
setup.py
pgcrypto_expressions/test/settings.py
ec2_security_groups_dumper/__init__.py
dirigible/info_pages/__init__.py
tests/python/tests/test_updatecollection.py
tests/testproject/settings.py
yandex_money/api.py
google/cloud/dataflow/utils/__init__.py
examples/Rest/ex20_get_ilo_nic.py
try/for_now.py
node_modules/node-sass/node_modules/node-gyp/gyp/pylib/gyp/easy_xml.py
vendor/distribute-0.6.36/easy_install.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
src/__init__.py
image_cropping/utils.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/GenFds/FfsInfStatement.py
src/VBox/Additions/common/crOpenGL/stub_common.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/Ecc/CodeFragmentCollector.py
project/admin/__init__.py
examples/backend/myproject/myproject/wsgi.py
tools/perf/tests/attr.py
health_check_celery/tasks.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/core/tests/test_multiarray_assignment.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/traversal/tests/test_dfs.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/core/cversions.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/lib/tests/test__datasource.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/f2py/f2py2e.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
path.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
servers/youtube.py
cloudify/manager.py
misc/webdriver-w3c-tests/cookie/cookie_test.py
Lessons/lesson15.py
layout_scanner.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
third_party/catapult/third_party/webtest/webtest/forms.py
tools/gyp/test/make_global_settings/full-toolchain/gyptest-make_global_settings.py
third_party/jinja2/compiler.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/perf/page_sets/page_reload_cases.py
third_party/jinja2/_compat.py
third_party/chromite/cbuildbot/stages/build_stages.py
third_party/chromite/lib/terminal.py
tools/telemetry/telemetry/internal/backends/mandoline/config.py
third_party/cython/src/Cython/Plex/Lexicons.py
tools/gyp/test/sanitize-rule-names/gyptest-sanitize-rule-names.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
third_party/google_appengine_cloudstorage/cloudstorage/common.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
tools/gyp/test/ios/gyptest-per-config-settings.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
third_party/catapult/tracing/tracing_build/__init__.py
tools/telemetry/telemetry/internal/backends/chrome_inspector/inspector_page_unittest.py
build/android/pylib/local/__init__.py
native_client/tools/scons_to_gn/print_data.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
tools/cr/cr/actions/linux.py
tools/swarming_client/third_party/rsa/rsa/transform.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
tools/gyp/test/win/gyptest-link-enable-winrt.py
third_party/chromite/scripts/dep_tracker_unittest.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/datapipeline/layer1.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
third_party/chromite/lib/patch.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/mws/__init__.py
tools/telemetry/third_party/modulegraph/modulegraph_tests/testpkg-edgedata/pkg/toplevel_conditional_import_existing.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/gs/bucketlistresultset.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
buildtools/clang_format/script/clang-format.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
third_party/tlslite/tlslite/utils/cryptomath.py
tools/telemetry/third_party/gsutilz/gslib/addlhelp/projects.py
tools/deep_memory_profiler/subcommands/buckets.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Pl

2025-01-25 07:54:28 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:28 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:28 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:30 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:54:30 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:30 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:30 - root - INFO - SQL:
SELECT
  REGEXP_SUBSTR(
    sc."content",
    'import\\s+([a-zA-Z0-9_\\.]+)',
    1, 1, 'i', 1
  ) AS "module",
  COUNT(*) AS "count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
GROUP BY "module"
ORDER BY "count" DESC
LIMIT 100
Results:
module,count
unicode_literals,1
deepcopy,1
quickstart,1
common,1

2025-01-25 07:54:30 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:30 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:30 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:32 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-01-25 07:54:32 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:32 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:54:52 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:54:52 - root - INFO - ```sql
SELECT
  'Python' AS "Language",
  REGEXP_REPLACE(
    REGEXP_SUBSTR(sc."content", 'import\\s+(\\S+)', 1, 1),
    'import\\s+', ''
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" LIKE '%import%'
GROUP BY "Module_or_Library"
ORDER BY "Count" DESC
LIMIT 100;
```
2025-01-25 07:54:52 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:54:52 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:54:52 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:54:53 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-01-25 07:54:53 - snowflake.connector.connection - INFO - closed
2025-01-25 07:54:54 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:55:31 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:55:31 - root - INFO - SQL:
SELECT
  'Python' AS "Language",
  REGEXP_REPLACE(
    REGEXP_SUBSTR(sc."content", 'import\\s+(\\S+)', 1, 1),
    'import\\s+', ''
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" LIKE '%import%'
GROUP BY "Module_or_Library"
ORDER BY "Count" DESC
LIMIT 100;
Results:
Language,Module_or_Library,Count
Python,"quickstart,",1
Python,common,1
Python,unicode_literals,1
Python,deepcopy,1

2025-01-25 07:55:31 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:55:31 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:55:31 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:55:32 - snowflake.connector.cursor - INFO - Number of results in first chunk: 2
2025-01-25 07:55:32 - snowflake.connector.connection - INFO - closed
2025-01-25 07:55:32 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:55:33 - root - INFO - SQL:
SELECT
  'Python' AS "Language",
  REGEXP_REPLACE(
    REGEXP_SUBSTR(sc."content", 'import\s+(\S+)', 1, 1),
    'import\s+', ''
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
WHERE sc."content" LIKE '%import%'
GROUP BY "Module_or_Library"
ORDER BY "Count" DESC
LIMIT 100
Results:
Language,Module_or_Library,Count
Python,,4448
Python,S,2

2025-01-25 07:55:33 - root - INFO - itercount: 0
2025-01-25 07:55:33 - root - INFO - Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
DDL describes table information.
,0
table_name,SAMPLE_FILES
description,
DDL,"create or replace TABLE SAMPLE_FILES (
	""repo_name"" VARCHAR(16777216),
	""ref"" VARCHAR(16777216),
	""path"" VARCHAR(16777216),
	""mode"" NUMBER(38,0),
	""id"" VARCHAR(16777216),
	""symlink_target"" VARCHAR(16777216)
);"

,1
table_name,LANGUAGES
description,
DDL,"create or replace TABLE LANGUAGES (
	""repo_name"" VARCHAR(16777216),
	""language"" VARIANT
);"

,2
table_name,LICENSES
description,
DDL,"create or replace TABLE LICENSES (
	""repo_name"" VARCHAR(16777216),
	""license"" VARCHAR(16777216)
);"

,3
table_name,SAMPLE_COMMITS
description,
DDL,"create or replace TABLE SAMPLE_COMMITS (
	""commit"" VARCHAR(16777216),
	""tree"" VARCHAR(16777216),
	""parent"" VARIANT,
	""author"" VARIANT,
	""committer"" VARIANT,
	""subject"" VARCHAR(16777216),
	""message"" VARCHAR(16777216),
	""trailer"" VARIANT,
	""difference"" VARIANT,
	""difference_truncated"" BOOLEAN,
	""repo_name"" VARCHAR(16777216),
	""encoding"" VARCHAR(16777216)
);"

,4
table_name,SAMPLE_CONTENTS
description,
DDL,"create or replace TABLE SAMPLE_CONTENTS (
	""id"" VARCHAR(16777216),
	""size"" NUMBER(38,0),
	""content"" VARCHAR(16777216),
	""binary"" BOOLEAN,
	""copies"" NUMBER(38,0),
	""sample_repo_name"" VARCHAR(16777216),
	""sample_ref"" VARCHAR(16777216),
	""sample_path"" VARCHAR(16777216),
	""sample_mode"" NUMBER(38,0),
	""sample_symlink_target"" VARCHAR(16777216)
);"

,5
table_name,SAMPLE_REPOS
description,
DDL,"create or replace TABLE SAMPLE_REPOS (
	""repo_name"" VARCHAR(16777216),
	""watch_count"" NUMBER(38,0)
);"

External knowledge that might be helpful: 
None
The table structure information is ({database name: {schema name: {table name}}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_FILES', 'SAMPLE_REPOS']}}
Begin Exploring Related Columns
To address the task of finding the imported Python modules and R libraries from the GitHub sample files, we'll consider the following tables and columns:

**Relevant Tables and Columns:**

1. **"GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"**
   - **"id"**: Unique identifier for each file. Useful for joining with "SAMPLE_CONTENTS".
   - **"path"**: File path, which includes the file name and extension. We can use this to identify Python and R files by their extensions (e.g., `.py`, `.R`, `.r`).

2. **"GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS"**
   - **"id"**: Unique identifier matching the "id" in "SAMPLE_FILES".
   - **"content"**: The actual content of the file, which may contain import statements.

**Potential Usage of Columns:**

- Use **"path"** to filter for Python (`%.py`) and R (`%.R` or `%.r`) files.
- Join **"SAMPLE_FILES"** and **"SAMPLE_CONTENTS"** on **"id"** to associate file paths with their contents.
- Use **"content"** to search for import statements like `import`, `from` (Python), `library(`, or `require(` (R).

**Conditions that May Be Used:**

- **File Type Filtering**: `"path" LIKE '%.py'` for Python files, `"path" LIKE '%.R'` or `"path" LIKE '%.r'` for R files.
- **Import Statement Filtering**: `"content" LIKE '%import%'`, `"content" LIKE '%from%'` for Python, `"content" LIKE '%library(%'`, `"content" LIKE '%require(%'` for R.

**SQL Queries to Understand the Data:**

1. **List Sample File Paths**

   ```sql
   SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
   ```

2. **Count Total Number of Files**

   ```sql
   SELECT COUNT(*) AS "total_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES";
   ```

3. **Get Distinct File Extensions**

   ```sql
   SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path") + 1) AS "extension"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
   ```

4. **Count Number of Python Files**

   ```sql
   SELECT COUNT(*) AS "python_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.py';
   ```

5. **Count Number of R Files**

   ```sql
   SELECT COUNT(*) AS "r_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.r';
   ```

6. **Preview Content of Python Files**

   ```sql
   SELECT sc."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
     ON sc."id" = sf."id"
   WHERE LOWER(sf."path") LIKE '%.py'
   LIMIT 100;
   ```

7. **Preview Content of R Files**

   ```sql
   SELECT sc."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
     ON sc."id" = sf."id"
   WHERE LOWER(sf."path") LIKE '%.r'
   LIMIT 100;
   ```

8. **Find Python Files Containing 'import'**

   ```sql
   SELECT sf."path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
     ON sf."id" = sc."id"
   WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import%'
   LIMIT 100;
   ```

9. **Find R Files Containing 'library(' or 'require('**

   ```sql
   SELECT sf."path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
     ON sf."id" = sc."id"
   WHERE LOWER(sf."path") LIKE '%.r' AND (sc."content" ILIKE '%library(%' OR sc."content" ILIKE '%require(%')
   LIMIT 100;
   ```

10. **Extract Lines with 'import' Statements from Python Files**

    ```sql
    SELECT REGEXP_SUBSTR(sc."content", '.*import.*') AS "import_line"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import%'
    LIMIT 100;
    ```

11. **Extract Lines with 'library(' or 'require(' Statements from R Files**

    ```sql
    SELECT REGEXP_SUBSTR(sc."content", '.*(library|require)\\(.*\\).*') AS "library_line"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.r' AND (sc."content" ILIKE '%library(%' OR sc."content" ILIKE '%require(%')
    LIMIT 100;
    ```

12. **Count Python Files with 'import' Statements**

    ```sql
    SELECT COUNT(*) AS "python_files_with_import"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
      ON sf."id" = sc."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import%';
    ```

13. **Count R Files with 'library(' or 'require(' Statements**

    ```sql
    SELECT COUNT(*) AS "r_files_with_library"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
      ON sf."id" = sc."id"
    WHERE LOWER(sf."path") LIKE '%.r' AND (sc."content" ILIKE '%library(%' OR sc."content" ILIKE '%require(%');
    ```

14. **List Unique Imported Modules in Python Files**

    ```sql
    SELECT DISTINCT REGEXP_SUBSTR(sc."content", 'import\\s+(\\w+)', 1, 1, 'e', 1) AS "module"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import%'
    LIMIT 100;
    ```

15. **List Unique Libraries Imported in R Files**

    ```sql
    SELECT DISTINCT REGEXP_SUBSTR(sc."content", '(library|require)\\(([^\\)]+)\\)', 1, 1, 'e', 2) AS "library"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.r' AND (sc."content" ILIKE '%library(%' OR sc."content" ILIKE '%require(%')
    LIMIT 100;
    ```

16. **Preview Content Where 'from' Import is Used in Python Files**

    ```sql
    SELECT REGEXP_SUBSTR(sc."content", 'from\\s+\\w+\\s+import\\s+\\w+', 1, 1, 'e') AS "from_import_line"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%from%import%'
    LIMIT 100;
    ```

17. **List All Modules Imported with 'from' in Python Files**

    ```sql
    SELECT DISTINCT REGEXP_SUBSTR(sc."content", 'from\\s+(\\w+)\\s+import', 1, 1, 'e', 1) AS "module"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%from%import%'
    LIMIT 100;
    ```

18. **Find Files Where 'import pandas' is Used**

    ```sql
    SELECT sf."path"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
      ON sf."id" = sc."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import pandas%'
    LIMIT 100;
    ```

19. **Count Occurrences of Each Imported Module in Python Files**

    ```sql
    SELECT REGEXP_SUBSTR(sc."content", 'import\\s+(\\w+)', 1, 1, 'e', 1) AS "module", COUNT(*) AS "count"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.py' AND sc."content" ILIKE '%import%'
    GROUP BY "module"
    ORDER BY "count" DESC
    LIMIT 100;
    ```

20. **Count Occurrences of Each Imported Library in R Files**

    ```sql
    SELECT REGEXP_SUBSTR(sc."content", '(library|require)\\(([^\\)]+)\\)', 1, 1, 'e', 2) AS "library", COUNT(*) AS "count"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
      ON sc."id" = sf."id"
    WHERE LOWER(sf."path") LIKE '%.r' AND (sc."content" ILIKE '%library(%' OR sc."content" ILIKE '%require(%')
    GROUP BY "library"
    ORDER BY "count" DESC
    LIMIT 100;
    ```

These queries progressively explore the data from basic counts and previews to extracting and counting imported modules and libraries. They help in understanding how the data is structured and how it can be used to fulfill the task.

**Note:** The queries use functions like `REGEXP_SUBSTR` and `ILIKE` for case-insensitive searches and regular expression matching to extract modules and libraries. Adjustments might be needed based on the actual data and SQL dialect supported by the database.Query:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
Answer:
Too long, hard cut:
path
fixtures/strict-par-success/fun1_to_proc_par2.ll
tests/failure/wrong_order_par_seq_middle.t/wrong_order_par_seq_middle.ll
fixtures/strict-par-success/layout_case.ll
fixtures/strict-par-success/merger_loli_Sort.ll
fixtures/failure/infer_recv.ll
fixtures/strict-par-success/parallel_assoc_tensor3_flat.ll
fixtures/strict-par-failure/ten_loli_par.ll
fixtures/compile/my_loli.ll
fixtures/failure/dead_lock_tensor2_tensor2.ll
fixtures/sequence/par_ten_ten_v1.ll
tests/failure/reusedParChannel.t/reusedParChannel.ll
Demo/Pods/Headers/Private/ReactiveCocoa/ReactiveCocoa/RACSerialDisposable.h
Demo/Pods/Headers/Private/ReactiveCocoa/UISlider+RACSignalSupport.h
libsel4/tools/xmllint.sh
themes/themes-available/Vautour/images/logo_thruk_mid.png
applications/solvers/incompressible/windEnergy/pisoFoamTurbine/Make/options
applications/solvers/incompressible/windEnergy/windPlantSolver/pEqn.H
cloudsqlip/Godeps/_workspace/src/github.com/prometheus/procfs/fixtures/26232/fd/1
examples/2D/two_dimensional_riemann_problems/kt-c03/off/OFF
test/zdtm/static/mntns_root_bind02.c
fastlane/metadata/fr-FR/marketing_url.txt
Pods/Headers/Private/CocoaLumberjack/DDContextFilterLogFormatter.h
Pods/Headers/Private/SVGKit/SVGKParserStyles.h
Pods/Headers/Public/SVGKit/SVGLength.h
Pods/Headers/Public/KSRefresh/UIScrollView+KS.h
Pods/Headers/Private/FMDB/FMDatabasePool.h
plugins/20_events/post-build-dockerfile
Pods/Headers/Public/KIF/CGGeometry-KIFAdditions.h
third_party/java/jetty/lib/jetty-servlet.jar
GYnews/Pods/Headers/Public/Masonry/MASConstraintMaker.h
test/05-diff/source/etc/symlink-modified.conf
examples/networking/neighbor_sharing/simulation.py
static/js/native.history.js
fast
deps/openssl/include/openssl/ui_compat.h
icons/CubLinux/actions/22/gnome-logout.png
icons/CubLinux/actions/16/window_new.png
icons/CubLinux/status/22/gpm-battery-040.png
icons/CubLinux/status/Scalable/notification-gpm-battery-020.svg
icons/CubLinux/status/48/battery_two_thirds.png
icons/CubLinux/places/128/gnome-fs-dav.png
icons/CubLinux/actions/22/rhythmbox-set-star.png
icons/CubLinux/places/128/gnome-fs-smb.png
icons/CubLinux/actions/22/gtk-find-and-replace.png
icons/CubLinux/actions/22/filesave.png
icons/CubLinux/actions/24/back.png
icons/chromiumos/cursors/sailboat
icons/CubLinux/status/96/ac-adapter.png
icons/CubLinux/stock/64/stock_person-panel.png
icons/CubLinux/places/96/folder-images.png
icons/CubLinux/actions/24/rotate.png
icons/CubLinux/status/22/battery-good-charging-symbolic.png
icons/CubLinux/status/128/notification-battery-020.png
icons/CubLinux/actions/16/stock_search.png
icons/CubLinux/actions/22/gtk-properties.png
icons/CubLinux/places/scalable/gnome-fs-directory-accept.svg
Debian Packages/chromiumos-cursor-theme/extract/usr/share/icons/chromiumos/cursors/hand1
rekaz/Zaker-C/Pods/Headers/Public/SDWebImage/SDWebImageManager.h
notanos/system/data/icons/buuf_watercolor/apps/kig.png
notanos/system/data/icons/buuf_watercolor/status/dialog-error.png
notanos/system/data/icons/buuf_watercolor/actions/edit-cut.png
notanos/system/data/icons/buuf_watercolor/actions/system-shutdown.png
notanos/system/data/icons/buuf_watercolor/devices/audio-input-microphone.png
PopMenu/Pods/Headers/Private/pop/POPAnimationRuntime.h
bin/yap_summary_call
test-apps/test-app-with-history-fallback/libs
build/prepare-commit-msg
man/std::wfstream::close.3
"man/std::u32string::rend,std::basic_string::crend.3"
"man/std::wstreambuf::pubimbue,std::basic_streambuf::imbue.3"
node_modules/html-webpack-plugin/node_modules/html-minifier/node_modules/.bin/uglifyjs
third_party/openssl/linux-x86_64/include/openssl/ecdsa.h
Pods/Headers/Private/FBSDKCoreKit/FBSDKUtility.h
Pods/Headers/Public/ParseTwitterUtils/PF_Twitter.h
Pods/Headers/Private/FBSDKCoreKit/FBSDKMutableCopying.h
Pods/Headers/Private/Parse/PFURLSessionDataTaskDelegate.h
Pods/Headers/Private/Parse/PFRESTObjectBatchCommand.h
Pods/Headers/Private/Parse/PFErrorUtilities.h
Pods/Headers/Private/Parse/PFPushUtilities.h
Pods/Headers/Private/ParseFacebookUtilsV4/PFFacebookUtils.h
Pods/Headers/Private/Parse/PFSession.h
Pods/Headers/Public/YYText/YYTextMagnifier.h
Pods/Headers/Private/MagicalRecord/NSPersistentStoreCoordinator+MagicalRecord.h
Library/ENV/4.3/g++-4.9
百思不得姐项目/Pods/Headers/Private/SVProgressHUD/SVRadialGradientLayer.h
DemoTests/Pods/Headers/Private/ZXingObjC/ZXMultiDetector.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceSubscribedSkuCollectionFetcher.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceReminder.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Private/MSGraph-SDK-iOS/MSGraphServiceAssignedLicense.h
Samples/MicrosoftGraph.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDirectoryRoleOperations.h
Samples/MicrosoftGraph.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDevice.h
S
Query:
SELECT COUNT(*) AS "total_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES";
Answer:
total_files
524077
Query:
SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path") + 1) AS "extension"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   LIMIT 100;
Answer:
extension
ll
t/wrong_order_par_seq_middle.ll
t/reusedParChannel.ll
sh
conf
icons/chromiumos/cursors/sailboat
test-apps/test-app-with-history-fallback/libs
3/g++-4.9
iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceSubscribedSkuCollectionFetcher.h
iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDevice.h
iOS.Swift.SendMail/Pods/Headers/Private/MSGraph-SDK-iOS/MSGraphServiceConversationCollectionFetcher.h
0.2d/include/openssl/pqueue.h
benchmarks/svm_on_grid/random_hyperopt_august2013_mod
ico
core/dslmcode/stacks/blog/sites/all/libraries/jmol
bin/jslint
example/share/templates
framework/Versions/Current
tests/virhostcpudata/linux-subcores2/node/node16/cpu102
applications/solvers/incompressible/windEnergy/pisoFoamTurbine/Make/options
iOS.Objective-C.SendMail/Pods/Headers/Public/ADALiOS/ADLogger.h
php
domains/grades
core/dslmcode/stacks/innovate/sites/default
system/boot/ix86/vmxboot/suse-tumbleweed/root/include
dtsi
S
legacyLibraryProject/PlayerHater/res
md
Debian Packages/chromiumos-cursor-theme/extract/usr/share/icons/chromiumos/cursors/hand1
oasis.opendocument.draw.template.svg
0.1.jar.LICENSE
adoc
tests/ostree-grub-generator
3/g++-4.5
tests/module-loading/sim_encoder/num_chan=1/PIN_NAME_REGEX
arch/arm64/boot/dts/include/dt-bindings
src/libc/locale/locale/et_EE/LC_MONETARY
events/display-state-change/25_create_xineliboutput_config
chroot-bin/startkde
alternatives/traceproto
bin/os-name
1-DVD-i586-Build0091-textmode/.thumbs/installation_overview-1.png
tmp/vendor/@angular/compiler/src/output/output_interpreter.js
tmp/vendor/rxjs/Observable.js
data/licenses/container-interop
package/feeds/packages/libexif
package/feeds/management/libnetconf
Examples/CocosBuilderExample/libs/CocosDenshion
oasis.opendocument.presentation.svg
8.3-1.yaml.erb
0.1/mpn/toom_eval_pm1.c
9.32_eabi/gcc-linaro-4.5-2011.02-0-final/arm-openwrt-linux-uclibcgnueabi/libstdc++-v3/include/debug/safe_iterator.tcc
6/copy_reg.py
src/journal-remote/Makefile
src/system-update-generator/Makefile
svg
core/dslmcode/stacks/media/scripts
core/dslmcode/stacks/blog/sites/all/libraries/google-api-cors-upload
128x128
infer/lib/wrappers/clang++
bin/which
config
core/dslmcode/stacks/discuss/sites/all/libraries/chosen
BaragonUI/vendor/vexCss
erai-30km
INSTALL
stampede-agent/files/opt/virsh
src/libc/locale/locale/is_IS/LC_NUMERIC
src/libc/locale/locale/ja_JP/LC_CTYPE
osg
ive
2.12/lib/auth/.libs/libgnutls_auth.la
framework/Headers
3.2.jar
sun.xml.calc.png
1-DVD-i586-Build0091-kde/ooffice-5.png
xctemplate/Sleipnir.framework/Versions/Current
tmp/vendor/@angular/compiler/src/output/ts_emitter.js
tmp/vendor/@angular/core/src/application_tokens.js
tmp/vendor/angularfire2/providers/auth.js
tmp/vendor/@angular/common/src/forms.js
tmp/vendor/@angular/http/esm/src/backends/browser_jsonp.js
theme/Numix-Cursor/cursors/wait
Library/Aliases/boot2docker-cli
com/mattermost/rsc/tmp/google
deps/protobuf/protobuf
package/feeds/oldpackages/nuttcp
package/feeds/oldpackages/updatedd
vgtest
1-DVD-i586-Build0091-textmode/mtab-1.png
 15-UI-test/Pods/BuildHeaders/Expecta/EXPMatchers+beLessThanOrEqualTo.h
m
package/feeds/oldpackages/libdlna
de_DE.php
bin/har-validator
woff
1.txt
bin/mocha
Query:
SELECT COUNT(*) AS "python_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.py';
Answer:
python_files
8963
Query:
SELECT COUNT(*) AS "r_files"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE LOWER("path") LIKE '%.r';
Answer:
r_files
184
Query:
SELECT sc."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
     ON sc."id" = sf."id"
   WHERE LOWER(sf."path") LIKE '%.py'
   LIMIT 100;
Answer:
Too long, hard cut:
content
"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API repre
Query:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
LIMIT 100;
Answer:
Too long, hard cut:
path,content
fs/xfs/xfs_itable.c,"/*
 * Copyright (c) 2000-2002,2005 Silicon Graphics, Inc.
 * All Rights Reserved.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write the Free Software Foundation,
 * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */
#include ""xfs.h""
#include ""xfs_fs.h""
#include ""xfs_types.h""
#include ""xfs_bit.h""
#include ""xfs_log.h""
#include ""xfs_inum.h""
#include ""xfs_trans.h""
#include ""xfs_sb.h""
#include ""xfs_ag.h""
#include ""xfs_mount.h""
#include ""xfs_bmap_btree.h""
#include ""xfs_alloc_btree.h""
#include ""xfs_ialloc_btree.h""
#include ""xfs_dinode.h""
#include ""xfs_inode.h""
#include ""xfs_ialloc.h""
#include ""xfs_itable.h""
#include ""xfs_error.h""
#include ""xfs_btree.h""
#include ""xfs_trace.h""

STATIC int
xfs_internal_inum(
	xfs_mount_t	*mp,
	xfs_ino_t	ino)
{
	return (ino == mp->m_sb.sb_rbmino || ino == mp->m_sb.sb_rsumino ||
		(xfs_sb_version_hasquota(&mp->m_sb) &&
		 (ino == mp->m_sb.sb_uquotino || ino == mp->m_sb.sb_gquotino)));
}

/*
 * Return stat information for one inode.
 * Return 0 if ok, else errno.
 */
int
xfs_bulkstat_one_int(
	struct xfs_mount	*mp,		/* mount point for filesystem */
	xfs_ino_t		ino,		/* inode to get data for */
	void __user		*buffer,	/* buffer to place output in */
	int			ubsize,		/* size of buffer */
	bulkstat_one_fmt_pf	formatter,	/* formatter, copy to user */
	int			*ubused,	/* bytes used by me */
	int			*stat)		/* BULKSTAT_RV_... */
{
	struct xfs_icdinode	*dic;		/* dinode core info pointer */
	struct xfs_inode	*ip;		/* incore inode pointer */
	struct xfs_bstat	*buf;		/* return buffer */
	int			error = 0;	/* error value */

	*stat = BULKSTAT_RV_NOTHING;

	if (!buffer || xfs_internal_inum(mp, ino))
		return XFS_ERROR(EINVAL);

	buf = kmem_alloc(sizeof(*buf), KM_SLEEP | KM_MAYFAIL);
	if (!buf)
		return XFS_ERROR(ENOMEM);

	error = xfs_iget(mp, NULL, ino,
			 (XFS_IGET_DONTCACHE | XFS_IGET_UNTRUSTED),
			 XFS_ILOCK_SHARED, &ip);
	if (error) {
		*stat = BULKSTAT_RV_NOTHING;
		goto out_free;
	}

	ASSERT(ip != NULL);
	ASSERT(ip->i_imap.im_blkno != 0);

	dic = &ip->i_d;

	/* xfs_iget returns the following without needing
	 * further change.
	 */
	buf->bs_nlink = dic->di_nlink;
	buf->bs_projid_lo = dic->di_projid_lo;
	buf->bs_projid_hi = dic->di_projid_hi;
	buf->bs_ino = ino;
	buf->bs_mode = dic->di_mode;
	buf->bs_uid = dic->di_uid;
	buf->bs_gid = dic->di_gid;
	buf->bs_size = dic->di_size;
	buf->bs_atime.tv_sec = dic->di_atime.t_sec;
	buf->bs_atime.tv_nsec = dic->di_atime.t_nsec;
	buf->bs_mtime.tv_sec = dic->di_mtime.t_sec;
	buf->bs_mtime.tv_nsec = dic->di_mtime.t_nsec;
	buf->bs_ctime.tv_sec = dic->di_ctime.t_sec;
	buf->bs_ctime.tv_nsec = dic->di_ctime.t_nsec;
	buf->bs_xflags = xfs_ip2xflags(ip);
	buf->bs_extsize = dic->di_extsize << mp->m_sb.sb_blocklog;
	buf->bs_extents = dic->di_nextents;
	buf->bs_gen = dic->di_gen;
	memset(buf->bs_pad, 0, sizeof(buf->bs_pad));
	buf->bs_dmevmask = dic->di_dmevmask;
	buf->bs_dmstate = dic->di_dmstate;
	buf->bs_aextents = dic->di_anextents;
	buf->bs_forkoff = XFS_IFORK_BOFF(ip);

	switch (dic->di_format) {
	case XFS_DINODE_FMT_DEV:
		buf->bs_rdev = ip->i_df.if_u2.if_rdev;
		buf->bs_blksize = BLKDEV_IOSIZE;
		buf->bs_blocks = 0;
		break;
	case XFS_DINODE_FMT_LOCAL:
	case XFS_DINODE_FMT_UUID:
		buf->bs_rdev = 0;
		buf->bs_blksize = mp->m_sb.sb_blocksize;
		buf->bs_blocks = 0;
		break;
	case XFS_DINODE_FMT_EXTENTS:
	case XFS_DINODE_FMT_BTREE:
		buf->bs_rdev = 0;
		buf->bs_blksize = mp->m_sb.sb_blocksize;
		buf->bs_blocks = dic->di_nblocks + ip->i_delayed_blks;
		break;
	}
	xfs_iunlock(ip, XFS_ILOCK_SHARED);
	IRELE(ip);

	error = formatter(buffer, ubsize, ubused, buf);

	if (!error)
		*stat = BULKSTAT_RV_DIDONE;

 out_free:
	kmem_free(buf);
	return error;
}

/* Return 0 on success or positive error */
STATIC int
xfs_bulkstat_one_fmt(
	void			__user *ubuffer,
	int			ubsize,
	int			*ubused,
	const xfs_bstat_t	*buffer)
{
	if (ubsize < sizeof(*buffer))
		return XFS_ERROR(ENOMEM);
	if (copy_to_user(ubuffer, buffer, sizeof(*buffer)))
		return XFS_ERROR(EFAULT);
	if (ubused)
		*ubused = sizeof(*buffer);
	return 0;
}

int
xfs_bulkstat_one(
	xfs_mount_t	*mp,		/* mount point for filesystem */
	xfs_ino_t	ino,		/* inode number to get data for */
	void		__user *buffer,	/* buffer to place output in */
	int		ubsize,		/* size of buffer */
	int		*ubused,	/* bytes used by me */
	int		*stat)		/* BULKSTAT_RV_... */
{
	return xfs_bulkstat_one_int(mp, ino, buffer, ubsize,
				    xfs_bulkstat_one_fmt, ubused, stat);
}

#define
Query:
SELECT sf."path"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sf."id" = sc."id"
WHERE LOWER(sf."path") LIKE '%.py'
LIMIT 100
Answer:
path
rekall-core/rekall/plugins/linux/iomem.py
examples/zeromq_receiver.py
corehq/sql_proxy_accessors/migrations/0006_get_case_by_external_id.py
maec/utils/merge.py
Query:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
Answer:
Too long, hard cut:
path,content
maec/utils/merge.py,"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if hash_value in binned_subjects:
                                binned_subjects[hash_value].append(malware_subject)
                            else:
                                binned_subjects[hash_value] = [malware_subject]
    return binned_subjects

def merge_entities(entity_list):
    '''Merge a list of MAEC/CybOX entities'''
    dict_list = [x.to_dict() for x in entity_list]
    output_dict = dict_merge({}, *dict_list)
    return output_dict

def deduplicate_vocabulary_list(entity_list, value_name = ""value""): # TODO: Move this t
Query:
SELECT sc."content" AS "import_line"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100
Answer:
Too long, hard cut:
import_line
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if hash_value in binned_subjects:
                                binned_subjects[hash_value].append(malware_subject)
                            else:
                                binned_subjects[hash_value] = [malware_subject]
    return binned_subjects

def merge_entities(entity_list):
    '''Merge a list of MAEC/CybOX entities'''
    dict_list = [x.to_dict() for x in entity_list]
    output_dict = dict_merge({}, *dict_list)
    return output_dict

def deduplicate_vocabulary_list(entity_list, value_name = ""value""): # TODO: Move this to the deduplicator mo
Query:
SELECT sf."path", sc."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
LIMIT 100;
Answer:
Too long, hard cut:
path,content
examples/zeromq_receiver.py,"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
rekall-core/rekall/plugins/linux/iomem.py,"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
corehq/sql_proxy_accessors/migrations/0006_get_case_by_external_id.py,"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
maec/utils/merge.py,"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, outp
Query:
SELECT COUNT(*) AS "python_files_with_content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
  ON sf."id" = sc."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py'
Answer:
python_files_with_content
4
Query:
SELECT
  CASE
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'Python'
    WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN 'R'
  END AS "Language",
  REGEXP_SUBSTR(
    sc."content",
    CASE
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py' THEN 'import\\s+([a-zA-Z0-9_\\.]+)'
      WHEN LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'r' THEN '(library|require)\\(([^\\)]+)\\)'
    END,
    1, 1, 'e', 1
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) IN ('py', 'r')
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
GROUP BY 1, 2
ORDER BY 1, 3 DESC;
Answer:
Language,Module_or_Library,Count
Python,unicode_literals,1
Python,common,1
Python,deepcopy,1
Python,quickstart,1
Query:
SELECT DISTINCT
  REGEXP_SUBSTR(sc."content", 'import\\s+([a-zA-Z0-9_\\.]+)', 1, 1, 'e', 1) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE LOWER(REVERSE(SPLIT_PART(REVERSE(sf."path"), '.', 1))) = 'py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Answer:
module
unicode_literals
common
deepcopy
quickstart
Query:
SELECT DISTINCT
  REGEXP_SUBSTR(sc."content", 'import\\s+([a-zA-Z0-9_\\.]+)', 1, 1, '', 1) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
LIMIT 100;
Answer:
module
deepcopy
common
unicode_literals
quickstart
Query:
SELECT
  REGEXP_SUBSTR(
    sc."content",
    'from\\s+[a-zA-Z0-9_\\.]+\\s+import\\s+[a-zA-Z0-9_\\*]+',
    1, 1, 'i'
  ) AS "from_import_line"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Answer:
from_import_line
from __future__ import unicode_literals
from diesel import quickstart
from copy import deepcopy
from rekall.plugins.linux import common
Query:
SELECT DISTINCT
  REGEXP_SUBSTR(
    sc."content",
    'import\\s+([a-zA-Z0-9_\\.]+)',
    1, 1, 'i', 1
  ) AS "module"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
LIMIT 100
Answer:
module
common
deepcopy
quickstart
unicode_literals
Query:
SELECT sf."path"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
WHERE sf."path" ILIKE '%.py'
LIMIT 100
Answer:
Too long, hard cut:
path
push_notifications/migrations/0002_auto_20160106_0850.py
Maximum_Subarray/Python/shivam5992/kadane.py
Caesar_Cipher/Python/kennyledet/caesar_cipher.py
setup.py
pgcrypto_expressions/test/settings.py
ec2_security_groups_dumper/__init__.py
dirigible/info_pages/__init__.py
tests/python/tests/test_updatecollection.py
tests/testproject/settings.py
yandex_money/api.py
google/cloud/dataflow/utils/__init__.py
examples/Rest/ex20_get_ilo_nic.py
try/for_now.py
node_modules/node-sass/node_modules/node-gyp/gyp/pylib/gyp/easy_xml.py
vendor/distribute-0.6.36/easy_install.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
src/__init__.py
image_cropping/utils.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/GenFds/FfsInfStatement.py
src/VBox/Additions/common/crOpenGL/stub_common.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/Ecc/CodeFragmentCollector.py
project/admin/__init__.py
examples/backend/myproject/myproject/wsgi.py
tools/perf/tests/attr.py
health_check_celery/tasks.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/core/tests/test_multiarray_assignment.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/traversal/tests/test_dfs.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/core/cversions.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/lib/tests/test__datasource.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/f2py/f2py2e.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
path.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
servers/youtube.py
cloudify/manager.py
misc/webdriver-w3c-tests/cookie/cookie_test.py
Lessons/lesson15.py
layout_scanner.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
third_party/catapult/third_party/webtest/webtest/forms.py
tools/gyp/test/make_global_settings/full-toolchain/gyptest-make_global_settings.py
third_party/jinja2/compiler.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/perf/page_sets/page_reload_cases.py
third_party/jinja2/_compat.py
third_party/chromite/cbuildbot/stages/build_stages.py
third_party/chromite/lib/terminal.py
tools/telemetry/telemetry/internal/backends/mandoline/config.py
third_party/cython/src/Cython/Plex/Lexicons.py
tools/gyp/test/sanitize-rule-names/gyptest-sanitize-rule-names.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
third_party/google_appengine_cloudstorage/cloudstorage/common.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
tools/gyp/test/ios/gyptest-per-config-settings.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
third_party/catapult/tracing/tracing_build/__init__.py
tools/telemetry/telemetry/internal/backends/chrome_inspector/inspector_page_unittest.py
build/android/pylib/local/__init__.py
native_client/tools/scons_to_gn/print_data.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
tools/cr/cr/actions/linux.py
tools/swarming_client/third_party/rsa/rsa/transform.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
tools/gyp/test/win/gyptest-link-enable-winrt.py
third_party/chromite/scripts/dep_tracker_unittest.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/datapipeline/layer1.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
third_party/chromite/lib/patch.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/mws/__init__.py
tools/telemetry/third_party/modulegraph/modulegraph_tests/testpkg-edgedata/pkg/toplevel_conditional_import_existing.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/gs/bucketlistresultset.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
buildtools/clang_format/script/clang-format.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
third_party/tlslite/tlslite/utils/cryptomath.py
tools/telemetry/third_party/gsutilz/gslib/addlhelp/projects.py
tools/deep_memory_profiler/subcommands/buckets.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Pl
Query:
SELECT
  REGEXP_SUBSTR(
    sc."content",
    'import\\s+([a-zA-Z0-9_\\.]+)',
    1, 1, 'i', 1
  ) AS "module",
  COUNT(*) AS "count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" IS NOT NULL
  AND sc."content" != ''
GROUP BY "module"
ORDER BY "count" DESC
LIMIT 100
Answer:
module,count
unicode_literals,1
deepcopy,1
quickstart,1
common,1
Query:
SELECT
  'Python' AS "Language",
  REGEXP_REPLACE(
    REGEXP_SUBSTR(sc."content", 'import\\s+(\\S+)', 1, 1),
    'import\\s+', ''
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  ON sc."id" = sf."id"
WHERE sf."path" ILIKE '%.py'
  AND sc."content" LIKE '%import%'
GROUP BY "Module_or_Library"
ORDER BY "Count" DESC
LIMIT 100;
Answer:
Language,Module_or_Library,Count
Python,"quickstart,",1
Python,common,1
Python,unicode_literals,1
Python,deepcopy,1
Query:
SELECT
  'Python' AS "Language",
  REGEXP_REPLACE(
    REGEXP_SUBSTR(sc."content", 'import\s+(\S+)', 1, 1),
    'import\s+', ''
  ) AS "Module_or_Library",
  COUNT(*) AS "Count"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
WHERE sc."content" LIKE '%import%'
GROUP BY "Module_or_Library"
ORDER BY "Count" DESC
LIMIT 100
Answer:
Language,Module_or_Library,Count
Python,,4448
Python,S,2
End Exploring Related Columns
Task: Can you find the imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order.

Please answer only one complete SQL in snowflake dialect in ```sql``` format.
Usage example: ```sql
SELECT "COLUMN_NAME" FROM DATABASE.SCHEMA.TABLE WHERE ... ``` (Adjust "DATABASE", "SCHEMA", and "TABLE" to match actual names, ensure all column names are enclosed in double quotations)
Follow the answer format like: To find the imported Python modules and R libraries from the GitHub sample files, we analyze the contents of the files in the `SAMPLE_CONTENTS` table. Below is the requested information:

```csv
Language,Module_or_Library,Count
Python,module_name1:str,count1:int
Python,module_name2:str,count2:int
...
R,library_name1:str,count3:int
R,library_name2:str,count4:int
...
```

This table lists the imported modules and libraries along with their occurrence counts, sorted by language and then by the number of occurrences in descending order..
Here are some useful tips for answering:
When performing a UNION operation on many tables, ensure that all table names are explicitly listed. Union first and then add condition and selection. e.g. SELECT "col1", "col2" FROM (TABLE1 UNION ALL TABLE2) WHERE ...; Don't write sqls as (SELECT col1, col2 FROM TABLE1 WHERE ...) UNION ALL (SELECT col1, col2 FROM TABLE2 WHERE ...); Don't use ['-- Include all', '-- Omit', '-- Continue', '-- Union all', '-- ...', '-- List all', '-- Replace this', '-- Each table', '-- Add other'] to omit any table. Table names here: ({database name: {schema name: {table name}}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_FILES', 'SAMPLE_REPOS']}}

For string-matching scenarios, if the string is decided, don't use fuzzy query, and avoid using REGEXP. e.g. Get the object's title contains the word "book"
However, if the string is not decided, you may use fuzzy query and ignore upper or lower case. e.g. Get articles that mention "education".
When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.
When using ORDER BY, if there are duplicate values in the primary sort column, sort by an additional column as a secondary criterion.Keep all decimals to four decimal places.
Be careful of information in nested columns. e.g. When it comes to completed purchase, `hits.eCommerceAction.action_type` Indicates the type of ecommerce action and '6' represents completed purchases.
Be careful one country may have different country_name and country_region in different columns in a database.
Don't be misled by examples. For instance, a question related to Android development on StackOverflow might include tags like 'android-layout,' 'android-activity,' or 'android-intent.' However, you should not limit your analysis to just these three tags; instead, consider all tags related to Android: "tags" LIKE '%android%'.

2025-01-25 07:56:12 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:56:12 - root - INFO - To find the imported Python modules and R libraries from the GitHub sample files, we'll analyze the contents of the files in the `SAMPLE_CONTENTS` table joined with `SAMPLE_FILES` to filter for Python and R files. Below is the SQL query that retrieves the requested information:

```sql
WITH python_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bimport\\s+([a-zA-Z0-9_\\.]+)',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
python_from_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bfrom\\s+([a-zA-Z0-9_\\.]+)\\s+import',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
r_imports AS (
  SELECT
    'R' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\b(library|require)\\s*\\(\\s*["'']?([a-zA-Z0-9_\\.]+)["'']?\\s*\\)',
      1, 1, 'i', 2
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.r'
    AND sc."content" IS NOT NULL
)
SELECT "Language", "Module_or_Library", COUNT(*) AS "Count"
FROM (
  SELECT * FROM python_imports
  UNION ALL
  SELECT * FROM python_from_imports
  UNION ALL
  SELECT * FROM r_imports
) t
WHERE "Module_or_Library" IS NOT NULL
GROUP BY "Language", "Module_or_Library"
ORDER BY "Language", "Count" DESC;
```

This query performs the following steps:

1. **Extract Python Imports:**
   - Uses regular expressions to extract module names from `import` and `from ... import` statements in Python files (`*.py`).
2. **Extract R Libraries:**
   - Extracts library names from `library()` or `require()` calls in R files (`*.R` or `*.r`).
3. **Combine Results:**
   - Combines the results from Python and R extractions using `UNION ALL`.
4. **Aggregate Counts:**
   - Groups the combined results by `Language` and `Module_or_Library` and counts the occurrences.
5. **Sort Results:**
   - Sorts the final output by `Language` and then by `Count` in descending order.

The output will be a table in the following format:

```csv
Language,Module_or_Library,Count
Python,module_name1,count1
Python,module_name2,count2
...
R,library_name1,count3
R,library_name2,count4
...
```

This table lists the imported modules and libraries along with their occurrence counts, sorted by language and then by the number of occurrences in descending order.
2025-01-25 07:56:12 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:56:12 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:56:12 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:56:14 - snowflake.connector.cursor - INFO - Number of results in first chunk: 8
2025-01-25 07:56:14 - snowflake.connector.connection - INFO - closed
2025-01-25 07:56:14 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:56:14 - root - INFO - itercount: 1
2025-01-25 07:56:14 - root - INFO - 0
2025-01-25 07:56:14 - root - INFO - results: 
Language,Module_or_Library,Count
Python,common,1
Python,rekall.plugins.linux,1
Python,unicode_literals,1
Python,copy,1
Python,__future__,1
Python,deepcopy,1
Python,diesel,1
Python,quickstart,1


2025-01-25 07:56:14 - root - INFO - Please check the answer again by reviewing Can you find the imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order., reviewing Relevant Tables and Columns and Possible Conditions and then give the final SQL query. Don't output other queries. If you think the answer is right, just output the current SQL.
Keep all decimals to four decimal places.
The answer format should be like: To find the imported Python modules and R libraries from the GitHub sample files, we analyze the contents of the files in the `SAMPLE_CONTENTS` table. Below is the requested information:

```csv
Language,Module_or_Library,Count
Python,module_name1:str,count1:int
Python,module_name2:str,count2:int
...
R,library_name1:str,count3:int
R,library_name2:str,count4:int
...
```

This table lists the imported modules and libraries along with their occurrence counts, sorted by language and then by the number of occurrences in descending order. The answer should match the number of rows, the column name of the format and the filled values in the format (e.g. filled year or month). Don't output extra rows or nested rows!
Current snswer: 
Language,Module_or_Library,Count
Python,common,1
Python,rekall.plugins.linux,1
Python,unicode_literals,1
Python,copy,1
Python,__future__,1
Python,deepcopy,1
Python,diesel,1
Python,quickstart,1
Current sql:
WITH python_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bimport\\s+([a-zA-Z0-9_\\.]+)',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
python_from_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bfrom\\s+([a-zA-Z0-9_\\.]+)\\s+import',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
r_imports AS (
  SELECT
    'R' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\b(library|require)\\s*\\(\\s*["'']?([a-zA-Z0-9_\\.]+)["'']?\\s*\\)',
      1, 1, 'i', 2
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.r'
    AND sc."content" IS NOT NULL
)
SELECT "Language", "Module_or_Library", COUNT(*) AS "Count"
FROM (
  SELECT * FROM python_imports
  UNION ALL
  SELECT * FROM python_from_imports
  UNION ALL
  SELECT * FROM r_imports
) t
WHERE "Module_or_Library" IS NOT NULL
GROUP BY "Language", "Module_or_Library"
ORDER BY "Language", "Count" DESC;When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.

2025-01-25 07:56:53 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:56:53 - root - INFO - ```sql
WITH python_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bimport\\s+([a-zA-Z0-9_\\.]+)',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
python_from_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bfrom\\s+([a-zA-Z0-9_\\.]+)\\s+import',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
r_imports AS (
  SELECT
    'R' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\b(library|require)\\s*\\(\\s*["'']?([a-zA-Z0-9_\\.]+)["'']?\\s*\\)',
      1, 1, 'i', 2
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.r'
    AND sc."content" IS NOT NULL
)
SELECT "Language", "Module_or_Library", COUNT(*) AS "Count"
FROM (
  SELECT * FROM python_imports
  UNION ALL
  SELECT * FROM python_from_imports
  UNION ALL
  SELECT * FROM r_imports
) t
WHERE "Module_or_Library" IS NOT NULL
GROUP BY "Language", "Module_or_Library"
ORDER BY "Language", "Count" DESC NULLS LAST;
```
2025-01-25 07:56:53 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:56:53 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:56:53 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:56:55 - snowflake.connector.cursor - INFO - Number of results in first chunk: 8
2025-01-25 07:56:55 - snowflake.connector.connection - INFO - closed
2025-01-25 07:56:55 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:56:55 - root - INFO - itercount: 2
2025-01-25 07:56:55 - root - INFO - 0
2025-01-25 07:56:55 - root - INFO - results: 
Language,Module_or_Library,Count
Python,deepcopy,1
Python,copy,1
Python,rekall.plugins.linux,1
Python,unicode_literals,1
Python,diesel,1
Python,common,1
Python,__future__,1
Python,quickstart,1


2025-01-25 07:56:55 - root - INFO - Please check the answer again by reviewing Can you find the imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order., reviewing Relevant Tables and Columns and Possible Conditions and then give the final SQL query. Don't output other queries. If you think the answer is right, just output the current SQL.
Keep all decimals to four decimal places.
The answer format should be like: To find the imported Python modules and R libraries from the GitHub sample files, we analyze the contents of the files in the `SAMPLE_CONTENTS` table. Below is the requested information:

```csv
Language,Module_or_Library,Count
Python,module_name1:str,count1:int
Python,module_name2:str,count2:int
...
R,library_name1:str,count3:int
R,library_name2:str,count4:int
...
```

This table lists the imported modules and libraries along with their occurrence counts, sorted by language and then by the number of occurrences in descending order. The answer should match the number of rows, the column name of the format and the filled values in the format (e.g. filled year or month). Don't output extra rows or nested rows!
Current snswer: 
Language,Module_or_Library,Count
Python,deepcopy,1
Python,copy,1
Python,rekall.plugins.linux,1
Python,unicode_literals,1
Python,diesel,1
Python,common,1
Python,__future__,1
Python,quickstart,1
Current sql:
WITH python_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bimport\\s+([a-zA-Z0-9_\\.]+)',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
python_from_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bfrom\\s+([a-zA-Z0-9_\\.]+)\\s+import',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
r_imports AS (
  SELECT
    'R' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\b(library|require)\\s*\\(\\s*["'']?([a-zA-Z0-9_\\.]+)["'']?\\s*\\)',
      1, 1, 'i', 2
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.r'
    AND sc."content" IS NOT NULL
)
SELECT "Language", "Module_or_Library", COUNT(*) AS "Count"
FROM (
  SELECT * FROM python_imports
  UNION ALL
  SELECT * FROM python_from_imports
  UNION ALL
  SELECT * FROM r_imports
) t
WHERE "Module_or_Library" IS NOT NULL
GROUP BY "Language", "Module_or_Library"
ORDER BY "Language", "Count" DESC NULLS LAST;
2025-01-25 07:57:16 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-01-25 07:57:16 - root - INFO - ```sql
WITH python_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bimport\\s+([a-zA-Z0-9_\\.]+)',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
python_from_imports AS (
  SELECT
    'Python' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\bfrom\\s+([a-zA-Z0-9_\\.]+)\\s+import',
      1, 1, 'i', 1
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.py'
    AND sc."content" IS NOT NULL
),
r_imports AS (
  SELECT
    'R' AS "Language",
    REGEXP_SUBSTR(
      sc."content",
      '\\b(library|require)\\s*\\(\\s*["'']?([a-zA-Z0-9_\\.]+)["'']?\\s*\\)',
      1, 1, 'i', 2
    ) AS "Module_or_Library"
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" sf
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" sc
    ON sc."id" = sf."id"
  WHERE sf."path" ILIKE '%.r'
    AND sc."content" IS NOT NULL
)
SELECT "Language", "Module_or_Library", COUNT(*) AS "Count"
FROM (
  SELECT * FROM python_imports
  UNION ALL
  SELECT * FROM python_from_imports
  UNION ALL
  SELECT * FROM r_imports
) t
WHERE "Module_or_Library" IS NOT NULL
GROUP BY "Language", "Module_or_Library"
ORDER BY "Language", "Count" DESC NULLS LAST;
```
2025-01-25 07:57:16 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.11.11, Platform: Linux-6.1.62-x86_64-with-glibc2.31
2025-01-25 07:57:16 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-01-25 07:57:16 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-01-25 07:57:17 - snowflake.connector.cursor - INFO - Number of results in first chunk: 8
2025-01-25 07:57:17 - snowflake.connector.connection - INFO - closed
2025-01-25 07:57:17 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-01-25 07:57:17 - root - INFO - itercount: 3
2025-01-25 07:57:17 - root - INFO - 0
2025-01-25 07:57:17 - root - INFO - Total iteration counts: 3
