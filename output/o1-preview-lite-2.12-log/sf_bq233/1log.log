2025-02-12 19:04:50 - openai._base_client - INFO - Retrying request to /chat/completions in 0.406701 seconds
2025-02-12 19:05:58 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:05:58 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:05:58 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:05:58 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:05:59 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-02-12 19:05:59 - snowflake.connector.connection - INFO - closed
2025-02-12 19:05:59 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:06:00 - root - INFO - SQL:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
Results:
Too long, hard cut:
path
push_notifications/migrations/0002_auto_20160106_0850.py
Maximum_Subarray/Python/shivam5992/kadane.py
Caesar_Cipher/Python/kennyledet/caesar_cipher.py
setup.py
pgcrypto_expressions/test/settings.py
ec2_security_groups_dumper/__init__.py
dirigible/info_pages/__init__.py
tests/python/tests/test_updatecollection.py
tests/testproject/settings.py
yandex_money/api.py
google/cloud/dataflow/utils/__init__.py
examples/Rest/ex20_get_ilo_nic.py
try/for_now.py
node_modules/node-sass/node_modules/node-gyp/gyp/pylib/gyp/easy_xml.py
vendor/distribute-0.6.36/easy_install.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
src/__init__.py
image_cropping/utils.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/GenFds/FfsInfStatement.py
src/VBox/Additions/common/crOpenGL/stub_common.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/Ecc/CodeFragmentCollector.py
project/admin/__init__.py
examples/backend/myproject/myproject/wsgi.py
tools/perf/tests/attr.py
health_check_celery/tasks.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/core/tests/test_multiarray_assignment.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/traversal/tests/test_dfs.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/core/cversions.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/lib/tests/test__datasource.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/f2py/f2py2e.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
path.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
servers/youtube.py
cloudify/manager.py
misc/webdriver-w3c-tests/cookie/cookie_test.py
Lessons/lesson15.py
layout_scanner.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
third_party/catapult/third_party/webtest/webtest/forms.py
tools/gyp/test/make_global_settings/full-toolchain/gyptest-make_global_settings.py
third_party/jinja2/compiler.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/perf/page_sets/page_reload_cases.py
third_party/jinja2/_compat.py
third_party/chromite/cbuildbot/stages/build_stages.py
third_party/chromite/lib/terminal.py
tools/telemetry/telemetry/internal/backends/mandoline/config.py
third_party/cython/src/Cython/Plex/Lexicons.py
tools/gyp/test/sanitize-rule-names/gyptest-sanitize-rule-names.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
third_party/google_appengine_cloudstorage/cloudstorage/common.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
tools/gyp/test/ios/gyptest-per-config-settings.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
third_party/catapult/tracing/tracing_build/__init__.py
tools/telemetry/telemetry/internal/backends/chrome_inspector/inspector_page_unittest.py
build/android/pylib/local/__init__.py
native_client/tools/scons_to_gn/print_data.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
tools/cr/cr/actions/linux.py
tools/swarming_client/third_party/rsa/rsa/transform.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
tools/gyp/test/win/gyptest-link-enable-winrt.py
third_party/chromite/scripts/dep_tracker_unittest.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/datapipeline/layer1.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
third_party/chromite/lib/patch.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/mws/__init__.py
tools/telemetry/third_party/modulegraph/modulegraph_tests/testpkg-edgedata/pkg/toplevel_conditional_import_existing.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/gs/bucketlistresultset.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
buildtools/clang_format/script/clang-format.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
third_party/tlslite/tlslite/utils/cryptomath.py
tools/telemetry/third_party/gsutilz/gslib/addlhelp/projects.py
tools/deep_memory_profiler/subcommands/buckets.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Pl

2025-02-12 19:06:00 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:06:00 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:06:00 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:06:01 - snowflake.connector.cursor - INFO - Number of results in first chunk: 25
2025-02-12 19:06:01 - snowflake.connector.connection - INFO - closed
2025-02-12 19:06:01 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:06:01 - root - INFO - SQL:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.r'
   LIMIT 100;
Results:
path
data-prep/split-train-test.r
experimental/R/rapachetest.r
src/tools/make-os-ext.r
run-all.r
iRODS/clients/icommands/test/rules/rulemsiSetRescSortScheme.r
iRODS/clients/icommands/test/rules/testsuiteForLcov.r
qiime/support_files/R/randomforests_util.r
R/clonevol.r
ppapi/native_client/tests/ppapi_messaging/ppapi_messaging.r
jni/SDL_mixer/external/libmikmod-3.1.12/macintosh/libmikmodversion.r
R/when.r
inst/examples/test_performance/demo_allgather_spmd.r
inst/examples/test_cs/simple.server.r
books/all-of-statistics/=Rprograms/stockprice.r
Stochastic/Exercise1_1.r
src/tools/make-natives.r
R/tabledata.r
build_dir/toolchain-arm_v5te_gcc-linaro_uClibc-0.9.32_eabi/binutils-2.20.1/ld/testsuite/ld-powerpc/tlsexetoc.r
R/msssr2tc.r
binr/old.rarc2/t/loop.r
examples/PlotDailyData/plot-daily-data-using-stdin-barb-detailed-instrumentation.r
examples/consoleSource/source1.r
src/01.Opt.3 - Memory Management Functions.r
dbug/user.r
ld/testsuite/ld-arm/tls-gdesc.r

2025-02-12 19:06:01 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:06:01 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:06:01 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:06:02 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-02-12 19:06:02 - snowflake.connector.connection - INFO - closed
2025-02-12 19:06:02 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:06:02 - root - INFO - SQL:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
   LIMIT 100;
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if has

2025-02-12 19:06:02 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:06:02 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:06:02 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:06:04 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:06:04 - snowflake.connector.connection - INFO - closed
2025-02-12 19:06:04 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:06:29 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:06:29 - root - INFO - ```sql
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py'
LIMIT 100;
```
2025-02-12 19:06:29 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:06:29 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:06:29 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:06:31 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-02-12 19:06:31 - snowflake.connector.connection - INFO - closed
2025-02-12 19:06:31 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:07:42 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:07:42 - root - INFO - SQL:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py'
LIMIT 100;
Results:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties

2025-02-12 19:07:42 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:07:42 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:07:42 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:07:43 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-02-12 19:07:43 - snowflake.connector.connection - INFO - closed
2025-02-12 19:07:43 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:07:43 - root - INFO - SQL:
SELECT DISTINCT f."repo_name"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
Results:
repo_name
kennyledet/Algorithm-Implementations
percolate/ec2-security-groups-dumper
pythonanywhere/dirigible-spreadsheet
rpm-software-management/createrepo_c
sbaechler/django-multilingual-search
yandex-money/yandex-money-sdk-python
Sorosliu1029/Rule-based_Expert_System
daigofuji/jekyll-foundation-5-starter
jonasundderwolf/django-image-cropping
michaelhelmick/django-bootstrap-admin
mw-ferretti/angular-resource-tastypie
Anonim47/android_kernel_lenovo_a7000-a
KristianOellegaard/django-health-check
designalyze/python-tutorials-for-rhino
dpapathanasiou/pdfminer-layout-scanner
globocom/GloboNetworkAPI-client-python
jonathanchu/django-custom-user-example
DarkEnergyScienceCollaboration/Twinkles
StephenPower/CollectorCity-Market-Place
bt3gl/Neat-Problems-in-Python-and-Flask
CyanogenMod/android_kernel_xiaomi_cancro
SandPox/android_kernel_samsung_kyleproxx
benjaminwilson/word2vec-norm-experiments
felipead/cassandra-to-elasticsearch-sync
tum-i22/indistinguishability-obfuscation
OnePlusOSS/android_kernel_oneplus_msm8994
Azure-Readiness/hol-azure-machine-learning
CenterForOpenScience/modular-file-renderer
sualfred/script.screensaver.ftvscreensaver
CyanogenMod/android_kernel_samsung_smdk4412
andela-uawili/django-bucketlist-application
GoogleCloudPlatform/appengine-django-skeleton
MuhammedHasan/labyrinth-application-of-dynamic-connectivity-problem
osrg/ryu
ciaa/linux
livid/v2ex
coreos/etcd
ivacf/ivanc
salilab/imp
trentm/json
domokit/mojo
chiulab/surpi
eerimoq/simba
moonso/genmod
r3b/phantomjs
demotu/BMC
rozofs/rozofs
frappe/erpnext
jserv/codezero
CZ-NIC/turris-os
cmu-mtlab/meteor
markokr/skytools
gnuradio/gnuradio
springhack/alxwvj
SpectraLogic/samba
eugene1g/phantomjs
phacility/arcanist
redhat-cip/edeploy
python-constraint/python-constraint
ingadhoc/odoo-infrastructure
LumPenPacK/NetworkExtractionFromImages
cloudify-cosmo/cloudify-plugins-common
hughperkins/kgsgo-dataset-preprocessor
AdventureLookup/adventurelookup-backend
chrisdev/wagtail-cookiecutter-foundation
cisco-system-traffic-generator/trex-core
liaohuqiu/android-ClipboardManagerCompat
googleads/googleads-dfa-reporting-samples
codeforamerica/pittsburgh-purchasing-suite
cwaldbieser/jhub_remote_user_authenticator
evidation-health/ContinuousTimeMarkovModel
Ramshackle-Jamathon/Interactive-Experiments
CumulusNetworks/cumulus-linux-ansible-modules
alexandrebarachant/Grasp-and-lift-EEG-challenge
wrichert/BuildingMachineLearningSystemsWithPython
Tommy-Geenexus/android_kernel_sony_msm8994_suzuran_5.x
cernanalysispreservation/analysis-preservation.cern.ch
bup/bup
PDAL/PDAL
cylc/cylc
nipy/dipy
mnot/redbot
rdkit/rdkit
ameily/pypsi
lpryszcz/bin
aaps/MCtricky
cvxopt/cvxopt
google/libkml
nfco/netforce
nipy/PySurfer
Autodesk/quack
Erotemic/ibeis
mapsme/omim
joowani/dtags
libkml/libkml
arq5x/lumpy-sv
lab11/M-ulator
Jenyay/outwiker
arthurv/OpenTLD
naver/nbase-arc

2025-02-12 19:07:43 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:07:43 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:07:43 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:07:44 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-02-12 19:07:44 - snowflake.connector.connection - INFO - closed
2025-02-12 19:07:44 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:07:45 - root - INFO - SQL:
SELECT f."repo_name", COUNT(*) AS "python_file_count"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   GROUP BY f."repo_name"
   ORDER BY "python_file_count" DESC
   LIMIT 100;
Results:
repo_name,python_file_count
anhstudios/swganh,135
mollstam/UnrealPy,90
iridium-browser/iridium-browser-ubuntu,60
JetBrains/intellij-community,54
openhatch/oh-mainline,46
bugvm/bugvm-studio,46
hortonworks/hortonworks-sandbox,38
espadrine/opera,37
cgstudiomap/cgstudiomap,37
mirror/chromium,36
cloudera/hue,33
gabrielfalcao/lettuce,32
hynnet/hiwifi-openwrt-HC5661-HC5761,29
justathoughtor2/atomicApe,29
dstockwell/chromium,27
nwjs/chromium.src,25
Samsung/ChromiumGStreamerBackend,25
Moguri/BlenderPanda,24
neeker/chromium_extract,23
IONISx/edx-platform,23
kbengine/kbengine,22
krintoxi/NoobSecToolkit,22
Microvellum/Fluid-Designer,21
harmy/kbengine,21
cnsoft/kbengine-cocos2dx,20
OCA/OpenUpgrade,20
damonkohler/sl4a,20
acatadog/kbengine,19
openfun/edx-platform,18
pymo/pymo,18
qunaibit/zippy-mirror,18
Southpaw-TACTIC/TACTIC,17
crosswalk-project/chromium-crosswalk,17
scheib/chromium,16
dimagi/commcare-hq,16
catapult-project/catapult,16
fabioz/Pydev,16
Abjad/abjad,16
frappe/erpnext,16
django-nonrel/django,15
django/django,15
openstack/nova,15
OCA/OCB,15
openmv/micropython,15
tendenci/tendenci,14
ms-iot/python,14
redhat-openstack/nova,14
gameduell/duell,14
azatoth/scons,14
qpython-android/QPython3-core,14
OsirisSPS/osiris-sps,14
3dprinteros/3dprinteros-client,14
pelya/commandergenius,14
SickRage/SickRage,13
moesy/AWS-Lambda-ML-Microservice-Skeleton,13
LumPenPacK/NetworkExtractionFromImages,13
qgis/QGIS,13
burakbayramli/kod,13
getsentry/sentry,13
daoluan/decode-Django,13
edx/edx-platform,13
snakeleon/YouCompleteMe-x64,13
CivicTechTO/open-cabinet,12
drawquest/drawquest-web,12
vlegoff/tsunami,12
golismero/golismero,12
deanhiller/databus,12
twitter/pants,12
odoo/odoo,12
hlzz/dotfiles,12
orangeduck/PyAutoC,12
pyfa-org/Pyfa,12
FFMG/myoddweb.piger,12
Stanford-Online/edx-platform,11
VisTrails/VisTrails,11
XX-net/XX-Net,11
goldeneye-source/ges-code,11
SciTools/iris,11
Jenyay/outwiker,11
kuri65536/python-for-android,11
Azure/azure-sdk-for-python,11
duducosmos/pgs4a,11
kovidgoyal/calibre,11
DarkFenX/Pyfa,11
loopCM/chromium,11
bq/bitbloq-offline,10
ChromiumWebApps/chromium,10
NeuralEnsemble/neuroConstruct,10
googleads/googleads-python-lib,10
google/ggrc-core,10
divio/django,10
GoogleChrome/big-rig,10
aliyun/oss-ftp,10
CubicERP/odoo,10
sametmax/Django--an-app-at-a-time,10
pantsbuild/pants,10
luci/luci-py,10
bruderstein/PythonScript,10
openstack/heat,10
openstack/tempest,10

2025-02-12 19:07:45 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:07:45 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:07:45 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:07:46 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-02-12 19:07:46 - snowflake.connector.connection - INFO - closed
2025-02-12 19:07:46 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:07:47 - root - INFO - SQL:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE AND c."content" ILIKE '%import %'
   LIMIT 100;
Results:
Too long, hard cut:
content
"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.m

2025-02-12 19:07:47 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:07:47 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:07:47 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:07:48 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:07:48 - snowflake.connector.connection - INFO - closed
2025-02-12 19:07:48 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:08:43 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:08:43 - root - INFO - ```sql
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
LIMIT 100;
```
2025-02-12 19:08:43 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:08:43 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:08:43 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:08:44 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:08:45 - snowflake.connector.connection - INFO - closed
2025-02-12 19:08:45 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:09:42 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:09:42 - root - INFO - SQL:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
LIMIT 100;
Results:
Too long, hard cut:
content
"/*
 * rtc-ds1305.c -- driver for DS1305 and DS1306 SPI RTC chips
 *
 * Copyright (C) 2008 David Brownell
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 */
#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/bcd.h>
#include <linux/slab.h>
#include <linux/rtc.h>
#include <linux/workqueue.h>

#include <linux/spi/spi.h>
#include <linux/spi/ds1305.h>
#include <linux/module.h>


/*
 * Registers ... mask DS1305_WRITE into register address to write,
 * otherwise you're reading it.  All non-bitmask values are BCD.
 */
#define DS1305_WRITE		0x80


/* RTC date/time ... the main special cases are that we:
 *  - Need fancy ""hours"" encoding in 12hour mode
 *  - Don't rely on the ""day-of-week"" field (or tm_wday)
 *  - Are a 21st-century clock (2000 <= year < 2100)
 */
#define DS1305_RTC_LEN		7		/* bytes for RTC regs */

#define DS1305_SEC		0x00		/* register addresses */
#define DS1305_MIN		0x01
#define DS1305_HOUR		0x02
#	define DS1305_HR_12		0x40	/* set == 12 hr mode */
#	define DS1305_HR_PM		0x20	/* set == PM (12hr mode) */
#define DS1305_WDAY		0x03
#define DS1305_MDAY		0x04
#define DS1305_MON		0x05
#define DS1305_YEAR		0x06


/* The two alarms have only sec/min/hour/wday fields (ALM_LEN).
 * DS1305_ALM_DISABLE disables a match field (some combos are bad).
 *
 * NOTE that since we don't use WDAY, we limit ourselves to alarms
 * only one day into the future (vs potentially up to a week).
 *
 * NOTE ALSO that while we could generate once-a-second IRQs (UIE), we
 * don't currently support them.  We'd either need to do it only when
 * no alarm is pending (not the standard model), or to use the second
 * alarm (implying that this is a DS1305 not DS1306, *and* that either
 * it's wired up a second IRQ we know, or that INTCN is set)
 */
#define DS1305_ALM_LEN		4		/* bytes for ALM regs */
#define DS1305_ALM_DISABLE	0x80

#define DS1305_ALM0(r)		(0x07 + (r))	/* register addresses */
#define DS1305_ALM1(r)		(0x0b + (r))


/* three control registers */
#define DS1305_CONTROL_LEN	3		/* bytes of control regs */

#define DS1305_CONTROL		0x0f		/* register addresses */
#	define DS1305_nEOSC		0x80	/* low enables oscillator */
#	define DS1305_WP		0x40	/* write protect */
#	define DS1305_INTCN		0x04	/* clear == only int0 used */
#	define DS1306_1HZ		0x04	/* enable 1Hz output */
#	define DS1305_AEI1		0x02	/* enable ALM1 IRQ */
#	define DS1305_AEI0		0x01	/* enable ALM0 IRQ */
#define DS1305_STATUS		0x10
/* status has just AEIx bits, mirrored as IRQFx */
#define DS1305_TRICKLE		0x11
/* trickle bits are defined in <linux/spi/ds1305.h> */

/* a bunch of NVRAM */
#define DS1305_NVRAM_LEN	96		/* bytes of NVRAM */

#define DS1305_NVRAM		0x20		/* register addresses */


struct ds1305 {
	struct spi_device	*spi;
	struct rtc_device	*rtc;

	struct work_struct	work;

	unsigned long		flags;
#define FLAG_EXITING	0

	bool			hr12;
	u8			ctrl[DS1305_CONTROL_LEN];
};


/*----------------------------------------------------------------------*/

/*
 * Utilities ...  tolerate 12-hour AM/PM notation in case of non-Linux
 * software (like a bootloader) which may require it.
 */

static unsigned bcd2hour(u8 bcd)
{
	if (bcd & DS1305_HR_12) {
		unsigned	hour = 0;

		bcd &= ~DS1305_HR_12;
		if (bcd & DS1305_HR_PM) {
			hour = 12;
			bcd &= ~DS1305_HR_PM;
		}
		hour += bcd2bin(bcd);
		return hour - 1;
	}
	return bcd2bin(bcd);
}

static u8 hour2bcd(bool hr12, int hour)
{
	if (hr12) {
		hour++;
		if (hour <= 12)
			return DS1305_HR_12 | bin2bcd(hour);
		hour -= 12;
		return DS1305_HR_12 | DS1305_HR_PM | bin2bcd(hour);
	}
	return bin2bcd(hour);
}

/*----------------------------------------------------------------------*/

/*
 * Interface to RTC framework
 */

static int ds1305_alarm_irq_enable(struct device *dev, unsigned int enabled)
{
	struct ds1305	*ds1305 = dev_get_drvdata(dev);
	u8		buf[2];
	long		err = -EINVAL;

	buf[0] = DS1305_WRITE | DS1305_CONTROL;
	buf[1] = ds1305->ctrl[0];

	if (enabled) {
		if (ds1305->ctrl[0] & DS1305_AEI0)
			goto done;
		buf[1] |= DS1305_AEI0;
	} else {
		if (!(buf[1] & DS1305_AEI0))
			goto done;
		buf[1] &= ~DS1305_AEI0;
	}
	err = spi_write_then_read(ds1305->spi, buf, sizeof(buf), NULL, 0);
	if (err >= 0)
		ds1305->ctrl[0] = buf[1];
done:
	return err;

}


/*
 * Get/set of date and time is pretty normal.
 */

static int ds1305_get_time(struct device *dev, struct rtc_time *time)
{
	struct ds1305	*ds1305 = dev_get_drvdata(dev);
	u8		addr = DS1305_SEC;
	u8		buf[DS1305_RTC_LEN];
	int		status;

	/* Use write-then-read to get all the date/time registers
	 * since dma from stack is nonportable
	 */
	status = spi_write_then_read(ds1305->spi, &addr, sizeof(addr),
			buf, sizeof(buf));
	if (status < 0)
		return status;

	dev_vdbg(dev, ""%s: %02x %02x %02x, %02x %02x %02x %02x\n"",
		""read"", buf[0], buf[1], buf[2], buf[3],
		buf[4], buf[5], buf[6]);

	/* Decode the registers

2025-02-12 19:09:42 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:09:42 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:09:42 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:09:43 - snowflake.connector.connection - INFO - closed
2025-02-12 19:09:44 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:09:57 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:09:57 - root - INFO - ```sql
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^(import .+|from .+ import .+)', 1, 1, 'm') AS "import_statement"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
```
2025-02-12 19:09:57 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:09:57 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:09:57 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:09:59 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-02-12 19:09:59 - snowflake.connector.connection - INFO - closed
2025-02-12 19:09:59 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:10:50 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:10:50 - root - INFO - SQL:
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^(import .+|from .+ import .+)', 1, 1, 'm') AS "import_statement"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
Results:
import_statement
"from diesel import quickstart, quickstop, sleep"
from copy import deepcopy
from __future__ import unicode_literals
from rekall.plugins.linux import common

2025-02-12 19:10:50 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:10:50 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:10:50 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:10:51 - snowflake.connector.connection - INFO - closed
2025-02-12 19:10:51 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:11:26 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:11:26 - root - INFO - ```sql
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM (
    SELECT SPLIT(c."content", '\\n') AS lines
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE f."path" LIKE '%.py'
  ), LATERAL FLATTEN(input => lines) AS t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
```

```sql
SELECT library_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, 'library\\(([^)]+)\\)', 1, 1, '', 1) AS library_name
  FROM (
    SELECT SPLIT(c."content", '\\n') AS lines
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE f."path" LIKE '%.r'
  ), LATERAL FLATTEN(input => lines) AS t
)
WHERE library_name IS NOT NULL
GROUP BY library_name
ORDER BY "count" DESC
LIMIT 100;
```

```sql
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^from\\s+(\\w+)', 1, 1, '', 1) AS "module_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py'
LIMIT 100;
```

```sql
SELECT COUNT(*) AS "total_python_files"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
WHERE f."path" LIKE '%.py';
```

```sql
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%'
LIMIT 100;
```

```sql
SELECT DISTINCT REGEXP_SUBSTR("path", '\\.[^.]+$') AS "file_extension"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
WHERE "path" LIKE '%.%'
LIMIT 100;
```
2025-02-12 19:11:26 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:11:26 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:11:26 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:11:28 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:11:28 - snowflake.connector.connection - INFO - closed
2025-02-12 19:11:28 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:11:48 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:11:48 - root - INFO - ```sql
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM (
    SELECT SPLIT(c."content", '\\n') AS lines
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE f."path" LIKE '%.py'
  ), LATERAL FLATTEN(input => lines) AS t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
```
2025-02-12 19:11:48 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:11:48 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:11:48 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:11:49 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:11:49 - snowflake.connector.connection - INFO - closed
2025-02-12 19:11:49 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:12:17 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:12:17 - root - INFO - ```sql
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM (
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
  ), LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
```
2025-02-12 19:12:17 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:12:17 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:12:17 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:12:18 - snowflake.connector.connection - INFO - closed
2025-02-12 19:12:18 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:12:19 - root - INFO - SQL:
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM (
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
  ), LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
Results:
000904 (42000): 01ba5960-0004-9c22-0001-11c301112932: SQL compilation error: error line 7 at position 36
invalid identifier 'C."content"'
2025-02-12 19:12:19 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:12:19 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:12:19 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:12:20 - snowflake.connector.connection - INFO - closed
2025-02-12 19:12:20 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:12:38 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:12:38 - root - INFO - ```sql
SELECT library_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, 'library\\(([^)]+)\\)', 1, 1, '', 1) AS library_name
  FROM (
    SELECT SPLIT(c."content", '\\n') AS lines
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE f."path" LIKE '%.r'
  ), LATERAL FLATTEN(input => lines) AS t
)
WHERE library_name IS NOT NULL
GROUP BY library_name
ORDER BY "count" DESC
LIMIT 100;
```
2025-02-12 19:12:38 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:12:38 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:12:38 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:12:40 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:12:40 - snowflake.connector.connection - INFO - closed
2025-02-12 19:12:40 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:13:03 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:13:03 - root - INFO - ```sql
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM
    "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c,
    LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
```
2025-02-12 19:13:03 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:13:03 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:13:03 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:13:05 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-02-12 19:13:05 - snowflake.connector.connection - INFO - closed
2025-02-12 19:13:05 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:13:35 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:13:35 - root - INFO - SQL:
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM
    "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c,
    LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
Results:
MODULE_NAME,count
React,28
T,16
os,15
java,10
assert,9
Ember,9
numpy,9
org,8
Result,8
QtQuick,7
unittest,7
sys,7
logging,6
NodeProcess,6
com,6
re,6
Validator,4
path,4
TimeoutError,4
Condition,4
sinon,3
_,3
pytest,3
math,2
qbs,2
RegExp,2
datetime,2
httplib,2
Cocoa,2
time,2
misago,2
DuplexStream,2
fs,2
Process,2
asyncio,2
PaperSidenav,1
type,1
Electron,1
ChildProcess,1
actionTypes,1
UIKit,1
matplotlib,1
DarkTheme,1
FlowGrapher,1
ddt,1
asyncore,1
io,1
subpackage,1
http,1
Adafruit_BBIO,1
Orange,1
createNode,1
redis,1
Database,1
Instance,1
MachineHypermediaToolkit,1
NumberUtils,1
sendgrid,1
Base,1
decimal,1
jsonPath,1
assign,1
Page,1
RollupCjs,1
webpack,1
defaults,1
json,1
stomp,1
core,1
threading,1
PaginatingHandler,1
AssemblyKeys,1
THREE,1
ConfigParser,1
gulp,1
networkx,1
ObjectMapper,1
argparse,1
express,1
pandas,1
getpass,1
fileinput,1
fireEvent,1
Foundation,1
Session,1
qualified,1
subprocess,1
System,1
dep1,1
sbt,1
itertools,1
classnames,1
essentia,1
Sights,1
EmberObject,1
random,1
static,1
XCTest,1
collections,1
once,1

2025-02-12 19:13:35 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:13:35 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:13:35 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:13:36 - snowflake.connector.connection - INFO - closed
2025-02-12 19:13:36 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:13:54 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:13:54 - root - INFO - ```sql
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^from\\s+(\\w+)', 1, 1, 'm', 1) AS "module_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
```
2025-02-12 19:13:54 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:13:54 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:13:54 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:13:55 - snowflake.connector.cursor - INFO - Number of results in first chunk: 4
2025-02-12 19:13:55 - snowflake.connector.connection - INFO - closed
2025-02-12 19:13:56 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:14:18 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:14:18 - root - INFO - SQL:
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^from\\s+(\\w+)', 1, 1, 'm', 1) AS "module_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
Results:
module_name
rekall
copy
diesel
__future__

2025-02-12 19:14:18 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:14:18 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:14:18 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:14:19 - snowflake.connector.cursor - INFO - Number of results in first chunk: 1
2025-02-12 19:14:19 - snowflake.connector.connection - INFO - closed
2025-02-12 19:14:19 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:14:20 - root - INFO - SQL:
SELECT COUNT(*) AS "total_python_files"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON f."id" = c."id"
    WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
    LIMIT 100;
Results:
total_python_files
4

2025-02-12 19:14:20 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:14:20 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:14:20 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:14:22 - snowflake.connector.cursor - INFO - Number of results in first chunk: 18
2025-02-12 19:14:22 - snowflake.connector.connection - INFO - closed
2025-02-12 19:14:22 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:14:23 - root - INFO - SQL:
SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE c."binary" = FALSE AND (c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%')
    LIMIT 100;
Results:
Too long, hard cut:
content
"<?xml version=""1.0"" encoding=""utf-8""?>
<root>
  <!-- 
    Microsoft ResX Schema 
    
    Version 2.0
    
    The primary goals of this format is to allow a simple XML format 
    that is mostly human readable. The generation and parsing of the 
    various data types are done through the TypeConverter classes 
    associated with the data types.
    
    Example:
    
    ... ado.net/XML headers & schema ...
    <resheader name=""resmimetype"">text/microsoft-resx</resheader>
    <resheader name=""version"">2.0</resheader>
    <resheader name=""reader"">System.Resources.ResXResourceReader, System.Windows.Forms, ...</resheader>
    <resheader name=""writer"">System.Resources.ResXResourceWriter, System.Windows.Forms, ...</resheader>
    <data name=""Name1""><value>this is my long string</value><comment>this is a comment</comment></data>
    <data name=""Color1"" type=""System.Drawing.Color, System.Drawing"">Blue</data>
    <data name=""Bitmap1"" mimetype=""application/x-microsoft.net.object.binary.base64"">
        <value>[base64 mime encoded serialized .NET Framework object]</value>
    </data>
    <data name=""Icon1"" type=""System.Drawing.Icon, System.Drawing"" mimetype=""application/x-microsoft.net.object.bytearray.base64"">
        <value>[base64 mime encoded string representing a byte array form of the .NET Framework object]</value>
        <comment>This is a comment</comment>
    </data>
                
    There are any number of ""resheader"" rows that contain simple 
    name/value pairs.
    
    Each data row contains a name, and value. The row also contains a 
    type or mimetype. Type corresponds to a .NET class that support 
    text/value conversion through the TypeConverter architecture. 
    Classes that don't support this are serialized and stored with the 
    mimetype set.
    
    The mimetype is used for serialized objects, and tells the 
    ResXResourceReader how to depersist the object. This is currently not 
    extensible. For a given mimetype the value must be set accordingly:
    
    Note - application/x-microsoft.net.object.binary.base64 is the format 
    that the ResXResourceWriter will generate, however the reader can 
    read any of the formats listed below.
    
    mimetype: application/x-microsoft.net.object.binary.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Binary.BinaryFormatter
            : and then encoded with base64 encoding.
    
    mimetype: application/x-microsoft.net.object.soap.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Soap.SoapFormatter
            : and then encoded with base64 encoding.

    mimetype: application/x-microsoft.net.object.bytearray.base64
    value   : The object must be serialized into a byte array 
            : using a System.ComponentModel.TypeConverter
            : and then encoded with base64 encoding.
    -->
  <xsd:schema id=""root"" xmlns="""" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:msdata=""urn:schemas-microsoft-com:xml-msdata"">
    <xsd:import namespace=""http://www.w3.org/XML/1998/namespace"" />
    <xsd:element name=""root"" msdata:IsDataSet=""true"">
      <xsd:complexType>
        <xsd:choice maxOccurs=""unbounded"">
          <xsd:element name=""metadata"">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name=""value"" type=""xsd:string"" minOccurs=""0"" />
              </xsd:sequence>
              <xsd:attribute name=""name"" use=""required"" type=""xsd:string"" />
              <xsd:attribute name=""type"" type=""xsd:string"" />
              <xsd:attribute name=""mimetype"" type=""xsd:string"" />
              <xsd:attribute ref=""xml:space"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""assembly"">
            <xsd:complexType>
              <xsd:attribute name=""alias"" type=""xsd:string"" />
              <xsd:attribute name=""name"" type=""xsd:string"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""data"">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name=""value"" type=""xsd:string"" minOccurs=""0"" msdata:Ordinal=""1"" />
                <xsd:element name=""comment"" type=""xsd:string"" minOccurs=""0"" msdata:Ordinal=""2"" />
              </xsd:sequence>
              <xsd:attribute name=""name"" type=""xsd:string"" use=""required"" msdata:Ordinal=""1"" />
              <xsd:attribute name=""type"" type=""xsd:string"" msdata:Ordinal=""3"" />
              <xsd:attribute name=""mimetype"" type=""xsd:string"" msdata:Ordinal=""4"" />
              <xsd:attribute ref=""xml:space"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""resheader"">
            <

2025-02-12 19:14:23 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:14:23 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:14:23 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:14:24 - snowflake.connector.cursor - INFO - Number of results in first chunk: 100
2025-02-12 19:14:24 - snowflake.connector.connection - INFO - closed
2025-02-12 19:14:24 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:14:24 - root - INFO - SQL:
SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path")) AS "file_extension"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
    WHERE "path" LIKE '%.%'
    LIMIT 100;
Results:
file_extension
.c
.h
.S
.xml
.txt
.svg
.png
.jar
.Tests/Data/simple.jpg.app1
.kevoree.modeling.microframework/src/main/java/org/kevoree/modeling/extrapolation/impl/maths/QRDecompositionHouseholderColumn_D64.java
.kevoree.modeling.microframework/src/main/java/org/kevoree/modeling/memory/struct/map/KUniverseOrderMap.java
.java
.js
.18/arch/mips/adm8668/time.c
.18/0057-Revert-ARM-dma-Use-dma_pfn_offset-for-dma-address-tr.patch
.18
.com/frodenas/bosh-google-cpi/vendor/github.com/onsi/gomega/matchers/be_closed_matcher.go
.dtsi
.json
.18/295-latch_led_driver.patch
.tmpl
.x/collate_report_data.rb
.pl
.1
.jshintrc
.2016.ctf.internetwache.org/irq/irq-9.rrd
.2016.ctf.internetwache.org/tcpconns-12049-local/tcp_connections-SYN_SENT.rrd
.ignore/gs_font_path.txt
.csv
.gradle
.WebApp/wwwroot/images/blogmedia/696e34be-338a-4696-ac1d-aa56cd509b27.jpeg
.xcodeproj/project.pbxproj
.swift
.2.0/gems/hashie-3.4.1/lib/hashie/extensions/merge_initializer.rb
.2.0/gems/ref-1.0.5/lib/ref/weak_reference/pure_ruby.rb
.key/Data/st32-507.jpg
.xcodeproj/project.xcworkspace/xcuserdata/huangchengdu.xcuserdatad/UserInterfaceState.xcuserstate
.html
.tex
.jpg
.scala
.properties
.2.0/specifications/httparty-0.13.5.gemspec
.editorconfig
.final_builds/packages/common/index.yml
.kevoree.modeling.microframework/src/test/java/org/kevoree/modeling/DeleteTest.java
.mk
.yml
.18/102-dt-sun7i-add_mmc2_pins.patch
.18/0053-i2c-bcm2708-add-device-tree-support.patch
.18/999-libata-hacks.patch
.xcodeproj/xcuserdata/huangchengdu.xcuserdatad/xcdebugger/Breakpoints_v2.xcbkptlist
.eslintrc
.2016.ctf.internetwache.org/tcpconns-11117-local/tcp_connections-SYN_SENT.rrd
.2016.ctf.internetwache.org/tcpconns-65534-local/tcp_connections-SYN_RECV.rrd
.conf
.gif
.validate.js
.2.0/gems/sass-3.4.13/test/sass/results/scss_importee.css
.2.0/specifications/git_repository-0.0.11.gemspec
.gemspec
.dylib.response
.app/Contents/Frameworks/Squirrel.framework/Versions/A/Modules/module.modulemap
.resx
.2.0/gems/tzinfo-1.2.2/Rakefile
.cpp.o
.debug
.xcassets/playlists.imageset/Contents.json
.mat
.lua
.rb
.2.0/gems/libv8-3.16.14.7-x86_64-linux/vendor/v8/out/x64.release/obj.target/tools/gyp/libv8_snapshot.a
.csslintrc
.t/wrong_order_par_seq_middle.ll
.t/reusedParChannel.ll
.H
.com/prometheus/procfs/fixtures/26232/fd/1
.history.js
.3
.3/g++-4.9
.iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceReminder.h
.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDirectoryRoleOperations.h
.ico
.target
.eslintrc.json
.hpp
.bin/esparse
.oasis.opendocument.draw.template.svg
.erai-30km
.4.0-b09.jar
.0.1.jar.LICENSE
.adoc
.3/g++-4.5
.5/Pf35wlWlf43N68EbhIgTcQ==
.osg
.7/genericpath.py
.min.js
.2.12/lib/auth/.libs/libgnutls_auth.la
.io/node_modules/request/node_modules/.bin/uuid

2025-02-12 19:14:24 - root - INFO - itercount: 0
2025-02-12 19:14:24 - root - INFO - Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,0
table_name,SAMPLE_FILES
description,
DDL,"create or replace TABLE SAMPLE_FILES (
	""repo_name"" VARCHAR(16777216),
	""ref"" VARCHAR(16777216),
	""path"" VARCHAR(16777216),
	""mode"" NUMBER(38,0),
	""id"" VARCHAR(16777216),
	""symlink_target"" VARCHAR(16777216)
);"

Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,1
table_name,LANGUAGES
description,
DDL,"create or replace TABLE LANGUAGES (
	""repo_name"" VARCHAR(16777216),
	""language"" VARIANT
);"

Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,2
table_name,LICENSES
description,
DDL,"create or replace TABLE LICENSES (
	""repo_name"" VARCHAR(16777216),
	""license"" VARCHAR(16777216)
);"

Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,3
table_name,SAMPLE_COMMITS
description,
DDL,"create or replace TABLE SAMPLE_COMMITS (
	""commit"" VARCHAR(16777216),
	""tree"" VARCHAR(16777216),
	""parent"" VARIANT,
	""author"" VARIANT,
	""committer"" VARIANT,
	""subject"" VARCHAR(16777216),
	""message"" VARCHAR(16777216),
	""trailer"" VARIANT,
	""difference"" VARIANT,
	""difference_truncated"" BOOLEAN,
	""repo_name"" VARCHAR(16777216),
	""encoding"" VARCHAR(16777216)
);"

Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,4
table_name,SAMPLE_CONTENTS
description,
DDL,"create or replace TABLE SAMPLE_CONTENTS (
	""id"" VARCHAR(16777216),
	""size"" NUMBER(38,0),
	""content"" VARCHAR(16777216),
	""binary"" BOOLEAN,
	""copies"" NUMBER(38,0),
	""sample_repo_name"" VARCHAR(16777216),
	""sample_ref"" VARCHAR(16777216),
	""sample_path"" VARCHAR(16777216),
	""sample_mode"" NUMBER(38,0),
	""sample_symlink_target"" VARCHAR(16777216)
);"

Database Name: GITHUB_REPOS
Schema Name: GITHUB_REPOS
,5
table_name,SAMPLE_REPOS
description,
DDL,"create or replace TABLE SAMPLE_REPOS (
	""repo_name"" VARCHAR(16777216),
	""watch_count"" NUMBER(38,0)
);"

External knowledge that might be helpful: 
None
The table structure information is ({database name: {schema name: [table name]}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['SAMPLE_FILES', 'LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_REPOS']}}
Begin Exploring Related Columns
**Relevant Tables and Columns:**

- `SAMPLE_FILES`:
  - `"id"`: Used to join with `SAMPLE_CONTENTS` on the file identifier.
  - `"path"`: Used to filter files ending with '.py' (Python) or '.r' (R).
  - `"repo_name"`: Provides the repository name, useful for grouping or context.

- `SAMPLE_CONTENTS`:
  - `"id"`: Used to join with `SAMPLE_FILES` on the file identifier.
  - `"content"`: Contains the file content from which we can extract import statements or library calls.
  - `"binary"`: Indicates if the file is binary; we need to filter out binary files (`FALSE` means the file is text).

**Conditions that may be used:**

- Filter files ending with '.py' or '.r' using `"path"` LIKE '%.py' or '%.r'.
- Exclude binary files using `"binary"` = FALSE.
- Search for specific patterns in `"content"`, such as lines starting with `import`, `from`, or containing `library(`.

---

**Sample SQL Queries:**

1. **Select all Python file paths:**
   ```sql
   SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
   ```

2. **Select all R file paths:**
   ```sql
   SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.r'
   LIMIT 100;
   ```

3. **Select contents of non-binary Python files:**
   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
   LIMIT 100;
   ```

4. **Select contents of non-binary R files:**
   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.r' AND c."binary" = FALSE
   LIMIT 100;
   ```

5. **Select distinct repository names containing Python files:**
   ```sql
   SELECT DISTINCT f."repo_name"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
   ```

6. **Count the number of Python files per repository:**
   ```sql
   SELECT f."repo_name", COUNT(*) AS "python_file_count"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   GROUP BY f."repo_name"
   ORDER BY "python_file_count" DESC
   LIMIT 100;
   ```

7. **Select non-binary Python file contents containing 'import':**
   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE AND c."content" ILIKE '%import %'
   LIMIT 100;
   ```

8. **Select non-binary R file contents containing 'library(':**
   ```sql
   SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.r' AND c."binary" = FALSE AND c."content" ILIKE '%library(%'
   LIMIT 100;
   ```

9. **Extract and list distinct Python import statements:**
   ```sql
   SELECT DISTINCT REGEXP_SUBSTR(c."content", '^(import .+|from .+ import .+)', 1, 'm') AS "import_statement"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
   LIMIT 100;
   ```

10. **Extract and count Python modules imported:**
    ```sql
    SELECT module_name, COUNT(*) AS "count"
    FROM (
      SELECT REGEXP_SUBSTR(line, '^import\\s+(\\w+)', 1, 1, 'e', 1) AS module_name
      FROM (
        SELECT SPLIT(c."content", '\\n') AS lines
        FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
        JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
        WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
      ), LATERAL FLATTEN(input => lines) AS t
      WHERE t.value ILIKE 'import %'
    )
    WHERE module_name IS NOT NULL
    GROUP BY module_name
    ORDER BY "count" DESC
    LIMIT 100;
    ```

11. **Extract and count R libraries used:**
    ```sql
    SELECT library_name, COUNT(*) AS "count"
    FROM (
      SELECT REGEXP_SUBSTR(line, 'library\\(([^)]+)\\)', 1, 1, 'e', 1) AS library_name
      FROM (
        SELECT SPLIT(c."content", '\\n') AS lines
        FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
        JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
        WHERE f."path" LIKE '%.r' AND c."binary" = FALSE
      ), LATERAL FLATTEN(input => lines) AS t
      WHERE t.value ILIKE '%library(%'
    )
    WHERE library_name IS NOT NULL
    GROUP BY library_name
    ORDER BY "count" DESC
    LIMIT 100;
    ```

12. **List distinct module names from 'from ... import' statements in Python files:**
    ```sql
    SELECT DISTINCT REGEXP_SUBSTR(c."content", '^from\\s+(\\w+)', 1, 'm') AS "module_name"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
    LIMIT 100;
    ```

13. **Count total non-binary Python files:**
    ```sql
    SELECT COUNT(*) AS "total_python_files"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON f."id" = c."id"
    WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
    LIMIT 100;
    ```

14. **Select non-binary file contents that contain either 'import' or 'library(':**
    ```sql
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE c."binary" = FALSE AND (c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%')
    LIMIT 100;
    ```

15. **List all unique file extensions in the dataset:**
    ```sql
    SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path")) AS "file_extension"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
    WHERE "path" LIKE '%.%'
    LIMIT 100;
    ```

These queries are designed to explore the data and understand how the relevant columns can be used to achieve the task. By examining file paths, content, and applying conditions, we can progressively build towards extracting and counting the occurrences of Python modules and R libraries used in the files.

*Note:* Some of the queries use regular expressions and functions like `REGEXP_SUBSTR`, `SPLIT`, and `LATERAL FLATTEN` to process and extract data from the content. The actual execution may depend on the available functions and syntax in the Snowflake SQL environment.Query:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.py'
   LIMIT 100;
Answer:
Too long, hard cut:
path
push_notifications/migrations/0002_auto_20160106_0850.py
Maximum_Subarray/Python/shivam5992/kadane.py
Caesar_Cipher/Python/kennyledet/caesar_cipher.py
setup.py
pgcrypto_expressions/test/settings.py
ec2_security_groups_dumper/__init__.py
dirigible/info_pages/__init__.py
tests/python/tests/test_updatecollection.py
tests/testproject/settings.py
yandex_money/api.py
google/cloud/dataflow/utils/__init__.py
examples/Rest/ex20_get_ilo_nic.py
try/for_now.py
node_modules/node-sass/node_modules/node-gyp/gyp/pylib/gyp/easy_xml.py
vendor/distribute-0.6.36/easy_install.py
vendor/pip-1.3.1/pip/vcs/subversion.py
vendor/distribute-0.6.36/tests/install_test.py
testing/testing/unit_tests/__init__.py
src/__init__.py
image_cropping/utils.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/GenFds/FfsInfStatement.py
src/VBox/Additions/common/crOpenGL/stub_common.py
src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/Ecc/CodeFragmentCollector.py
project/admin/__init__.py
examples/backend/myproject/myproject/wsgi.py
tools/perf/tests/attr.py
health_check_celery/tasks.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/lib/user_array.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/core/tests/test_multiarray_assignment.py
osx_build/nefi2_osx_amd64_xcode_2015/site-packages/numpy/distutils/command/sdist.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/traversal/tests/test_dfs.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/networkx/algorithms/bipartite/tests/test_matching.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/core/cversions.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/doc/howtofind.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/lib/tests/test__datasource.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/f2py/f2py2e.py
win_build/nefi2_win_amd64_msvc_2015/site-packages/numpy/polynomial/laguerre.py
osx_build/nefi2_osx_amd64_xcode_2015/bin/nefi2/model/algorithms/invert_color.py
path.py
lib/gdata/tlslite/utils/PyCrypto_AES.py
servers/youtube.py
cloudify/manager.py
misc/webdriver-w3c-tests/cookie/cookie_test.py
Lessons/lesson15.py
layout_scanner.py
networkapiclient/UsuarioGrupo.py
networkapiclient/EnvironmentVIP.py
thirdparty/future/src/libfuturize/fixes/fix_print.py
third_party/catapult/third_party/Paste/paste/util/ip4.py
third_party/catapult/third_party/webtest/webtest/forms.py
tools/gyp/test/make_global_settings/full-toolchain/gyptest-make_global_settings.py
third_party/jinja2/compiler.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/file/__init__.py
content/test/gpu/gpu_tests/trace_test_expectations.py
tools/telemetry/telemetry/value/value_unittest.py
tools/perf/page_sets/page_reload_cases.py
third_party/jinja2/_compat.py
third_party/chromite/cbuildbot/stages/build_stages.py
third_party/chromite/lib/terminal.py
tools/telemetry/telemetry/internal/backends/mandoline/config.py
third_party/cython/src/Cython/Plex/Lexicons.py
tools/gyp/test/sanitize-rule-names/gyptest-sanitize-rule-names.py
tools/grit/grit/format/policy_templates/writers/xml_writer_base_unittest.py
third_party/google_appengine_cloudstorage/cloudstorage/common.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/cloudfront/distribution.py
chrome/app/theme/PRESUBMIT.py
tools/gyp/test/ios/gyptest-per-config-settings.py
tools/telemetry/third_party/gsutilz/third_party/apitools/apitools/base/py/buffered_stream_test.py
third_party/catapult/tracing/tracing_build/__init__.py
tools/telemetry/telemetry/internal/backends/chrome_inspector/inspector_page_unittest.py
build/android/pylib/local/__init__.py
native_client/tools/scons_to_gn/print_data.py
tools/memory_inspector/memory_inspector/backends/memdump_parser.py
third_party/catapult/hooks/__init__.py
tools/cr/cr/actions/linux.py
tools/swarming_client/third_party/rsa/rsa/transform.py
third_party/catapult/perf_insights/perf_insights/corpus_driver.py
tools/gyp/test/win/gyptest-link-enable-winrt.py
third_party/chromite/scripts/dep_tracker_unittest.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/datapipeline/layer1.py
third_party/catapult/tracing/third_party/tvcm/third_party/rcssmin/_setup/py3/setup.py
third_party/chromite/lib/patch.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/mws/__init__.py
tools/telemetry/third_party/modulegraph/modulegraph_tests/testpkg-edgedata/pkg/toplevel_conditional_import_existing.py
tools/telemetry/third_party/gsutilz/third_party/boto/boto/gs/bucketlistresultset.py
tools/telemetry/third_party/gsutilz/third_party/boto/tests/unit/sqs/test_queue.py
buildtools/clang_format/script/clang-format.py
third_party/catapult/third_party/oauth2client/oauth2client/crypt.py
third_party/tlslite/tlslite/utils/cryptomath.py
tools/telemetry/third_party/gsutilz/gslib/addlhelp/projects.py
tools/deep_memory_profiler/subcommands/buckets.py
tools/telemetry/third_party/webpagereplay/dnsproxy.py
tools/chrome_proxy/integration_tests/__init__.py
third_party/cython/src/Cython/Pl
Query:
SELECT "path"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
   WHERE "path" LIKE '%.r'
   LIMIT 100;
Answer:
path
data-prep/split-train-test.r
experimental/R/rapachetest.r
src/tools/make-os-ext.r
run-all.r
iRODS/clients/icommands/test/rules/rulemsiSetRescSortScheme.r
iRODS/clients/icommands/test/rules/testsuiteForLcov.r
qiime/support_files/R/randomforests_util.r
R/clonevol.r
ppapi/native_client/tests/ppapi_messaging/ppapi_messaging.r
jni/SDL_mixer/external/libmikmod-3.1.12/macintosh/libmikmodversion.r
R/when.r
inst/examples/test_performance/demo_allgather_spmd.r
inst/examples/test_cs/simple.server.r
books/all-of-statistics/=Rprograms/stockprice.r
Stochastic/Exercise1_1.r
src/tools/make-natives.r
R/tabledata.r
build_dir/toolchain-arm_v5te_gcc-linaro_uClibc-0.9.32_eabi/binutils-2.20.1/ld/testsuite/ld-powerpc/tlsexetoc.r
R/msssr2tc.r
binr/old.rarc2/t/loop.r
examples/PlotDailyData/plot-daily-data-using-stdin-barb-detailed-instrumentation.r
examples/consoleSource/source1.r
src/01.Opt.3 - Memory Management Functions.r
dbug/user.r
ld/testsuite/ld-arm/tls-gdesc.r
Query:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
   LIMIT 100;
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties and obj_properties.hashes:
                for hash in obj_properties.hashes:
                    if hash.type_ and hash.simple_hash_value:
                        hash_type = ''
                        hash_value = ''
                        # Get the hash type
                        hash_type = str(hash.type_).lower()
                        # Get the hash value
                        hash_value = str(hash.simple_hash_value).lower()
                            
                        # Check the hash type and bin accordingly
                        if hash_type == default_hash_type:
                            if has
Query:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py'
LIMIT 100;
Answer:
Too long, hard cut:
content
"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.conf import settings
from django.db import migrations

from corehq.sql_db.operations import RawSQLMigration

migrator = RawSQLMigration(('corehq', 'sql_proxy_accessors', 'sql_templates'), {
    'PL_PROXY_CLUSTER_NAME': settings.PL_PROXY_CLUSTER_NAME
})


class Migration(migrations.Migration):

    dependencies = [
        ('sql_proxy_accessors', '0005_rename_get_case_attachment_by_name'),
    ]

    operations = [
        migrator.get_migration('get_case_by_external_id.sql'),
    ]
"
"from diesel import quickstart, quickstop, sleep
from diesel.protocols.zeromq import DieselZMQSocket, zctx, zmq
import time

def get_messages():
    outsock = DieselZMQSocket(zctx.socket(zmq.DEALER), bind=""tcp://127.0.0.1:5000"")

    t = time.time()
    for x in xrange(500000):
        msg = outsock.recv()
        assert msg == ""yo dawg %s"" % x
        if x % 1000 == 0:
            sleep()

    delt = time.time() - t
    print ""500000 messages in %ss (%.1f/s)"" % (delt, 500000.0 / delt)
    quickstop()

def tick():
    while True:
        print ""Other diesel stuff""
        sleep(1)

quickstart(get_messages, tick)
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.malware_subjects:
            malware_subjects.append(malware_subject)
    # Merge the Malware Subjects
    merged_subjects = merge_malware_subjects(malware_subjects)
    # Merge the input namespace/schemaLocation dictionaries
    merged_namespaces = {}
    merged_schemalocations = {}
    for package in package_list:
        merged_namespaces.update(package.__input_namespaces__)
        merged_schemalocations.update(package.__input_schemalocations__)
    # Create a new Package with the merged Malware Subjects
    merged_package = Package()
    merged_package.malware_subjects = MalwareSubjectList(merged_subjects)
    merged_package.__input_namespaces__ = merged_namespaces
    merged_package.__input_schemalocations__ = merged_schemalocations
    return merged_package

def bin_malware_subjects(malware_subject_list, default_hash_type='md5'):
    '''Bin a list of Malware Subjects by hash
       Default = MD5
    '''
    binned_subjects = {}
    for malware_subject in malware_subject_list:
        mal_inst_obj = malware_subject.malware_instance_object_attributes
        if mal_inst_obj:
            obj_properties = mal_inst_obj.properties
            if obj_properties
Query:
SELECT DISTINCT f."repo_name"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   LIMIT 100;
Answer:
repo_name
kennyledet/Algorithm-Implementations
percolate/ec2-security-groups-dumper
pythonanywhere/dirigible-spreadsheet
rpm-software-management/createrepo_c
sbaechler/django-multilingual-search
yandex-money/yandex-money-sdk-python
Sorosliu1029/Rule-based_Expert_System
daigofuji/jekyll-foundation-5-starter
jonasundderwolf/django-image-cropping
michaelhelmick/django-bootstrap-admin
mw-ferretti/angular-resource-tastypie
Anonim47/android_kernel_lenovo_a7000-a
KristianOellegaard/django-health-check
designalyze/python-tutorials-for-rhino
dpapathanasiou/pdfminer-layout-scanner
globocom/GloboNetworkAPI-client-python
jonathanchu/django-custom-user-example
DarkEnergyScienceCollaboration/Twinkles
StephenPower/CollectorCity-Market-Place
bt3gl/Neat-Problems-in-Python-and-Flask
CyanogenMod/android_kernel_xiaomi_cancro
SandPox/android_kernel_samsung_kyleproxx
benjaminwilson/word2vec-norm-experiments
felipead/cassandra-to-elasticsearch-sync
tum-i22/indistinguishability-obfuscation
OnePlusOSS/android_kernel_oneplus_msm8994
Azure-Readiness/hol-azure-machine-learning
CenterForOpenScience/modular-file-renderer
sualfred/script.screensaver.ftvscreensaver
CyanogenMod/android_kernel_samsung_smdk4412
andela-uawili/django-bucketlist-application
GoogleCloudPlatform/appengine-django-skeleton
MuhammedHasan/labyrinth-application-of-dynamic-connectivity-problem
osrg/ryu
ciaa/linux
livid/v2ex
coreos/etcd
ivacf/ivanc
salilab/imp
trentm/json
domokit/mojo
chiulab/surpi
eerimoq/simba
moonso/genmod
r3b/phantomjs
demotu/BMC
rozofs/rozofs
frappe/erpnext
jserv/codezero
CZ-NIC/turris-os
cmu-mtlab/meteor
markokr/skytools
gnuradio/gnuradio
springhack/alxwvj
SpectraLogic/samba
eugene1g/phantomjs
phacility/arcanist
redhat-cip/edeploy
python-constraint/python-constraint
ingadhoc/odoo-infrastructure
LumPenPacK/NetworkExtractionFromImages
cloudify-cosmo/cloudify-plugins-common
hughperkins/kgsgo-dataset-preprocessor
AdventureLookup/adventurelookup-backend
chrisdev/wagtail-cookiecutter-foundation
cisco-system-traffic-generator/trex-core
liaohuqiu/android-ClipboardManagerCompat
googleads/googleads-dfa-reporting-samples
codeforamerica/pittsburgh-purchasing-suite
cwaldbieser/jhub_remote_user_authenticator
evidation-health/ContinuousTimeMarkovModel
Ramshackle-Jamathon/Interactive-Experiments
CumulusNetworks/cumulus-linux-ansible-modules
alexandrebarachant/Grasp-and-lift-EEG-challenge
wrichert/BuildingMachineLearningSystemsWithPython
Tommy-Geenexus/android_kernel_sony_msm8994_suzuran_5.x
cernanalysispreservation/analysis-preservation.cern.ch
bup/bup
PDAL/PDAL
cylc/cylc
nipy/dipy
mnot/redbot
rdkit/rdkit
ameily/pypsi
lpryszcz/bin
aaps/MCtricky
cvxopt/cvxopt
google/libkml
nfco/netforce
nipy/PySurfer
Autodesk/quack
Erotemic/ibeis
mapsme/omim
joowani/dtags
libkml/libkml
arq5x/lumpy-sv
lab11/M-ulator
Jenyay/outwiker
arthurv/OpenTLD
naver/nbase-arc
Query:
SELECT f."repo_name", COUNT(*) AS "python_file_count"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
   WHERE f."path" LIKE '%.py'
   GROUP BY f."repo_name"
   ORDER BY "python_file_count" DESC
   LIMIT 100;
Answer:
repo_name,python_file_count
anhstudios/swganh,135
mollstam/UnrealPy,90
iridium-browser/iridium-browser-ubuntu,60
JetBrains/intellij-community,54
openhatch/oh-mainline,46
bugvm/bugvm-studio,46
hortonworks/hortonworks-sandbox,38
espadrine/opera,37
cgstudiomap/cgstudiomap,37
mirror/chromium,36
cloudera/hue,33
gabrielfalcao/lettuce,32
hynnet/hiwifi-openwrt-HC5661-HC5761,29
justathoughtor2/atomicApe,29
dstockwell/chromium,27
nwjs/chromium.src,25
Samsung/ChromiumGStreamerBackend,25
Moguri/BlenderPanda,24
neeker/chromium_extract,23
IONISx/edx-platform,23
kbengine/kbengine,22
krintoxi/NoobSecToolkit,22
Microvellum/Fluid-Designer,21
harmy/kbengine,21
cnsoft/kbengine-cocos2dx,20
OCA/OpenUpgrade,20
damonkohler/sl4a,20
acatadog/kbengine,19
openfun/edx-platform,18
pymo/pymo,18
qunaibit/zippy-mirror,18
Southpaw-TACTIC/TACTIC,17
crosswalk-project/chromium-crosswalk,17
scheib/chromium,16
dimagi/commcare-hq,16
catapult-project/catapult,16
fabioz/Pydev,16
Abjad/abjad,16
frappe/erpnext,16
django-nonrel/django,15
django/django,15
openstack/nova,15
OCA/OCB,15
openmv/micropython,15
tendenci/tendenci,14
ms-iot/python,14
redhat-openstack/nova,14
gameduell/duell,14
azatoth/scons,14
qpython-android/QPython3-core,14
OsirisSPS/osiris-sps,14
3dprinteros/3dprinteros-client,14
pelya/commandergenius,14
SickRage/SickRage,13
moesy/AWS-Lambda-ML-Microservice-Skeleton,13
LumPenPacK/NetworkExtractionFromImages,13
qgis/QGIS,13
burakbayramli/kod,13
getsentry/sentry,13
daoluan/decode-Django,13
edx/edx-platform,13
snakeleon/YouCompleteMe-x64,13
CivicTechTO/open-cabinet,12
drawquest/drawquest-web,12
vlegoff/tsunami,12
golismero/golismero,12
deanhiller/databus,12
twitter/pants,12
odoo/odoo,12
hlzz/dotfiles,12
orangeduck/PyAutoC,12
pyfa-org/Pyfa,12
FFMG/myoddweb.piger,12
Stanford-Online/edx-platform,11
VisTrails/VisTrails,11
XX-net/XX-Net,11
goldeneye-source/ges-code,11
SciTools/iris,11
Jenyay/outwiker,11
kuri65536/python-for-android,11
Azure/azure-sdk-for-python,11
duducosmos/pgs4a,11
kovidgoyal/calibre,11
DarkFenX/Pyfa,11
loopCM/chromium,11
bq/bitbloq-offline,10
ChromiumWebApps/chromium,10
NeuralEnsemble/neuroConstruct,10
googleads/googleads-python-lib,10
google/ggrc-core,10
divio/django,10
GoogleChrome/big-rig,10
aliyun/oss-ftp,10
CubicERP/odoo,10
sametmax/Django--an-app-at-a-time,10
pantsbuild/pants,10
luci/luci-py,10
bruderstein/PythonScript,10
openstack/heat,10
openstack/tempest,10
Query:
SELECT c."content"
   FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
   JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
   WHERE f."path" LIKE '%.py' AND c."binary" = FALSE AND c."content" ILIKE '%import %'
   LIMIT 100;
Answer:
Too long, hard cut:
content
"# Rekall Memory Forensics
#
# Copyright Digital Forensics Solutions.
# Copyright 2013 Google Inc. All Rights Reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or (at
# your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA

__author__ = (""Andrew Case <atcuno@gmail.com>"",
              ""Michael Cohen <scudette@google.com>"")


from rekall.plugins.linux import common


class IOmem(common.LinuxPlugin):
    '''mimics /proc/iomem.'''

    __name = ""iomem""

    table_header = [
        dict(name=""Resource"", cname=""resource"", style=""address""),
        dict(name=""Start"", cname=""start"", style=""address""),
        dict(name=""End"", cname=""end"", style=""address""),
        dict(name=""Name"", cname=""name"", type=""TreeNode""),
    ]

    def GetResources(self):
        # Resources are organized in a tree structure.
        resource_tree_root = self.profile.get_constant_object(
            ""iomem_resource"", target=""resource"")

        seen = set()

        return self._GetResources(resource_tree_root, seen)

    def _GetResources(self, node, seen, depth=0):
        """"""Traverse the resource tree depth first.""""""
        if not node or node in seen:
            return

        seen.add(node)

        yield node, depth

        if node.child:
            for x in self._GetResources(node.child.deref(), seen, depth+1):
                yield x

        for sibling in node.walk_list(""sibling""):
            for x in self._GetResources(sibling, seen, depth):
                yield x


    def collect(self):
        for node, depth in self.GetResources():
            yield dict(
                resource=node,
                start=node.start,
                end=node.end,
                name=node.name.deref(),
                depth=depth)
"
"# Copyright (c) 2015, The MITRE Corporation. All rights reserved.
# See LICENSE.txt for complete terms.

""""""Methods for merging MAEC documents""""""

from copy import deepcopy
import itertools

from mixbox import idgen
from mixbox.namespaces import Namespace

from cybox.core import Object
from cybox.common import HashList

import maec
from maec.package import (Package, MalwareSubject, MalwareConfigurationDetails,
                          FindingsBundleList, MetaAnalysis, Analyses,
                          MinorVariants, MalwareSubjectRelationshipList,
                          MalwareSubjectList)

def dict_merge(target, *args):
    '''Merge multiple dictionaries into one'''
    if len(args) > 1:
        for obj in args:
            dict_merge(target, obj)
        return target
 
    # Recursively merge dicts and set non-dict values
    obj = args[0]
    if not isinstance(obj, dict):
        return obj
    for k, v in obj.iteritems():
        if k in target and isinstance(target[k], dict):
            dict_merge(target[k], v)
        elif k in target and isinstance(target[k], list):
                target[k] = (target[k] + v)
        else:
            target[k] = deepcopy(v)
    return target

def merge_documents(input_list, output_file):
    '''Merge a list of input MAEC documents and write them to an output file'''
    parsed_documents = []
    # Parse the documents and get their API representation
    for input_file in input_list:
        api_representation = maec.parse_xml_instance(input_file)['api']
        parsed_documents.append(api_representation)
    # Do a sanity check on the input list of documents
    for document in parsed_documents:
        if isinstance(document, Package):
            continue
        else:
            print 'Error: unsupported document type. Currently only MAEC Packages are supported'

    # Merge the MAEC packages
    merged_package = merge_packages(parsed_documents)
    # Write the merged package to the output file
    merged_package.to_xml_file(output_file, {""https://github.com/MAECProject/python-maec"":""merged""})

def merge_packages(package_list, namespace = None):
    '''Merge a list of input MAEC Packages and return a merged Package instance.'''
    malware_subjects = []
    # Instantiate the ID generator class (for automatic ID generation)
    if not namespace:
        NS = Namespace(""https://github.com/MAECProject/python-maec"", ""merged"")
    else:
        NS = namespace
    maec.utils.set_id_namespace(NS)
    # Build the list of Malware Subjects
    for package in package_list:
        for malware_subject in package.m
Query:
SELECT c."content"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
LIMIT 100;
Answer:
Too long, hard cut:
content
"/*
 * rtc-ds1305.c -- driver for DS1305 and DS1306 SPI RTC chips
 *
 * Copyright (C) 2008 David Brownell
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 */
#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/bcd.h>
#include <linux/slab.h>
#include <linux/rtc.h>
#include <linux/workqueue.h>

#include <linux/spi/spi.h>
#include <linux/spi/ds1305.h>
#include <linux/module.h>


/*
 * Registers ... mask DS1305_WRITE into register address to write,
 * otherwise you're reading it.  All non-bitmask values are BCD.
 */
#define DS1305_WRITE		0x80


/* RTC date/time ... the main special cases are that we:
 *  - Need fancy ""hours"" encoding in 12hour mode
 *  - Don't rely on the ""day-of-week"" field (or tm_wday)
 *  - Are a 21st-century clock (2000 <= year < 2100)
 */
#define DS1305_RTC_LEN		7		/* bytes for RTC regs */

#define DS1305_SEC		0x00		/* register addresses */
#define DS1305_MIN		0x01
#define DS1305_HOUR		0x02
#	define DS1305_HR_12		0x40	/* set == 12 hr mode */
#	define DS1305_HR_PM		0x20	/* set == PM (12hr mode) */
#define DS1305_WDAY		0x03
#define DS1305_MDAY		0x04
#define DS1305_MON		0x05
#define DS1305_YEAR		0x06


/* The two alarms have only sec/min/hour/wday fields (ALM_LEN).
 * DS1305_ALM_DISABLE disables a match field (some combos are bad).
 *
 * NOTE that since we don't use WDAY, we limit ourselves to alarms
 * only one day into the future (vs potentially up to a week).
 *
 * NOTE ALSO that while we could generate once-a-second IRQs (UIE), we
 * don't currently support them.  We'd either need to do it only when
 * no alarm is pending (not the standard model), or to use the second
 * alarm (implying that this is a DS1305 not DS1306, *and* that either
 * it's wired up a second IRQ we know, or that INTCN is set)
 */
#define DS1305_ALM_LEN		4		/* bytes for ALM regs */
#define DS1305_ALM_DISABLE	0x80

#define DS1305_ALM0(r)		(0x07 + (r))	/* register addresses */
#define DS1305_ALM1(r)		(0x0b + (r))


/* three control registers */
#define DS1305_CONTROL_LEN	3		/* bytes of control regs */

#define DS1305_CONTROL		0x0f		/* register addresses */
#	define DS1305_nEOSC		0x80	/* low enables oscillator */
#	define DS1305_WP		0x40	/* write protect */
#	define DS1305_INTCN		0x04	/* clear == only int0 used */
#	define DS1306_1HZ		0x04	/* enable 1Hz output */
#	define DS1305_AEI1		0x02	/* enable ALM1 IRQ */
#	define DS1305_AEI0		0x01	/* enable ALM0 IRQ */
#define DS1305_STATUS		0x10
/* status has just AEIx bits, mirrored as IRQFx */
#define DS1305_TRICKLE		0x11
/* trickle bits are defined in <linux/spi/ds1305.h> */

/* a bunch of NVRAM */
#define DS1305_NVRAM_LEN	96		/* bytes of NVRAM */

#define DS1305_NVRAM		0x20		/* register addresses */


struct ds1305 {
	struct spi_device	*spi;
	struct rtc_device	*rtc;

	struct work_struct	work;

	unsigned long		flags;
#define FLAG_EXITING	0

	bool			hr12;
	u8			ctrl[DS1305_CONTROL_LEN];
};


/*----------------------------------------------------------------------*/

/*
 * Utilities ...  tolerate 12-hour AM/PM notation in case of non-Linux
 * software (like a bootloader) which may require it.
 */

static unsigned bcd2hour(u8 bcd)
{
	if (bcd & DS1305_HR_12) {
		unsigned	hour = 0;

		bcd &= ~DS1305_HR_12;
		if (bcd & DS1305_HR_PM) {
			hour = 12;
			bcd &= ~DS1305_HR_PM;
		}
		hour += bcd2bin(bcd);
		return hour - 1;
	}
	return bcd2bin(bcd);
}

static u8 hour2bcd(bool hr12, int hour)
{
	if (hr12) {
		hour++;
		if (hour <= 12)
			return DS1305_HR_12 | bin2bcd(hour);
		hour -= 12;
		return DS1305_HR_12 | DS1305_HR_PM | bin2bcd(hour);
	}
	return bin2bcd(hour);
}

/*----------------------------------------------------------------------*/

/*
 * Interface to RTC framework
 */

static int ds1305_alarm_irq_enable(struct device *dev, unsigned int enabled)
{
	struct ds1305	*ds1305 = dev_get_drvdata(dev);
	u8		buf[2];
	long		err = -EINVAL;

	buf[0] = DS1305_WRITE | DS1305_CONTROL;
	buf[1] = ds1305->ctrl[0];

	if (enabled) {
		if (ds1305->ctrl[0] & DS1305_AEI0)
			goto done;
		buf[1] |= DS1305_AEI0;
	} else {
		if (!(buf[1] & DS1305_AEI0))
			goto done;
		buf[1] &= ~DS1305_AEI0;
	}
	err = spi_write_then_read(ds1305->spi, buf, sizeof(buf), NULL, 0);
	if (err >= 0)
		ds1305->ctrl[0] = buf[1];
done:
	return err;

}


/*
 * Get/set of date and time is pretty normal.
 */

static int ds1305_get_time(struct device *dev, struct rtc_time *time)
{
	struct ds1305	*ds1305 = dev_get_drvdata(dev);
	u8		addr = DS1305_SEC;
	u8		buf[DS1305_RTC_LEN];
	int		status;

	/* Use write-then-read to get all the date/time registers
	 * since dma from stack is nonportable
	 */
	status = spi_write_then_read(ds1305->spi, &addr, sizeof(addr),
			buf, sizeof(buf));
	if (status < 0)
		return status;

	dev_vdbg(dev, ""%s: %02x %02x %02x, %02x %02x %02x %02x\n"",
		""read"", buf[0], buf[1], buf[2], buf[3],
		buf[4], buf[5], buf[6]);

	/* Decode the registers
Query:
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^(import .+|from .+ import .+)', 1, 1, 'm') AS "import_statement"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
Answer:
import_statement
"from diesel import quickstart, quickstop, sleep"
from copy import deepcopy
from __future__ import unicode_literals
from rekall.plugins.linux import common
Query:
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM (
    SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
  ), LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
Answer:
000904 (42000): 01ba5960-0004-9c22-0001-11c301112932: SQL compilation error: error line 7 at position 36
invalid identifier 'C."content"'Query:
SELECT module_name, COUNT(*) AS "count"
FROM (
  SELECT REGEXP_SUBSTR(t.value, '^import\\s+(\\w+)', 1, 1, '', 1) AS module_name
  FROM
    "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c,
    LATERAL FLATTEN(input => SPLIT(c."content", '\\n')) t
)
WHERE module_name IS NOT NULL
GROUP BY module_name
ORDER BY "count" DESC
LIMIT 100;
Answer:
MODULE_NAME,count
React,28
T,16
os,15
java,10
assert,9
Ember,9
numpy,9
org,8
Result,8
QtQuick,7
unittest,7
sys,7
logging,6
NodeProcess,6
com,6
re,6
Validator,4
path,4
TimeoutError,4
Condition,4
sinon,3
_,3
pytest,3
math,2
qbs,2
RegExp,2
datetime,2
httplib,2
Cocoa,2
time,2
misago,2
DuplexStream,2
fs,2
Process,2
asyncio,2
PaperSidenav,1
type,1
Electron,1
ChildProcess,1
actionTypes,1
UIKit,1
matplotlib,1
DarkTheme,1
FlowGrapher,1
ddt,1
asyncore,1
io,1
subpackage,1
http,1
Adafruit_BBIO,1
Orange,1
createNode,1
redis,1
Database,1
Instance,1
MachineHypermediaToolkit,1
NumberUtils,1
sendgrid,1
Base,1
decimal,1
jsonPath,1
assign,1
Page,1
RollupCjs,1
webpack,1
defaults,1
json,1
stomp,1
core,1
threading,1
PaginatingHandler,1
AssemblyKeys,1
THREE,1
ConfigParser,1
gulp,1
networkx,1
ObjectMapper,1
argparse,1
express,1
pandas,1
getpass,1
fileinput,1
fireEvent,1
Foundation,1
Session,1
qualified,1
subprocess,1
System,1
dep1,1
sbt,1
itertools,1
classnames,1
essentia,1
Sights,1
EmberObject,1
random,1
static,1
XCTest,1
collections,1
once,1
Query:
SELECT DISTINCT REGEXP_SUBSTR(c."content", '^from\\s+(\\w+)', 1, 1, 'm', 1) AS "module_name"
FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
LIMIT 100;
Answer:
module_name
rekall
copy
diesel
__future__
Query:
SELECT COUNT(*) AS "total_python_files"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON f."id" = c."id"
    WHERE f."path" LIKE '%.py' AND c."binary" = FALSE
    LIMIT 100;
Answer:
total_python_files
4
Query:
SELECT c."content"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c
    JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f ON c."id" = f."id"
    WHERE c."binary" = FALSE AND (c."content" ILIKE '%import %' OR c."content" ILIKE '%library(%')
    LIMIT 100;
Answer:
Too long, hard cut:
content
"<?xml version=""1.0"" encoding=""utf-8""?>
<root>
  <!-- 
    Microsoft ResX Schema 
    
    Version 2.0
    
    The primary goals of this format is to allow a simple XML format 
    that is mostly human readable. The generation and parsing of the 
    various data types are done through the TypeConverter classes 
    associated with the data types.
    
    Example:
    
    ... ado.net/XML headers & schema ...
    <resheader name=""resmimetype"">text/microsoft-resx</resheader>
    <resheader name=""version"">2.0</resheader>
    <resheader name=""reader"">System.Resources.ResXResourceReader, System.Windows.Forms, ...</resheader>
    <resheader name=""writer"">System.Resources.ResXResourceWriter, System.Windows.Forms, ...</resheader>
    <data name=""Name1""><value>this is my long string</value><comment>this is a comment</comment></data>
    <data name=""Color1"" type=""System.Drawing.Color, System.Drawing"">Blue</data>
    <data name=""Bitmap1"" mimetype=""application/x-microsoft.net.object.binary.base64"">
        <value>[base64 mime encoded serialized .NET Framework object]</value>
    </data>
    <data name=""Icon1"" type=""System.Drawing.Icon, System.Drawing"" mimetype=""application/x-microsoft.net.object.bytearray.base64"">
        <value>[base64 mime encoded string representing a byte array form of the .NET Framework object]</value>
        <comment>This is a comment</comment>
    </data>
                
    There are any number of ""resheader"" rows that contain simple 
    name/value pairs.
    
    Each data row contains a name, and value. The row also contains a 
    type or mimetype. Type corresponds to a .NET class that support 
    text/value conversion through the TypeConverter architecture. 
    Classes that don't support this are serialized and stored with the 
    mimetype set.
    
    The mimetype is used for serialized objects, and tells the 
    ResXResourceReader how to depersist the object. This is currently not 
    extensible. For a given mimetype the value must be set accordingly:
    
    Note - application/x-microsoft.net.object.binary.base64 is the format 
    that the ResXResourceWriter will generate, however the reader can 
    read any of the formats listed below.
    
    mimetype: application/x-microsoft.net.object.binary.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Binary.BinaryFormatter
            : and then encoded with base64 encoding.
    
    mimetype: application/x-microsoft.net.object.soap.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Soap.SoapFormatter
            : and then encoded with base64 encoding.

    mimetype: application/x-microsoft.net.object.bytearray.base64
    value   : The object must be serialized into a byte array 
            : using a System.ComponentModel.TypeConverter
            : and then encoded with base64 encoding.
    -->
  <xsd:schema id=""root"" xmlns="""" xmlns:xsd=""http://www.w3.org/2001/XMLSchema"" xmlns:msdata=""urn:schemas-microsoft-com:xml-msdata"">
    <xsd:import namespace=""http://www.w3.org/XML/1998/namespace"" />
    <xsd:element name=""root"" msdata:IsDataSet=""true"">
      <xsd:complexType>
        <xsd:choice maxOccurs=""unbounded"">
          <xsd:element name=""metadata"">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name=""value"" type=""xsd:string"" minOccurs=""0"" />
              </xsd:sequence>
              <xsd:attribute name=""name"" use=""required"" type=""xsd:string"" />
              <xsd:attribute name=""type"" type=""xsd:string"" />
              <xsd:attribute name=""mimetype"" type=""xsd:string"" />
              <xsd:attribute ref=""xml:space"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""assembly"">
            <xsd:complexType>
              <xsd:attribute name=""alias"" type=""xsd:string"" />
              <xsd:attribute name=""name"" type=""xsd:string"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""data"">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name=""value"" type=""xsd:string"" minOccurs=""0"" msdata:Ordinal=""1"" />
                <xsd:element name=""comment"" type=""xsd:string"" minOccurs=""0"" msdata:Ordinal=""2"" />
              </xsd:sequence>
              <xsd:attribute name=""name"" type=""xsd:string"" use=""required"" msdata:Ordinal=""1"" />
              <xsd:attribute name=""type"" type=""xsd:string"" msdata:Ordinal=""3"" />
              <xsd:attribute name=""mimetype"" type=""xsd:string"" msdata:Ordinal=""4"" />
              <xsd:attribute ref=""xml:space"" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name=""resheader"">
            <
Query:
SELECT DISTINCT SUBSTRING("path", POSITION('.' IN "path")) AS "file_extension"
    FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES"
    WHERE "path" LIKE '%.%'
    LIMIT 100;
Answer:
file_extension
.c
.h
.S
.xml
.txt
.svg
.png
.jar
.Tests/Data/simple.jpg.app1
.kevoree.modeling.microframework/src/main/java/org/kevoree/modeling/extrapolation/impl/maths/QRDecompositionHouseholderColumn_D64.java
.kevoree.modeling.microframework/src/main/java/org/kevoree/modeling/memory/struct/map/KUniverseOrderMap.java
.java
.js
.18/arch/mips/adm8668/time.c
.18/0057-Revert-ARM-dma-Use-dma_pfn_offset-for-dma-address-tr.patch
.18
.com/frodenas/bosh-google-cpi/vendor/github.com/onsi/gomega/matchers/be_closed_matcher.go
.dtsi
.json
.18/295-latch_led_driver.patch
.tmpl
.x/collate_report_data.rb
.pl
.1
.jshintrc
.2016.ctf.internetwache.org/irq/irq-9.rrd
.2016.ctf.internetwache.org/tcpconns-12049-local/tcp_connections-SYN_SENT.rrd
.ignore/gs_font_path.txt
.csv
.gradle
.WebApp/wwwroot/images/blogmedia/696e34be-338a-4696-ac1d-aa56cd509b27.jpeg
.xcodeproj/project.pbxproj
.swift
.2.0/gems/hashie-3.4.1/lib/hashie/extensions/merge_initializer.rb
.2.0/gems/ref-1.0.5/lib/ref/weak_reference/pure_ruby.rb
.key/Data/st32-507.jpg
.xcodeproj/project.xcworkspace/xcuserdata/huangchengdu.xcuserdatad/UserInterfaceState.xcuserstate
.html
.tex
.jpg
.scala
.properties
.2.0/specifications/httparty-0.13.5.gemspec
.editorconfig
.final_builds/packages/common/index.yml
.kevoree.modeling.microframework/src/test/java/org/kevoree/modeling/DeleteTest.java
.mk
.yml
.18/102-dt-sun7i-add_mmc2_pins.patch
.18/0053-i2c-bcm2708-add-device-tree-support.patch
.18/999-libata-hacks.patch
.xcodeproj/xcuserdata/huangchengdu.xcuserdatad/xcdebugger/Breakpoints_v2.xcbkptlist
.eslintrc
.2016.ctf.internetwache.org/tcpconns-11117-local/tcp_connections-SYN_SENT.rrd
.2016.ctf.internetwache.org/tcpconns-65534-local/tcp_connections-SYN_RECV.rrd
.conf
.gif
.validate.js
.2.0/gems/sass-3.4.13/test/sass/results/scss_importee.css
.2.0/specifications/git_repository-0.0.11.gemspec
.gemspec
.dylib.response
.app/Contents/Frameworks/Squirrel.framework/Versions/A/Modules/module.modulemap
.resx
.2.0/gems/tzinfo-1.2.2/Rakefile
.cpp.o
.debug
.xcassets/playlists.imageset/Contents.json
.mat
.lua
.rb
.2.0/gems/libv8-3.16.14.7-x86_64-linux/vendor/v8/out/x64.release/obj.target/tools/gyp/libv8_snapshot.a
.csslintrc
.t/wrong_order_par_seq_middle.ll
.t/reusedParChannel.ll
.H
.com/prometheus/procfs/fixtures/26232/fd/1
.history.js
.3
.3/g++-4.9
.iOS.Objective-C.SendMail/Pods/Headers/Private/orc/api/MSOrcCredentials.h
.iOS.Objective-C.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceReminder.h
.iOS.Swift.SendMail/Pods/Headers/Public/MSGraph-SDK-iOS/MSGraphServiceDirectoryRoleOperations.h
.ico
.target
.eslintrc.json
.hpp
.bin/esparse
.oasis.opendocument.draw.template.svg
.erai-30km
.4.0-b09.jar
.0.1.jar.LICENSE
.adoc
.3/g++-4.5
.5/Pf35wlWlf43N68EbhIgTcQ==
.osg
.7/genericpath.py
.min.js
.2.12/lib/auth/.libs/libgnutls_auth.la
.io/node_modules/request/node_modules/.bin/uuid
End Exploring Related Columns
Task: Can you analyze the joined data from github repos files and github_repos contents, focusing only on files ending with '.py' or '.r', then extract Python modules from 'import' or 'from ... import' lines and R libraries from 'library(...)' lines, count their occurrences, and finally list the results sorted by language and by the number of occurrences in descending order?

Please answer only one complete SQL in snowflake dialect in ```sql``` format.
Usage example: ```sql
SELECT "COLUMN_NAME" FROM DATABASE.SCHEMA.TABLE WHERE ... ``` (Adjust "DATABASE", "SCHEMA", and "TABLE" to match actual names, ensure all column names are enclosed in double quotations)
Follow the answer format like: ```csv
Language,Module_or_Library,Count
Python,module_name:str,count:int
Python,module_name:str,count:int
...
R,library_name:str,count:int
R,library_name:str,count:int
...
```

After analyzing the joined data from `GITHUB_REPOS.SAMPLE_FILES` and `GITHUB_REPOS.SAMPLE_CONTENTS`, focusing on files ending with `.py` (Python) and `.r` (R), we extracted Python modules from lines starting with `import` or `from ... import`, and R libraries from lines containing `library(...)`. We then counted the occurrences of each module or library. The results are listed above, sorted by language and the number of occurrences in descending order..
Here are some useful tips for answering:
When performing a UNION operation on many tables, ensure that all table names are explicitly listed. Union first and then add condition and selection. e.g. SELECT "col1", "col2" FROM (TABLE1 UNION ALL TABLE2) WHERE ...; Don't write sqls as (SELECT col1, col2 FROM TABLE1 WHERE ...) UNION ALL (SELECT col1, col2 FROM TABLE2 WHERE ...); Don't use ['-- Include all', '-- Omit', '-- Continue', '-- Union all', '-- ...', '-- List all', '-- Replace this', '-- Each table', '-- Add other'] to omit any table. Table names here: The table structure information is ({database name: {schema name: [table name]}}): 
{'GITHUB_REPOS': {'GITHUB_REPOS': ['SAMPLE_FILES', 'LANGUAGES', 'LICENSES', 'SAMPLE_COMMITS', 'SAMPLE_CONTENTS', 'SAMPLE_REPOS']}}

For string-matching scenarios, if the string is decided, don't use fuzzy query, and avoid using REGEXP. e.g. Get the object's title contains the word "book"
However, if the string is not decided, you may use fuzzy query and ignore upper or lower case. e.g. Get articles that mention "education".
When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.
When using ORDER BY, if there are duplicate values in the primary sort column, sort by an additional column as a secondary criterion.If the task description does not specify the number of decimal places, retain all decimals to four places.
Be careful of information in nested columns. e.g. When it comes to completed purchase, `hits.eCommerceAction.action_type` Indicates the type of ecommerce action and '6' represents completed purchases.
Be careful one country may have different country_name and country_region in different columns in a database.
Don't be misled by examples. For instance, a question related to Android development on StackOverflow might include tags like 'android-layout,' 'android-activity,' or 'android-intent.' However, you should not limit your analysis to just these three tags; instead, consider all tags related to Android: "tags" LIKE '%android%'.

2025-02-12 19:14:50 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:14:50 - root - INFO - ```sql
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN LOWER(f."path") LIKE '%.py' THEN 'Python'
      WHEN LOWER(f."path") LIKE '%.r' THEN 'R'
      ELSE NULL
    END AS language,
    CASE
      WHEN LOWER(f."path") LIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(TRIM(c_line.value), '^import\s+.*') THEN
            REGEXP_SUBSTR(TRIM(c_line.value), '^import\s+([\w\.]+)', 1, 1, 'e', 1)
          WHEN REGEXP_LIKE(TRIM(c_line.value), '^from\s+.*\s+import') THEN
            REGEXP_SUBSTR(TRIM(c_line.value), '^from\s+([\w\.]+)\s+import', 1, 1, 'e', 1)
          ELSE NULL
        END
      WHEN LOWER(f."path") LIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, 'library\(.*\)') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, 'e', 1)
          ELSE NULL
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id"
  , LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (f."path" LIKE '%.py' OR f."path" LIKE '%.r') AND c."binary" = FALSE
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC
```
2025-02-12 19:14:50 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:14:50 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:14:50 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:14:51 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:14:51 - snowflake.connector.connection - INFO - closed
2025-02-12 19:14:51 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:14:52 - root - INFO - itercount: 1
2025-02-12 19:14:52 - root - INFO - No data found for the specified query.

2025-02-12 19:14:52 - root - INFO - Input sql:
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN LOWER(f."path") LIKE '%.py' THEN 'Python'
      WHEN LOWER(f."path") LIKE '%.r' THEN 'R'
      ELSE NULL
    END AS language,
    CASE
      WHEN LOWER(f."path") LIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(TRIM(c_line.value), '^import\s+.*') THEN
            REGEXP_SUBSTR(TRIM(c_line.value), '^import\s+([\w\.]+)', 1, 1, 'e', 1)
          WHEN REGEXP_LIKE(TRIM(c_line.value), '^from\s+.*\s+import') THEN
            REGEXP_SUBSTR(TRIM(c_line.value), '^from\s+([\w\.]+)\s+import', 1, 1, 'e', 1)
          ELSE NULL
        END
      WHEN LOWER(f."path") LIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, 'library\(.*\)') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, 'e', 1)
          ELSE NULL
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id"
  , LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (f."path" LIKE '%.py' OR f."path" LIKE '%.r') AND c."binary" = FALSE
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC
The error information is:
 No data found for the specified query.
When using ORDER BY xxx DESC, add NULLS LAST to exclude null records: ORDER BY xxx DESC NULLS LAST.

2025-02-12 19:15:22 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:15:22 - root - INFO - ```sql
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN LOWER(f."path") LIKE '%.py' THEN 'Python'
      WHEN LOWER(f."path") LIKE '%.r' THEN 'R'
      ELSE NULL
    END AS language,
    CASE
      WHEN LOWER(f."path") LIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '^\s*import\s+[\w\.]+') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*import\s+([\w\.]+)', 1, 1, '', 1)
          WHEN REGEXP_LIKE(c_line.value, '^\s*from\s+[\w\.]+\s+import') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 1, 1, '', 1)
          ELSE NULL
        END
      WHEN LOWER(f."path") LIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '.*library\(.+\).*') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, '', 1)
          ELSE NULL
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id",
    LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (LOWER(f."path") LIKE '%.py' OR LOWER(f."path") LIKE '%.r')
    AND c."binary" = FALSE
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC NULLS LAST;
```
2025-02-12 19:15:22 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:15:22 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:15:22 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:15:24 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:15:24 - snowflake.connector.connection - INFO - closed
2025-02-12 19:15:24 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:15:25 - root - INFO - itercount: 2
2025-02-12 19:15:25 - root - INFO - No data found for the specified query.

2025-02-12 19:15:25 - root - INFO - Input sql:
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN LOWER(f."path") LIKE '%.py' THEN 'Python'
      WHEN LOWER(f."path") LIKE '%.r' THEN 'R'
      ELSE NULL
    END AS language,
    CASE
      WHEN LOWER(f."path") LIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '^\s*import\s+[\w\.]+') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*import\s+([\w\.]+)', 1, 1, '', 1)
          WHEN REGEXP_LIKE(c_line.value, '^\s*from\s+[\w\.]+\s+import') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 1, 1, '', 1)
          ELSE NULL
        END
      WHEN LOWER(f."path") LIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '.*library\(.+\).*') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, '', 1)
          ELSE NULL
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id",
    LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (LOWER(f."path") LIKE '%.py' OR LOWER(f."path") LIKE '%.r')
    AND c."binary" = FALSE
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC NULLS LAST;
The error information is:
 No data found for the specified query.

2025-02-12 19:16:24 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:16:24 - root - INFO - ```sql
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN f."path" ILIKE '%.py' THEN 'Python'
      WHEN f."path" ILIKE '%.r' THEN 'R'
    END AS language,
    CASE
      WHEN f."path" ILIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '^\s*import\s+([\w\.]+)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*import\s+([\w\.]+)', 1, 1, 'i', 1)
          WHEN REGEXP_LIKE(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 1, 1, 'i', 1)
        END
      WHEN f."path" ILIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, 'library\(([^)]+)\)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, 'i', 1)
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id",
    LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (f."path" ILIKE '%.py' OR f."path" ILIKE '%.r')
    AND c."content" IS NOT NULL
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC NULLS LAST;
```
2025-02-12 19:16:24 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:16:24 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:16:24 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:16:25 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:16:25 - snowflake.connector.connection - INFO - closed
2025-02-12 19:16:25 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:16:26 - root - INFO - itercount: 3
2025-02-12 19:16:26 - root - INFO - No data found for the specified query.

2025-02-12 19:16:26 - root - INFO - Input sql:
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN f."path" ILIKE '%.py' THEN 'Python'
      WHEN f."path" ILIKE '%.r' THEN 'R'
    END AS language,
    CASE
      WHEN f."path" ILIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '^\s*import\s+([\w\.]+)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*import\s+([\w\.]+)', 1, 1, 'i', 1)
          WHEN REGEXP_LIKE(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 1, 1, 'i', 1)
        END
      WHEN f."path" ILIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, 'library\(([^)]+)\)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, 'i', 1)
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_FILES" f
  JOIN "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c ON c."id" = f."id",
    LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (f."path" ILIKE '%.py' OR f."path" ILIKE '%.r')
    AND c."content" IS NOT NULL
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC NULLS LAST;
The error information is:
 No data found for the specified query.

2025-02-12 19:17:20 - httpx - INFO - HTTP Request: POST https://sfc-ml-sweden.openai.azure.com//openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-02-12 19:17:20 - root - INFO - ```sql
SELECT language, module_or_library, COUNT(*) AS count
FROM (
  SELECT
    CASE 
      WHEN c."sample_path" ILIKE '%.py' THEN 'Python'
      WHEN c."sample_path" ILIKE '%.r' THEN 'R'
    END AS language,
    CASE
      WHEN c."sample_path" ILIKE '%.py' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, '^\s*import\s+([\w\.]+)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*import\s+([\w\.]+)', 1, 1, 'i', 1)
          WHEN REGEXP_LIKE(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 'i') THEN
            REGEXP_SUBSTR(c_line.value, '^\s*from\s+([\w\.]+)\s+import', 1, 1, 'i', 1)
        END
      WHEN c."sample_path" ILIKE '%.r' THEN
        CASE
          WHEN REGEXP_LIKE(c_line.value, 'library\(([^)]+)\)', 'i') THEN
            REGEXP_SUBSTR(c_line.value, 'library\(([^)]+)\)', 1, 1, 'i', 1)
        END
    END AS module_or_library
  FROM "GITHUB_REPOS"."GITHUB_REPOS"."SAMPLE_CONTENTS" c,
    LATERAL FLATTEN(input => SPLIT(c."content", '\n')) c_line
  WHERE (c."sample_path" ILIKE '%.py' OR c."sample_path" ILIKE '%.r')
    AND c."content" IS NOT NULL
) sub
WHERE module_or_library IS NOT NULL
GROUP BY language, module_or_library
ORDER BY language, count DESC NULLS LAST;
```

In this query, we focus solely on the `SAMPLE_CONTENTS` table, using the `sample_path` column to filter for `.py` and `.r` files. We split the `content` of each file into lines using `SPLIT(c."content", '\n')` and `LATERAL FLATTEN`. We then apply the appropriate regular expressions to extract Python modules from `import` and `from ... import` statements, and R libraries from `library(...)` calls.

By adjusting the query to use the `sample_path` and eliminating the join with `SAMPLE_FILES`, we ensure that we are accessing the content directly and efficiently. This should resolve the issue of no data being returned.
2025-02-12 19:17:20 - snowflake.connector.connection - INFO - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.10.16, Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35
2025-02-12 19:17:20 - snowflake.connector.connection - INFO - Connecting to GLOBAL Snowflake domain
2025-02-12 19:17:20 - snowflake.connector.connection - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
2025-02-12 19:17:22 - snowflake.connector.cursor - INFO - Number of results in first chunk: 0
2025-02-12 19:17:22 - snowflake.connector.connection - INFO - closed
2025-02-12 19:17:22 - snowflake.connector.connection - INFO - No async queries seem to be running, deleting session
2025-02-12 19:17:22 - root - INFO - No data found for the specified query, remove file.

2025-02-12 19:17:22 - root - INFO - Total iteration counts: 4
